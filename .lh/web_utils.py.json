{
    "sourceFile": "web_utils.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 26,
            "patches": [
                {
                    "date": 1756856975158,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1756857277361,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,9 @@\n from ddgs import DDGS\r\n import requests\r\n from bs4 import BeautifulSoup\r\n import re\r\n+import os\r\n \r\n def search_web(query, site=None, timelimit=None):\r\n     print(f\"Searching web for query: {query}\" + (f\" site:{site}\" if site else \"\") + (f\" timelimit:{timelimit}\" if timelimit else \"\"))\r\n     with DDGS() as ddgs:\r\n"
                },
                {
                    "date": 1756857428118,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,9 +1,8 @@\n from ddgs import DDGS\r\n import requests\r\n from bs4 import BeautifulSoup\r\n import re\r\n-import os\r\n \r\n def search_web(query, site=None, timelimit=None):\r\n     print(f\"Searching web for query: {query}\" + (f\" site:{site}\" if site else \"\") + (f\" timelimit:{timelimit}\" if timelimit else \"\"))\r\n     with DDGS() as ddgs:\r\n"
                },
                {
                    "date": 1756857550797,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,9 @@\n from ddgs import DDGS\r\n import requests\r\n from bs4 import BeautifulSoup\r\n import re\r\n+import os\r\n \r\n def search_web(query, site=None, timelimit=None):\r\n     print(f\"Searching web for query: {query}\" + (f\" site:{site}\" if site else \"\") + (f\" timelimit:{timelimit}\" if timelimit else \"\"))\r\n     with DDGS() as ddgs:\r\n"
                },
                {
                    "date": 1756858468969,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,9 +1,8 @@\n from ddgs import DDGS\r\n import requests\r\n from bs4 import BeautifulSoup\r\n import re\r\n-import os\r\n \r\n def search_web(query, site=None, timelimit=None):\r\n     print(f\"Searching web for query: {query}\" + (f\" site:{site}\" if site else \"\") + (f\" timelimit:{timelimit}\" if timelimit else \"\"))\r\n     with DDGS() as ddgs:\r\n"
                },
                {
                    "date": 1756858853144,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,4 +1,5 @@\n+import os\r\n from ddgs import DDGS\r\n import requests\r\n from bs4 import BeautifulSoup\r\n import re\r\n"
                },
                {
                    "date": 1756861078450,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,8 +7,10 @@\n def search_web(query, site=None, timelimit=None):\r\n     print(f\"Searching web for query: {query}\" + (f\" site:{site}\" if site else \"\") + (f\" timelimit:{timelimit}\" if timelimit else \"\"))\r\n     with DDGS() as ddgs:\r\n         search_query = query\r\n+        if 'lyrics' in query.lower():\r\n+            search_query += \" site:genius.com OR site:azlyrics.com\"\r\n         if site:\r\n             search_query += f\" site:{site}\"\r\n         results = list(ddgs.text(search_query, max_results=10, timelimit=timelimit))\r\n         urls = [result['href'] for result in results if 'href' in result]\r\n"
                },
                {
                    "date": 1756932162131,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,10 +1,14 @@\n+# web_utils.py\r\n import os\r\n+import spacy\r\n+import requests\r\n from ddgs import DDGS\r\n-import requests\r\n from bs4 import BeautifulSoup\r\n import re\r\n \r\n+nlp = spacy.load(\"en_core_web_sm\")\r\n+\r\n def search_web(query, site=None, timelimit=None):\r\n     print(f\"Searching web for query: {query}\" + (f\" site:{site}\" if site else \"\") + (f\" timelimit:{timelimit}\" if timelimit else \"\"))\r\n     with DDGS() as ddgs:\r\n         search_query = query\r\n@@ -16,10 +20,10 @@\n         urls = [result['href'] for result in results if 'href' in result]\r\n     print(f\"Found {len(urls)} URLs: {urls}\")\r\n     return urls\r\n \r\n-def clean_web_content(url):\r\n-    print(f\"Fetching and cleaning URL: {url}\")\r\n+def clean_web_content(url, use_ollama=False):\r\n+    print(f\"Fetching and cleaning URL: {url} with Ollama: {use_ollama}\")\r\n     try:\r\n         response = requests.get(url, timeout=10)\r\n         response.raise_for_status()\r\n         html = response.text\r\n@@ -32,9 +36,52 @@\n         text = re.sub(r'[\\n\\t\\r]+', ' ', text)\r\n         text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\r\n         lines = [line.strip() for line in text.split('.') if len(line.strip()) > 20]\r\n         cleaned_text = '. '.join(lines)\r\n-        print(f\"Cleaned content length for {url}: {len(cleaned_text)} characters\")\r\n-        return cleaned_text\r\n+\r\n+        # NLP Processing\r\n+        doc = nlp(cleaned_text)\r\n+        processed_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.text.strip()]\r\n+        processed_text = ' '.join(processed_tokens)\r\n+\r\n+        # Chunking\r\n+        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\r\n+        chunk_size = 200  # Words per chunk\r\n+        chunks = []\r\n+        current_chunk = []\r\n+        current_word_count = 0\r\n+        for sent in sentences:\r\n+            word_count = len(sent.split())\r\n+            if current_word_count + word_count > chunk_size:\r\n+                chunks.append(' '.join(current_chunk))\r\n+                current_chunk = [sent]\r\n+                current_word_count = word_count\r\n+            else:\r\n+                current_chunk.append(sent)\r\n+                current_word_count += word_count\r\n+        if current_chunk:\r\n+            chunks.append(' '.join(current_chunk))\r\n+\r\n+        # Optional Ollama enhancement\r\n+        if use_ollama:\r\n+            ollama_url = \"http://localhost:11434/api/generate\"\r\n+            enhanced_chunks = []\r\n+            for chunk in chunks:\r\n+                payload = {\r\n+                    \"model\": \"qwen2.5:7b\",\r\n+                    \"prompt\": f\"Enhance and correct this web content chunk for clarity and accuracy: {chunk}. Include only the corrected text, do not include a summarization of the changes.\",\r\n+                    \"stream\": False\r\n+                }\r\n+                response = requests.post(ollama_url, json=payload)\r\n+                if response.status_code == 200:\r\n+                    enhanced_text = response.json()['response']\r\n+                    enhanced_chunks.append(enhanced_text)\r\n+                else:\r\n+                    print(f\"Error enhancing with Ollama for {url}: {response.text}\")\r\n+                    enhanced_chunks.append(chunk)  # Fallback\r\n+            processed_text = '\\n\\n'.join(enhanced_chunks)\r\n+\r\n+        print(f\"Cleaned and processed content length for {url}: {len(processed_text)} characters\")\r\n+        return processed_text\r\n     except Exception as e:\r\n         print(f\"Error cleaning {url}: {e}\")\r\n         return None\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756933891613,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -21,13 +21,17 @@\n     print(f\"Found {len(urls)} URLs: {urls}\")\r\n     return urls\r\n \r\n def clean_web_content(url, use_ollama=False):\r\n-    print(f\"Fetching and cleaning URL: {url} with Ollama: {use_ollama}\")\r\n+    yield (\"status\", f\"Fetching and cleaning URL: {url} with Ollama: {use_ollama}\")\r\n     try:\r\n+        yield (\"status\", \"Step 1: Sending request to URL...\")\r\n         response = requests.get(url, timeout=10)\r\n         response.raise_for_status()\r\n         html = response.text\r\n+        yield (\"status\", \"Step 1 completed: Response received.\")\r\n+\r\n+        yield (\"status\", \"Step 2: Parsing HTML with BeautifulSoup...\")\r\n         soup = BeautifulSoup(html, 'html.parser')\r\n         for elem in soup.select('script, style, nav, header, footer, .ad, .advert, iframe, noscript'):\r\n             elem.extract()\r\n         main_content = soup.find('main') or soup.find('article') or soup\r\n@@ -36,15 +40,19 @@\n         text = re.sub(r'[\\n\\t\\r]+', ' ', text)\r\n         text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\r\n         lines = [line.strip() for line in text.split('.') if len(line.strip()) > 20]\r\n         cleaned_text = '. '.join(lines)\r\n+        yield (\"status\", f\"Step 2 completed: Cleaned text length: {len(cleaned_text)} characters.\")\r\n \r\n         # NLP Processing\r\n+        yield (\"status\", \"Step 3: Processing with NLP...\")\r\n         doc = nlp(cleaned_text)\r\n         processed_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.text.strip()]\r\n         processed_text = ' '.join(processed_tokens)\r\n+        yield (\"status\", \"Step 3 completed: NLP processing done.\")\r\n \r\n         # Chunking\r\n+        yield (\"status\", \"Step 4: Chunking content...\")\r\n         sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\r\n         chunk_size = 200  # Words per chunk\r\n         chunks = []\r\n         current_chunk = []\r\n@@ -59,29 +67,37 @@\n                 current_chunk.append(sent)\r\n                 current_word_count += word_count\r\n         if current_chunk:\r\n             chunks.append(' '.join(current_chunk))\r\n+        yield (\"status\", f\"Step 4 completed: Created {len(chunks)} chunks.\")\r\n \r\n         # Optional Ollama enhancement\r\n         if use_ollama:\r\n+            yield (\"status\", \"Step 5: Enhancing chunks with Ollama...\")\r\n             ollama_url = \"http://localhost:11434/api/generate\"\r\n             enhanced_chunks = []\r\n-            for chunk in chunks:\r\n+            for i, chunk in enumerate(chunks):\r\n+                yield (\"status\", f\"Enhancing chunk {i+1}/{len(chunks)}...\")\r\n                 payload = {\r\n                     \"model\": \"qwen2.5:7b\",\r\n                     \"prompt\": f\"Enhance and correct this web content chunk for clarity and accuracy: {chunk}. Include only the corrected text, do not include a summarization of the changes.\",\r\n                     \"stream\": False\r\n                 }\r\n-                response = requests.post(ollama_url, json=payload)\r\n-                if response.status_code == 200:\r\n-                    enhanced_text = response.json()['response']\r\n-                    enhanced_chunks.append(enhanced_text)\r\n-                else:\r\n-                    print(f\"Error enhancing with Ollama for {url}: {response.text}\")\r\n-                    enhanced_chunks.append(chunk)  # Fallback\r\n+                try:\r\n+                    resp = requests.post(ollama_url, json=payload, timeout=30)\r\n+                    if resp.status_code == 200:\r\n\\ No newline at end of file\n+                        enhanced_text = resp.json()['response']\r\n+                        enhanced_chunks.append(enhanced_text)\r\n+                        yield (\"status\", f\"Chunk {i+1} enhanced.\")\r\n+                    else:\r\n+                        yield (\"status\", f\"Error enhancing chunk {i+1}: {resp.text}\")\r\n+                        enhanced_chunks.append(chunk)\r\n+                except requests.exceptions.RequestException as e:\r\n+                    yield (\"status\", f\"Ollama request failed for chunk {i+1}: {e}. Falling back.\")\r\n+                    enhanced_chunks.append(chunk)\r\n             processed_text = '\\n\\n'.join(enhanced_chunks)\r\n+            yield (\"status\", \"Step 5 completed: Ollama enhancement done.\")\r\n \r\n-        print(f\"Cleaned and processed content length for {url}: {len(processed_text)} characters\")\r\n-        return processed_text\r\n+        yield (\"content\", processed_text)\r\n     except Exception as e:\r\n-        print(f\"Error cleaning {url}: {e}\")\r\n-        return None\n+        yield (\"status\", f\"Error cleaning {url}: {e}\")\r\n+        yield (\"content\", None)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756935161950,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,103 +1,55 @@\n # web_utils.py\r\n import os\r\n import spacy\r\n import requests\r\n+import threading\r\n from ddgs import DDGS\r\n from bs4 import BeautifulSoup\r\n import re\r\n+from config import MAX_URLS, FAISS_PATH\r\n+from process_utils import process_urls\r\n+from utils import lock\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n def search_web(query, site=None, timelimit=None):\r\n-    print(f\"Searching web for query: {query}\" + (f\" site:{site}\" if site else \"\") + (f\" timelimit:{timelimit}\" if timelimit else \"\"))\r\n-    with DDGS() as ddgs:\r\n-        search_query = query\r\n-        if 'lyrics' in query.lower():\r\n-            search_query += \" site:genius.com OR site:azlyrics.com\"\r\n-        if site:\r\n-            search_query += f\" site:{site}\"\r\n-        results = list(ddgs.text(search_query, max_results=10, timelimit=timelimit))\r\n-        urls = [result['href'] for result in results if 'href' in result]\r\n-    print(f\"Found {len(urls)} URLs: {urls}\")\r\n-    return urls\r\n+    # (unchanged)\r\n \r\n def clean_web_content(url, use_ollama=False):\r\n-    yield (\"status\", f\"Fetching and cleaning URL: {url} with Ollama: {use_ollama}\")\r\n+    # (unchanged)\r\n+\r\n+def run_web_collection(task_id, query, timelimit, use_ollama, tasks, completed_collections, conn=None):\r\n+    print(f\"Starting Web collection task {task_id} for query: {query}\")\r\n     try:\r\n-        yield (\"status\", \"Step 1: Sending request to URL...\")\r\n-        response = requests.get(url, timeout=10)\r\n-        response.raise_for_status()\r\n-        html = response.text\r\n-        yield (\"status\", \"Step 1 completed: Response received.\")\r\n+        timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n+        urls = search_web(query, timelimit=timelimit_code)\r\n+        all_urls = list(set(urls))[:MAX_URLS]\r\n \r\n-        yield (\"status\", \"Step 2: Parsing HTML with BeautifulSoup...\")\r\n-        soup = BeautifulSoup(html, 'html.parser')\r\n-        for elem in soup.select('script, style, nav, header, footer, .ad, .advert, iframe, noscript'):\r\n-            elem.extract()\r\n-        main_content = soup.find('main') or soup.find('article') or soup\r\n-        text = main_content.get_text(separator='\\n', strip=True)\r\n-        text = re.sub(r'\\s+', ' ', text)\r\n-        text = re.sub(r'[\\n\\t\\r]+', ' ', text)\r\n-        text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\r\n-        lines = [line.strip() for line in text.split('.') if len(line.strip()) > 20]\r\n-        cleaned_text = '. '.join(lines)\r\n-        yield (\"status\", f\"Step 2 completed: Cleaned text length: {len(cleaned_text)} characters.\")\r\n+        tag = f\"web_{query}_{timelimit}\"\r\n+        history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy\r\n+        message = query\r\n+        response = \"\"\r\n \r\n-        # NLP Processing\r\n-        yield (\"status\", \"Step 3: Processing with NLP...\")\r\n-        doc = nlp(cleaned_text)\r\n-        processed_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.text.strip()]\r\n-        processed_text = ' '.join(processed_tokens)\r\n-        yield (\"status\", \"Step 3 completed: NLP processing done.\")\r\n+        process_gen = process_urls(all_urls, response, history, message, conn=conn, source_tag=tag, use_ollama=use_ollama)\r\n+        try:\r\n+            while True:\r\n+                next(process_gen)\r\n+        except StopIteration as e:\r\n+            sources, response, history = e.value\r\n \r\n-        # Chunking\r\n-        yield (\"status\", \"Step 4: Chunking content...\")\r\n-        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\r\n-        chunk_size = 200  # Words per chunk\r\n-        chunks = []\r\n-        current_chunk = []\r\n-        current_word_count = 0\r\n-        for sent in sentences:\r\n-            word_count = len(sent.split())\r\n-            if current_word_count + word_count > chunk_size:\r\n-                chunks.append(' '.join(current_chunk))\r\n-                current_chunk = [sent]\r\n-                current_word_count = word_count\r\n-            else:\r\n-                current_chunk.append(sent)\r\n-                current_word_count += word_count\r\n-        if current_chunk:\r\n-            chunks.append(' '.join(current_chunk))\r\n-        yield (\"status\", f\"Step 4 completed: Created {len(chunks)} chunks.\")\r\n+        tasks[task_id]['status'] = 'completed'\r\n+        tasks[task_id]['message'] = \"Collection completed. New content added to vectorstore if applicable.\"\r\n+        tasks[task_id]['tag'] = tag\r\n+        completed_collections.append({'name': f\"Web - {query} ({timelimit})\", 'tag': tag})\r\n+        print(f\"Web collection task {task_id} completed.\")\r\n+    except Exception as e:\r\n+        tasks[task_id]['status'] = 'error'\r\n+        tasks[task_id]['message'] = str(e)\r\n+        print(f\"Web collection task {task_id} error: {e}\")\r\n \r\n\\ No newline at end of file\n-        # Optional Ollama enhancement\r\n-        if use_ollama:\r\n-            yield (\"status\", \"Step 5: Enhancing chunks with Ollama...\")\r\n-            ollama_url = \"http://localhost:11434/api/generate\"\r\n-            enhanced_chunks = []\r\n-            for i, chunk in enumerate(chunks):\r\n-                yield (\"status\", f\"Enhancing chunk {i+1}/{len(chunks)}...\")\r\n-                payload = {\r\n-                    \"model\": \"qwen2.5:7b\",\r\n-                    \"prompt\": f\"Enhance and correct this web content chunk for clarity and accuracy: {chunk}. Include only the corrected text, do not include a summarization of the changes.\",\r\n-                    \"stream\": False\r\n-                }\r\n-                try:\r\n-                    resp = requests.post(ollama_url, json=payload, timeout=30)\r\n-                    if resp.status_code == 200:\r\n-                        enhanced_text = resp.json()['response']\r\n-                        enhanced_chunks.append(enhanced_text)\r\n-                        yield (\"status\", f\"Chunk {i+1} enhanced.\")\r\n-                    else:\r\n-                        yield (\"status\", f\"Error enhancing chunk {i+1}: {resp.text}\")\r\n-                        enhanced_chunks.append(chunk)\r\n-                except requests.exceptions.RequestException as e:\r\n-                    yield (\"status\", f\"Ollama request failed for chunk {i+1}: {e}. Falling back.\")\r\n-                    enhanced_chunks.append(chunk)\r\n-            processed_text = '\\n\\n'.join(enhanced_chunks)\r\n-            yield (\"status\", \"Step 5 completed: Ollama enhancement done.\")\r\n-\r\n-        yield (\"content\", processed_text)\r\n-    except Exception as e:\r\n-        yield (\"status\", f\"Error cleaning {url}: {e}\")\r\n-        yield (\"content\", None)\n+def start_web_collection(query, timelimit, use_ollama, tasks, completed_collections, conn=None):\r\n+    task_id = len(tasks)\r\n+    task = {'id': task_id, 'type': 'web', 'query': query, 'timelimit': timelimit, 'use_ollama': use_ollama, 'status': 'running', 'message': ''}\r\n+    tasks.append(task)\r\n+    threading.Thread(target=run_web_collection, args=(task_id, query, timelimit, use_ollama, tasks, completed_collections, conn)).start()\r\n+    return \"Web collection started in background.\", tasks, completed_collections\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756935277826,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,13 +12,101 @@\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n def search_web(query, site=None, timelimit=None):\r\n-    # (unchanged)\r\n+    print(f\"Searching web for query: {query}\" + (f\" site:{site}\" if site else \"\") + (f\" timelimit:{timelimit}\" if timelimit else \"\"))\r\n+    with DDGS() as ddgs:\r\n+        search_query = query\r\n+        if 'lyrics' in query.lower():\r\n+            search_query += \" site:genius.com OR site:azlyrics.com\"\r\n+        if site:\r\n+            search_query += f\" site:{site}\"\r\n+        results = list(ddgs.text(search_query, max_results=10, timelimit=timelimit))\r\n+        urls = [result['href'] for result in results if 'href' in result]\r\n+    print(f\"Found {len(urls)} URLs: {urls}\")\r\n+    return urls\r\n \r\n def clean_web_content(url, use_ollama=False):\r\n-    # (unchanged)\r\n+    yield (\"status\", f\"Fetching and cleaning URL: {url} with Ollama: {use_ollama}\")\r\n+    try:\r\n+        yield (\"status\", \"Step 1: Sending request to URL...\")\r\n+        response = requests.get(url, timeout=10)\r\n+        response.raise_for_status()\r\n+        html = response.text\r\n+        yield (\"status\", \"Step 1 completed: Response received.\")\r\n \r\n+        yield (\"status\", \"Step 2: Parsing HTML with BeautifulSoup...\")\r\n+        soup = BeautifulSoup(html, 'html.parser')\r\n+        for elem in soup.select('script, style, nav, header, footer, .ad, .advert, iframe, noscript'):\r\n+            elem.extract()\r\n+        main_content = soup.find('main') or soup.find('article') or soup\r\n+        text = main_content.get_text(separator='\\n', strip=True)\r\n+        text = re.sub(r'\\s+', ' ', text)\r\n+        text = re.sub(r'[\\n\\t\\r]+', ' ', text)\r\n+        text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\r\n+        lines = [line.strip() for line in text.split('.') if len(line.strip()) > 20]\r\n+        cleaned_text = '. '.join(lines)\r\n+        yield (\"status\", f\"Step 2 completed: Cleaned text length: {len(cleaned_text)} characters.\")\r\n+\r\n+        # NLP Processing\r\n+        yield (\"status\", \"Step 3: Processing with NLP...\")\r\n+        doc = nlp(cleaned_text)\r\n+        processed_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.text.strip()]\r\n+        processed_text = ' '.join(processed_tokens)\r\n+        yield (\"status\", \"Step 3 completed: NLP processing done.\")\r\n+\r\n+        # Chunking\r\n+        yield (\"status\", \"Step 4: Chunking content...\")\r\n+        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\r\n+        chunk_size = 200  # Words per chunk\r\n+        chunks = []\r\n+        current_chunk = []\r\n+        current_word_count = 0\r\n+        for sent in sentences:\r\n+            word_count = len(sent.split())\r\n+            if current_word_count + word_count > chunk_size:\r\n+                chunks.append(' '.join(current_chunk))\r\n+                current_chunk = [sent]\r\n+                current_word_count = word_count\r\n+            else:\r\n+                current_chunk.append(sent)\r\n+                current_word_count += word_count\r\n+        if current_chunk:\r\n+            chunks.append(' '.join(current_chunk))\r\n+        yield (\"status\", f\"Step 4 completed: Created {len(chunks)} chunks.\")\r\n+\r\n+        # Optional Ollama enhancement\r\n+        if use_ollama:\r\n+            yield (\"status\", \"Step 5: Enhancing chunks with Ollama...\")\r\n+            ollama_url = \"http://localhost:11434/api/generate\"\r\n+            enhanced_chunks = []\r\n+            for i, chunk in enumerate(chunks):\r\n+                yield (\"status\", f\"Enhancing chunk {i+1}/{len(chunks)}...\")\r\n+                payload = {\r\n+                    \"model\": \"qwen2.5:7b\",\r\n+                    \"prompt\": f\"Enhance and correct this web content chunk for clarity and accuracy: {chunk}. Include only the corrected text, do not include a summarization of the changes.\",\r\n+                    \"stream\": False\r\n+                }\r\n+                try:\r\n+                    resp = requests.post(ollama_url, json=payload, timeout=30)\r\n+                    if resp.status_code == 200:\r\n+                        enhanced_text = resp.json()['response']\r\n+                        enhanced_chunks.append(enhanced_text)\r\n+                        yield (\"status\", f\"Chunk {i+1} enhanced.\")\r\n+                    else:\r\n+                        yield (\"status\", f\"Error enhancing chunk {i+1}: {resp.text}\")\r\n+                        enhanced_chunks.append(chunk)\r\n+                except requests.exceptions.RequestException as e:\r\n+                    yield (\"status\", f\"Ollama request failed for chunk {i+1}: {e}. Falling back.\")\r\n+                    enhanced_chunks.append(chunk)\r\n+            processed_text = '\\n\\n'.join(enhanced_chunks)\r\n+            yield (\"status\", \"Step 5 completed: Ollama enhancement done.\")\r\n+\r\n+        yield (\"content\", processed_text)\r\n+    except Exception as e:\r\n+        yield (\"status\", f\"Error cleaning {url}: {e}\")\r\n+        yield (\"content\", None)\r\n+\r\n def run_web_collection(task_id, query, timelimit, use_ollama, tasks, completed_collections, conn=None):\r\n     print(f\"Starting Web collection task {task_id} for query: {query}\")\r\n     try:\r\n         timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n"
                },
                {
                    "date": 1756935358103,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,89 +24,8 @@\n         urls = [result['href'] for result in results if 'href' in result]\r\n     print(f\"Found {len(urls)} URLs: {urls}\")\r\n     return urls\r\n \r\n-def clean_web_content(url, use_ollama=False):\r\n-    yield (\"status\", f\"Fetching and cleaning URL: {url} with Ollama: {use_ollama}\")\r\n-    try:\r\n-        yield (\"status\", \"Step 1: Sending request to URL...\")\r\n-        response = requests.get(url, timeout=10)\r\n-        response.raise_for_status()\r\n-        html = response.text\r\n-        yield (\"status\", \"Step 1 completed: Response received.\")\r\n-\r\n-        yield (\"status\", \"Step 2: Parsing HTML with BeautifulSoup...\")\r\n-        soup = BeautifulSoup(html, 'html.parser')\r\n-        for elem in soup.select('script, style, nav, header, footer, .ad, .advert, iframe, noscript'):\r\n-            elem.extract()\r\n-        main_content = soup.find('main') or soup.find('article') or soup\r\n-        text = main_content.get_text(separator='\\n', strip=True)\r\n-        text = re.sub(r'\\s+', ' ', text)\r\n-        text = re.sub(r'[\\n\\t\\r]+', ' ', text)\r\n-        text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\r\n-        lines = [line.strip() for line in text.split('.') if len(line.strip()) > 20]\r\n-        cleaned_text = '. '.join(lines)\r\n-        yield (\"status\", f\"Step 2 completed: Cleaned text length: {len(cleaned_text)} characters.\")\r\n-\r\n-        # NLP Processing\r\n-        yield (\"status\", \"Step 3: Processing with NLP...\")\r\n-        doc = nlp(cleaned_text)\r\n-        processed_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.text.strip()]\r\n-        processed_text = ' '.join(processed_tokens)\r\n-        yield (\"status\", \"Step 3 completed: NLP processing done.\")\r\n-\r\n-        # Chunking\r\n-        yield (\"status\", \"Step 4: Chunking content...\")\r\n-        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\r\n-        chunk_size = 200  # Words per chunk\r\n-        chunks = []\r\n-        current_chunk = []\r\n-        current_word_count = 0\r\n-        for sent in sentences:\r\n-            word_count = len(sent.split())\r\n-            if current_word_count + word_count > chunk_size:\r\n-                chunks.append(' '.join(current_chunk))\r\n-                current_chunk = [sent]\r\n-                current_word_count = word_count\r\n-            else:\r\n-                current_chunk.append(sent)\r\n-                current_word_count += word_count\r\n-        if current_chunk:\r\n-            chunks.append(' '.join(current_chunk))\r\n-        yield (\"status\", f\"Step 4 completed: Created {len(chunks)} chunks.\")\r\n-\r\n-        # Optional Ollama enhancement\r\n-        if use_ollama:\r\n-            yield (\"status\", \"Step 5: Enhancing chunks with Ollama...\")\r\n-            ollama_url = \"http://localhost:11434/api/generate\"\r\n-            enhanced_chunks = []\r\n-            for i, chunk in enumerate(chunks):\r\n-                yield (\"status\", f\"Enhancing chunk {i+1}/{len(chunks)}...\")\r\n-                payload = {\r\n-                    \"model\": \"qwen2.5:7b\",\r\n-                    \"prompt\": f\"Enhance and correct this web content chunk for clarity and accuracy: {chunk}. Include only the corrected text, do not include a summarization of the changes.\",\r\n-                    \"stream\": False\r\n-                }\r\n-                try:\r\n-                    resp = requests.post(ollama_url, json=payload, timeout=30)\r\n-                    if resp.status_code == 200:\r\n-                        enhanced_text = resp.json()['response']\r\n-                        enhanced_chunks.append(enhanced_text)\r\n-                        yield (\"status\", f\"Chunk {i+1} enhanced.\")\r\n-                    else:\r\n-                        yield (\"status\", f\"Error enhancing chunk {i+1}: {resp.text}\")\r\n-                        enhanced_chunks.append(chunk)\r\n-                except requests.exceptions.RequestException as e:\r\n-                    yield (\"status\", f\"Ollama request failed for chunk {i+1}: {e}. Falling back.\")\r\n-                    enhanced_chunks.append(chunk)\r\n-            processed_text = '\\n\\n'.join(enhanced_chunks)\r\n-            yield (\"status\", \"Step 5 completed: Ollama enhancement done.\")\r\n-\r\n-        yield (\"content\", processed_text)\r\n-    except Exception as e:\r\n-        yield (\"status\", f\"Error cleaning {url}: {e}\")\r\n-        yield (\"content\", None)\r\n-\r\n def run_web_collection(task_id, query, timelimit, use_ollama, tasks, completed_collections, conn=None):\r\n     print(f\"Starting Web collection task {task_id} for query: {query}\")\r\n     try:\r\n         timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n"
                },
                {
                    "date": 1756935568038,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,55 @@\n+# web_utils.py\r\n+import os\r\n+import spacy\r\n+import requests\r\n+import threading\r\n+from ddgs import DDGS\r\n+from bs4 import BeautifulSoup\r\n+import re\r\n+from config import MAX_URLS, FAISS_PATH\r\n+from process_utils import process_urls\r\n+from utils import lock\r\n+\r\n+nlp = spacy.load(\"en_core_web_sm\")\r\n+\r\n+def search_web(query, site=None, timelimit=None):\r\n+    # (unchanged)\r\n+\r\n+def clean_web_content(url, use_ollama=False):\r\n+    # (unchanged)\r\n+\r\n+def run_web_collection(task_id, query, timelimit, use_ollama, tasks, completed_collections, conn=None):\r\n+    print(f\"Starting Web collection task {task_id} for query: {query}\")\r\n+    try:\r\n+        timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n+        urls = search_web(query, timelimit=timelimit_code)\r\n+        all_urls = list(set(urls))[:MAX_URLS]\r\n+\r\n+        tag = f\"web_{query}_{timelimit}\"\r\n+        history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy\r\n+        message = query\r\n+        response = \"\"\r\n+\r\n+        process_gen = process_urls(all_urls, response, history, message, conn=conn, source_tag=tag, use_ollama=use_ollama)\r\n+        try:\r\n+            while True:\r\n+                next(process_gen)\r\n+        except StopIteration as e:\r\n+            sources, response, history = e.value\r\n+\r\n+        tasks[task_id]['status'] = 'completed'\r\n+        tasks[task_id]['message'] = \"Collection completed. New content added to vectorstore if applicable.\"\r\n+        tasks[task_id]['tag'] = tag\r\n+        completed_collections.append({'name': f\"Web - {query} ({timelimit})\", 'tag': tag})\r\n+        print(f\"Web collection task {task_id} completed.\")\r\n+    except Exception as e:\r\n+        tasks[task_id]['status'] = 'error'\r\n+        tasks[task_id]['message'] = str(e)\r\n+        print(f\"Web collection task {task_id} error: {e}\")\r\n+\r\n+def start_web_collection(query, timelimit, use_ollama, tasks, completed_collections, conn=None):\r\n+    task_id = len(tasks)\r\n+    task = {'id': task_id, 'type': 'web', 'query': query, 'timelimit': timelimit, 'use_ollama': use_ollama, 'status': 'running', 'message': ''}\r\n+    tasks.append(task)\r\n+    threading.Thread(target=run_web_collection, args=(task_id, query, timelimit, use_ollama, tasks, completed_collections, conn)).start()\r\n+    return \"Web collection started in background.\", tasks, completed_collections\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756935588211,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,63 +12,8 @@\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n def search_web(query, site=None, timelimit=None):\r\n-    # (unchanged)\r\n-\r\n-def clean_web_content(url, use_ollama=False):\r\n-    # (unchanged)\r\n-\r\n-def run_web_collection(task_id, query, timelimit, use_ollama, tasks, completed_collections, conn=None):\r\n-    print(f\"Starting Web collection task {task_id} for query: {query}\")\r\n-    try:\r\n-        timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n-        urls = search_web(query, timelimit=timelimit_code)\r\n-        all_urls = list(set(urls))[:MAX_URLS]\r\n-\r\n-        tag = f\"web_{query}_{timelimit}\"\r\n-        history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy\r\n-        message = query\r\n-        response = \"\"\r\n-\r\n-        process_gen = process_urls(all_urls, response, history, message, conn=conn, source_tag=tag, use_ollama=use_ollama)\r\n-        try:\r\n-            while True:\r\n-                next(process_gen)\r\n-        except StopIteration as e:\r\n-            sources, response, history = e.value\r\n-\r\n-        tasks[task_id]['status'] = 'completed'\r\n-        tasks[task_id]['message'] = \"Collection completed. New content added to vectorstore if applicable.\"\r\n-        tasks[task_id]['tag'] = tag\r\n-        completed_collections.append({'name': f\"Web - {query} ({timelimit})\", 'tag': tag})\r\n-        print(f\"Web collection task {task_id} completed.\")\r\n-    except Exception as e:\r\n-        tasks[task_id]['status'] = 'error'\r\n-        tasks[task_id]['message'] = str(e)\r\n-        print(f\"Web collection task {task_id} error: {e}\")\r\n-\r\n-def start_web_collection(query, timelimit, use_ollama, tasks, completed_collections, conn=None):\r\n-    task_id = len(tasks)\r\n-    task = {'id': task_id, 'type': 'web', 'query': query, 'timelimit': timelimit, 'use_ollama': use_ollama, 'status': 'running', 'message': ''}\r\n-    tasks.append(task)\r\n-    threading.Thread(target=run_web_collection, args=(task_id, query, timelimit, use_ollama, tasks, completed_collections, conn)).start()\r\n-    return \"Web collection started in background.\", tasks, completed_collections\n-# web_utils.py\r\n-import os\r\n-import spacy\r\n-import requests\r\n-import threading\r\n-from ddgs import DDGS\r\n-from bs4 import BeautifulSoup\r\n-import re\r\n-from config import MAX_URLS, FAISS_PATH\r\n-from process_utils import process_urls\r\n-from utils import lock\r\n-\r\n-nlp = spacy.load(\"en_core_web_sm\")\r\n-\r\n-def search_web(query, site=None, timelimit=None):\r\n     print(f\"Searching web for query: {query}\" + (f\" site:{site}\" if site else \"\") + (f\" timelimit:{timelimit}\" if timelimit else \"\"))\r\n     with DDGS() as ddgs:\r\n         search_query = query\r\n         if 'lyrics' in query.lower():\r\n"
                },
                {
                    "date": 1756936112747,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,8 +2,9 @@\n import os\r\n import spacy\r\n import requests\r\n import threading\r\n+import sqlite3\r\n from ddgs import DDGS\r\n from bs4 import BeautifulSoup\r\n import re\r\n from config import MAX_URLS, FAISS_PATH\r\n@@ -24,11 +25,25 @@\n         urls = [result['href'] for result in results if 'href' in result]\r\n     print(f\"Found {len(urls)} URLs: {urls}\")\r\n     return urls\r\n \r\n-def run_web_collection(task_id, query, timelimit, use_ollama, tasks, completed_collections, conn=None):\r\n+def run_web_collection(task_id, query, timelimit, use_ollama, tasks, completed_collections):\r\n     print(f\"Starting Web collection task {task_id} for query: {query}\")\r\n+    conn = sqlite3.connect('crawled.db')\r\n+    c = conn.cursor()\r\n+    c.execute('''CREATE TABLE IF NOT EXISTS urls\r\n+                 (url TEXT PRIMARY KEY, timestamp DATETIME, cleaned_text TEXT)''')\r\n+    c.execute('''CREATE TABLE IF NOT EXISTS chunks\r\n+                 (hash TEXT PRIMARY KEY, content TEXT, source TEXT, tag TEXT)''')\r\n     try:\r\n+        c.execute(\"ALTER TABLE chunks ADD COLUMN tag TEXT\")\r\n+        print(\"Added 'tag' column to chunks table.\")\r\n+    except sqlite3.OperationalError as e:\r\n+        if \"duplicate column name\" not in str(e):\r\n+            raise e\r\n+        print(\"'tag' column already exists in chunks table.\")\r\n+    conn.commit()\r\n+    try:\r\n         timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n         urls = search_web(query, timelimit=timelimit_code)\r\n         all_urls = list(set(urls))[:MAX_URLS]\r\n \r\n@@ -52,11 +67,13 @@\n     except Exception as e:\r\n         tasks[task_id]['status'] = 'error'\r\n         tasks[task_id]['message'] = str(e)\r\n         print(f\"Web collection task {task_id} error: {e}\")\r\n+    finally:\r\n+        conn.close()\r\n \r\n-def start_web_collection(query, timelimit, use_ollama, tasks, completed_collections, conn=None):\r\n+def start_web_collection(query, timelimit, use_ollama, tasks, completed_collections):\r\n     task_id = len(tasks)\r\n     task = {'id': task_id, 'type': 'web', 'query': query, 'timelimit': timelimit, 'use_ollama': use_ollama, 'status': 'running', 'message': ''}\r\n     tasks.append(task)\r\n-    threading.Thread(target=run_web_collection, args=(task_id, query, timelimit, use_ollama, tasks, completed_collections, conn)).start()\r\n+    threading.Thread(target=run_web_collection, args=(task_id, query, timelimit, use_ollama, tasks, completed_collections)).start()\r\n     return \"Web collection started in background.\", tasks, completed_collections\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756937978284,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n         urls = [result['href'] for result in results if 'href' in result]\r\n     print(f\"Found {len(urls)} URLs: {urls}\")\r\n     return urls\r\n \r\n-def run_web_collection(task_id, query, timelimit, use_ollama, tasks, completed_collections):\r\n+def run_web_collection(task_id, custom_name, query, timelimit, use_ollama, tasks, completed_collections):\r\n     print(f\"Starting Web collection task {task_id} for query: {query}\")\r\n     conn = sqlite3.connect('crawled.db')\r\n     c = conn.cursor()\r\n     c.execute('''CREATE TABLE IF NOT EXISTS urls\r\n@@ -47,8 +47,9 @@\n         urls = search_web(query, timelimit=timelimit_code)\r\n         all_urls = list(set(urls))[:MAX_URLS]\r\n \r\n         tag = f\"web_{query}_{timelimit}\"\r\n+        name = custom_name or f\"Web - {query} ({timelimit})\"\r\n         history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy\r\n         message = query\r\n         response = \"\"\r\n \r\n@@ -61,19 +62,19 @@\n \r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = \"Collection completed. New content added to vectorstore if applicable.\"\r\n         tasks[task_id]['tag'] = tag\r\n-        completed_collections.append({'name': f\"Web - {query} ({timelimit})\", 'tag': tag})\r\n+        completed_collections.append({'name': name, 'tag': tag})\r\n         print(f\"Web collection task {task_id} completed.\")\r\n     except Exception as e:\r\n         tasks[task_id]['status'] = 'error'\r\n         tasks[task_id]['message'] = str(e)\r\n         print(f\"Web collection task {task_id} error: {e}\")\r\n     finally:\r\n         conn.close()\r\n \r\n-def start_web_collection(query, timelimit, use_ollama, tasks, completed_collections):\r\n+def start_web_collection(custom_name, query, timelimit, use_ollama, tasks, completed_collections):\r\n     task_id = len(tasks)\r\n-    task = {'id': task_id, 'type': 'web', 'query': query, 'timelimit': timelimit, 'use_ollama': use_ollama, 'status': 'running', 'message': ''}\r\n+    task = {'id': task_id, 'type': 'web', 'custom_name': custom_name, 'query': query, 'timelimit': timelimit, 'use_ollama': use_ollama, 'status': 'running', 'message': ''}\r\n     tasks.append(task)\r\n-    threading.Thread(target=run_web_collection, args=(task_id, query, timelimit, use_ollama, tasks, completed_collections)).start()\r\n+    threading.Thread(target=run_web_collection, args=(task_id, custom_name, query, timelimit, use_ollama, tasks, completed_collections)).start()\r\n     return \"Web collection started in background.\", tasks, completed_collections\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756940982068,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -9,8 +9,9 @@\n import re\r\n from config import MAX_URLS, FAISS_PATH\r\n from process_utils import process_urls\r\n from utils import lock\r\n+from vectorstore_manager import get_vectorstore\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n def search_web(query, site=None, timelimit=None):\r\n"
                },
                {
                    "date": 1756941113924,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -9,9 +9,9 @@\n import re\r\n from config import MAX_URLS, FAISS_PATH\r\n from process_utils import process_urls\r\n from utils import lock\r\n-from vectorstore_manager import get_vectorstore\r\n+from vectorstore_utils import get_vectorstore\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n def search_web(query, site=None, timelimit=None):\r\n"
                },
                {
                    "date": 1756943230963,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,12 +6,14 @@\n import sqlite3\r\n from ddgs import DDGS\r\n from bs4 import BeautifulSoup\r\n import re\r\n-from config import MAX_URLS, FAISS_PATH\r\n+from config import MAX_URLS, FAISS_PATH, RAW_DIR\r\n from process_utils import process_urls\r\n from utils import lock\r\n-from vectorstore_utils import get_vectorstore\r\n+from vectorstore_manager import get_vectorstore\r\n+from urllib.parse import quote\r\n+import html\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n def search_web(query, site=None, timelimit=None):\r\n"
                },
                {
                    "date": 1756946387064,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,8 +10,9 @@\n from config import MAX_URLS, FAISS_PATH, RAW_DIR\r\n from process_utils import process_urls\r\n from utils import lock\r\n from vectorstore_manager import get_vectorstore\r\n+from db_utils import add_collection\r\n from urllib.parse import quote\r\n import html\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n@@ -62,8 +63,10 @@\n                 next(process_gen)\r\n         except StopIteration as e:\r\n             sources, response, history = e.value\r\n \r\n+        add_collection(conn, name, tag)  # Save to DB\r\n+\r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = \"Collection completed. New content added to vectorstore if applicable.\"\r\n         tasks[task_id]['tag'] = tag\r\n         completed_collections.append({'name': name, 'tag': tag})\r\n"
                },
                {
                    "date": 1756960130202,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -13,11 +13,22 @@\n from vectorstore_manager import get_vectorstore\r\n from db_utils import add_collection\r\n from urllib.parse import quote\r\n import html\r\n+import re  # Added for sanitization\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n+def sanitize_tag(name):\r\n+    # Replace invalid path characters with '_'\r\n+    invalid_chars = r'[<>:\"/\\\\|?*]'\r\n+    sanitized = re.sub(invalid_chars, '_', name)\r\n+    # Strip leading/trailing whitespace\r\n+    sanitized = sanitized.strip()\r\n+    # Collapse multiple '_' into single\r\n+    sanitized = re.sub(r'_+', '_', sanitized)\r\n+    return sanitized\r\n+\r\n def search_web(query, site=None, timelimit=None):\r\n     print(f\"Searching web for query: {query}\" + (f\" site:{site}\" if site else \"\") + (f\" timelimit:{timelimit}\" if timelimit else \"\"))\r\n     with DDGS() as ddgs:\r\n         search_query = query\r\n@@ -50,9 +61,11 @@\n         timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n         urls = search_web(query, timelimit=timelimit_code)\r\n         all_urls = list(set(urls))[:MAX_URLS]\r\n \r\n-        tag = f\"web_{query}_{timelimit}\"\r\n+        raw_tag = f\"web_{query}_{timelimit}\"\r\n+        tag = sanitize_tag(raw_tag)  # Sanitize to prevent invalid path characters\r\n+        print(f\"Debug: Sanitized tag from '{raw_tag}' to '{tag}'\")\r\n         name = custom_name or f\"Web - {query} ({timelimit})\"\r\n         history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy\r\n         message = query\r\n         response = \"\"\r\n"
                },
                {
                    "date": 1756961136819,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,9 +6,9 @@\n import sqlite3\r\n from ddgs import DDGS\r\n from bs4 import BeautifulSoup\r\n import re\r\n-from config import MAX_URLS, FAISS_PATH, RAW_DIR\r\n+from config import FAISS_PATH, RAW_DIR\r\n from process_utils import process_urls\r\n from utils import lock\r\n from vectorstore_manager import get_vectorstore\r\n from db_utils import add_collection\r\n@@ -40,9 +40,9 @@\n         urls = [result['href'] for result in results if 'href' in result]\r\n     print(f\"Found {len(urls)} URLs: {urls}\")\r\n     return urls\r\n \r\n-def run_web_collection(task_id, custom_name, query, timelimit, use_ollama, tasks, completed_collections):\r\n+def run_web_collection(task_id, custom_name, query, timelimit, max_urls, use_ollama, tasks, completed_collections):\r\n     print(f\"Starting Web collection task {task_id} for query: {query}\")\r\n     conn = sqlite3.connect('crawled.db')\r\n     c = conn.cursor()\r\n     c.execute('''CREATE TABLE IF NOT EXISTS urls\r\n@@ -59,9 +59,9 @@\n     conn.commit()\r\n     try:\r\n         timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n         urls = search_web(query, timelimit=timelimit_code)\r\n-        all_urls = list(set(urls))[:MAX_URLS]\r\n+        all_urls = list(set(urls))[:max_urls]\r\n \r\n         raw_tag = f\"web_{query}_{timelimit}\"\r\n         tag = sanitize_tag(raw_tag)  # Sanitize to prevent invalid path characters\r\n         print(f\"Debug: Sanitized tag from '{raw_tag}' to '{tag}'\")\r\n@@ -90,10 +90,10 @@\n         print(f\"Web collection task {task_id} error: {e}\")\r\n     finally:\r\n         conn.close()\r\n \r\n-def start_web_collection(custom_name, query, timelimit, use_ollama, tasks, completed_collections):\r\n+def start_web_collection(custom_name, query, timelimit, max_urls=10, use_ollama=False, tasks=None, completed_collections=None):\r\n     task_id = len(tasks)\r\n-    task = {'id': task_id, 'type': 'web', 'custom_name': custom_name, 'query': query, 'timelimit': timelimit, 'use_ollama': use_ollama, 'status': 'running', 'message': ''}\r\n+    task = {'id': task_id, 'type': 'web', 'custom_name': custom_name, 'query': query, 'timelimit': timelimit, 'max_urls': max_urls, 'use_ollama': use_ollama, 'status': 'running', 'message': ''}\r\n     tasks.append(task)\r\n-    threading.Thread(target=run_web_collection, args=(task_id, custom_name, query, timelimit, use_ollama, tasks, completed_collections)).start()\r\n+    threading.Thread(target=run_web_collection, args=(task_id, custom_name, query, timelimit, max_urls, use_ollama, tasks, completed_collections)).start()\r\n     return \"Web collection started in background.\", tasks, completed_collections\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756965591346,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,9 +6,9 @@\n import sqlite3\r\n from ddgs import DDGS\r\n from bs4 import BeautifulSoup\r\n import re\r\n-from config import FAISS_PATH, RAW_DIR\r\n+from config import MAX_URLS, FAISS_PATH, RAW_DIR\r\n from process_utils import process_urls\r\n from utils import lock\r\n from vectorstore_manager import get_vectorstore\r\n from db_utils import add_collection\r\n"
                },
                {
                    "date": 1756966134083,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,8 +14,10 @@\n from db_utils import add_collection\r\n from urllib.parse import quote\r\n import html\r\n import re  # Added for sanitization\r\n+from datetime import datetime  # Added for timestamp in consolidated file\r\n+from augment_utils import augment_chunk  # Import for augmentation\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n def sanitize_tag(name):\r\n@@ -76,8 +78,23 @@\n                 next(process_gen)\r\n         except StopIteration as e:\r\n             sources, response, history = e.value\r\n \r\n+        # Create consolidated file after processing\r\n+        current_time = datetime.now()\r\n+        timestamp_str = current_time.strftime(\"%H:%M:%d:%m:%Y\")\r\n+        prefix = \"augmented-\" if use_ollama else \"combined-\"\r\n+        consolidated_filename = f\"{prefix}{tag}-{timestamp_str}.txt\"\r\n+        consolidated_filepath = os.path.join(RAW_DIR, consolidated_filename)\r\n+        consolidated_content = \"\"\r\n+        for url in all_urls:\r\n+            content = get_stored_content(conn, url)\r\n+            if content:\r\n+                consolidated_content += f\"Content from {url}:\\n{content}\\n\\n\"\r\n+        with open(consolidated_filepath, \"w\", encoding=\"utf-8\") as f:\r\n+            f.write(consolidated_content)\r\n+        print(f\"Created consolidated file: {consolidated_filepath}\")\r\n+\r\n         add_collection(conn, name, tag)  # Save to DB\r\n \r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = \"Collection completed. New content added to vectorstore if applicable.\"\r\n"
                },
                {
                    "date": 1756966800729,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -30,40 +30,42 @@\n     sanitized = re.sub(r'_+', '_', sanitized)\r\n     return sanitized\r\n \r\n def search_web(query, site=None, timelimit=None):\r\n-    print(f\"Searching web for query: {query}\" + (f\" site:{site}\" if site else \"\") + (f\" timelimit:{timelimit}\" if timelimit else \"\"))\r\n+    print(f\"Debug: Searching web for query: {query}\" + (f\" site:{site}\" if site else \"\") + (f\" timelimit:{timelimit}\" if timelimit else \"\"))\r\n     with DDGS() as ddgs:\r\n         search_query = query\r\n         if 'lyrics' in query.lower():\r\n             search_query += \" site:genius.com OR site:azlyrics.com\"\r\n         if site:\r\n             search_query += f\" site:{site}\"\r\n         results = list(ddgs.text(search_query, max_results=10, timelimit=timelimit))\r\n         urls = [result['href'] for result in results if 'href' in result]\r\n-    print(f\"Found {len(urls)} URLs: {urls}\")\r\n+    print(f\"Debug: Found {len(urls)} URLs: {urls}\")\r\n     return urls\r\n \r\n def run_web_collection(task_id, custom_name, query, timelimit, max_urls, use_ollama, tasks, completed_collections):\r\n-    print(f\"Starting Web collection task {task_id} for query: {query}\")\r\n+    print(f\"Debug: Starting Web collection task {task_id} for query: {query}\")\r\n     conn = sqlite3.connect('crawled.db')\r\n     c = conn.cursor()\r\n     c.execute('''CREATE TABLE IF NOT EXISTS urls\r\n                  (url TEXT PRIMARY KEY, timestamp DATETIME, cleaned_text TEXT)''')\r\n     c.execute('''CREATE TABLE IF NOT EXISTS chunks\r\n                  (hash TEXT PRIMARY KEY, content TEXT, source TEXT, tag TEXT)''')\r\n     try:\r\n         c.execute(\"ALTER TABLE chunks ADD COLUMN tag TEXT\")\r\n-        print(\"Added 'tag' column to chunks table.\")\r\n+        print(\"Debug: Added 'tag' column to chunks table.\")\r\n     except sqlite3.OperationalError as e:\r\n         if \"duplicate column name\" not in str(e):\r\n             raise e\r\n-        print(\"'tag' column already exists in chunks table.\")\r\n+        print(\"Debug: 'tag' column already exists in chunks table.\")\r\n     conn.commit()\r\n     try:\r\n+        print(\"Debug: Mapping timelimit to code...\")\r\n         timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n         urls = search_web(query, timelimit=timelimit_code)\r\n         all_urls = list(set(urls))[:max_urls]\r\n+        print(f\"Debug: Filtered to {len(all_urls)} unique URLs.\")\r\n \r\n         raw_tag = f\"web_{query}_{timelimit}\"\r\n         tag = sanitize_tag(raw_tag)  # Sanitize to prevent invalid path characters\r\n         print(f\"Debug: Sanitized tag from '{raw_tag}' to '{tag}'\")\r\n@@ -71,16 +73,19 @@\n         history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy\r\n         message = query\r\n         response = \"\"\r\n \r\n+        print(\"Debug: Starting URL processing...\")\r\n         process_gen = process_urls(all_urls, response, history, message, conn=conn, source_tag=tag, use_ollama=use_ollama)\r\n         try:\r\n             while True:\r\n                 next(process_gen)\r\n         except StopIteration as e:\r\n             sources, response, history = e.value\r\n+        print(\"Debug: URL processing completed.\")\r\n \r\n         # Create consolidated file after processing\r\n+        print(\"Debug: Creating consolidated file...\")\r\n         current_time = datetime.now()\r\n         timestamp_str = current_time.strftime(\"%H:%M:%d:%m:%Y\")\r\n         prefix = \"augmented-\" if use_ollama else \"combined-\"\r\n         consolidated_filename = f\"{prefix}{tag}-{timestamp_str}.txt\"\r\n@@ -91,25 +96,26 @@\n             if content:\r\n                 consolidated_content += f\"Content from {url}:\\n{content}\\n\\n\"\r\n         with open(consolidated_filepath, \"w\", encoding=\"utf-8\") as f:\r\n             f.write(consolidated_content)\r\n-        print(f\"Created consolidated file: {consolidated_filepath}\")\r\n+        print(f\"Debug: Created consolidated file: {consolidated_filepath}\")\r\n \r\n         add_collection(conn, name, tag)  # Save to DB\r\n \r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = \"Collection completed. New content added to vectorstore if applicable.\"\r\n         tasks[task_id]['tag'] = tag\r\n         completed_collections.append({'name': name, 'tag': tag})\r\n-        print(f\"Web collection task {task_id} completed.\")\r\n+        print(f\"Debug: Web collection task {task_id} completed.\")\r\n     except Exception as e:\r\n         tasks[task_id]['status'] = 'error'\r\n         tasks[task_id]['message'] = str(e)\r\n-        print(f\"Web collection task {task_id} error: {e}\")\r\n+        print(f\"Debug: Web collection task {task_id} error: {e}\")\r\n     finally:\r\n         conn.close()\r\n \r\n def start_web_collection(custom_name, query, timelimit, max_urls=10, use_ollama=False, tasks=None, completed_collections=None):\r\n+    print(\"Debug: Starting web collection in background.\")\r\n     task_id = len(tasks)\r\n     task = {'id': task_id, 'type': 'web', 'custom_name': custom_name, 'query': query, 'timelimit': timelimit, 'max_urls': max_urls, 'use_ollama': use_ollama, 'status': 'running', 'message': ''}\r\n     tasks.append(task)\r\n     threading.Thread(target=run_web_collection, args=(task_id, custom_name, query, timelimit, max_urls, use_ollama, tasks, completed_collections)).start()\r\n"
                },
                {
                    "date": 1756967139465,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,9 +10,9 @@\n from config import MAX_URLS, FAISS_PATH, RAW_DIR\r\n from process_utils import process_urls\r\n from utils import lock\r\n from vectorstore_manager import get_vectorstore\r\n-from db_utils import add_collection\r\n+from db_utils import add_collection, get_stored_content  # Added get_stored_content\r\n from urllib.parse import quote\r\n import html\r\n import re  # Added for sanitization\r\n from datetime import datetime  # Added for timestamp in consolidated file\r\n"
                },
                {
                    "date": 1756967847143,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,12 +20,14 @@\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n def sanitize_tag(name):\r\n+    # Replace spaces with '_'\r\n+    name = re.sub(r'\\s+', '_', name)\r\n     # Replace invalid path characters with '_'\r\n     invalid_chars = r'[<>:\"/\\\\|?*]'\r\n     sanitized = re.sub(invalid_chars, '_', name)\r\n-    # Strip leading/trailing whitespace\r\n+    # Strip leading/trailing whitespace (though after sub, unlikely)\r\n     sanitized = sanitized.strip()\r\n     # Collapse multiple '_' into single\r\n     sanitized = re.sub(r'_+', '_', sanitized)\r\n     return sanitized\r\n@@ -85,17 +87,19 @@\n \r\n         # Create consolidated file after processing\r\n         print(\"Debug: Creating consolidated file...\")\r\n         current_time = datetime.now()\r\n-        timestamp_str = current_time.strftime(\"%H:%M:%d:%m:%Y\")\r\n+        timestamp_str = current_time.strftime(\"%H-%M-%d-%m-%Y\")  # Use dashes to avoid invalid characters\r\n         prefix = \"augmented-\" if use_ollama else \"combined-\"\r\n         consolidated_filename = f\"{prefix}{tag}-{timestamp_str}.txt\"\r\n         consolidated_filepath = os.path.join(RAW_DIR, consolidated_filename)\r\n         consolidated_content = \"\"\r\n         for url in all_urls:\r\n             content = get_stored_content(conn, url)\r\n             if content:\r\n                 consolidated_content += f\"Content from {url}:\\n{content}\\n\\n\"\r\n+            else:\r\n+                print(f\"Debug: No content found for {url} in consolidated file.\")\r\n         with open(consolidated_filepath, \"w\", encoding=\"utf-8\") as f:\r\n             f.write(consolidated_content)\r\n         print(f\"Debug: Created consolidated file: {consolidated_filepath}\")\r\n \r\n"
                }
            ],
            "date": 1756856975158,
            "name": "Commit-0",
            "content": "from ddgs import DDGS\r\nimport requests\r\nfrom bs4 import BeautifulSoup\r\nimport re\r\n\r\ndef search_web(query, site=None, timelimit=None):\r\n    print(f\"Searching web for query: {query}\" + (f\" site:{site}\" if site else \"\") + (f\" timelimit:{timelimit}\" if timelimit else \"\"))\r\n    with DDGS() as ddgs:\r\n        search_query = query\r\n        if site:\r\n            search_query += f\" site:{site}\"\r\n        results = list(ddgs.text(search_query, max_results=10, timelimit=timelimit))\r\n        urls = [result['href'] for result in results if 'href' in result]\r\n    print(f\"Found {len(urls)} URLs: {urls}\")\r\n    return urls\r\n\r\ndef clean_web_content(url):\r\n    print(f\"Fetching and cleaning URL: {url}\")\r\n    try:\r\n        response = requests.get(url, timeout=10)\r\n        response.raise_for_status()\r\n        html = response.text\r\n        soup = BeautifulSoup(html, 'html.parser')\r\n        for elem in soup.select('script, style, nav, header, footer, .ad, .advert, iframe, noscript'):\r\n            elem.extract()\r\n        main_content = soup.find('main') or soup.find('article') or soup\r\n        text = main_content.get_text(separator='\\n', strip=True)\r\n        text = re.sub(r'\\s+', ' ', text)\r\n        text = re.sub(r'[\\n\\t\\r]+', ' ', text)\r\n        text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\r\n        lines = [line.strip() for line in text.split('.') if len(line.strip()) > 20]\r\n        cleaned_text = '. '.join(lines)\r\n        print(f\"Cleaned content length for {url}: {len(cleaned_text)} characters\")\r\n        return cleaned_text\r\n    except Exception as e:\r\n        print(f\"Error cleaning {url}: {e}\")\r\n        return None"
        }
    ]
}