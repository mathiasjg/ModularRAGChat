{
    "sourceFile": "file_utils.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 9,
            "patches": [
                {
                    "date": 1756939613184,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1756939732748,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -107,8 +107,9 @@\n \r\n         # Store and add to vectorstore\r\n         store_content(conn, file_path, processed_text)  # Use file_path as 'url'\r\n         new_docs_total = 0\r\n+        new_docs = []  # Initialize new_docs here\r\n         for chunk in chunks:\r\n             if add_chunk_if_new(conn, chunk, file_path, tag=tag):\r\n                 metadata = {\"source\": file_path, \"tag\": tag}\r\n                 new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n"
                },
                {
                    "date": 1756940715945,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,9 +8,9 @@\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n from config import FAISS_PATH, RAW_DIR\r\n from db_utils import add_chunk_if_new, store_content, get_stored_content\r\n-from vectorstore_utils import vectorstore\r\n+from vectorstore_manager import get_vectorstore\r\n from utils import lock\r\n from urllib.parse import quote\r\n import html\r\n \r\n@@ -115,10 +115,11 @@\n                 new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n                 new_docs_total += 1\r\n         if new_docs_total > 0:\r\n             with lock:\r\n-                vectorstore.add_documents(new_docs)\r\n-                vectorstore.save_local(FAISS_PATH)\r\n+                vs = get_vectorstore(tag)\r\n+                vs.add_documents(new_docs)\r\n+                vs.save_local(os.path.join(FAISS_PATH, tag))\r\n \r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = f\"Ingestion completed. {new_docs_total} new chunks added. Please refresh sources in the Chat tab.\"\r\n         tasks[task_id]['tag'] = tag\r\n"
                },
                {
                    "date": 1756940979046,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,139 @@\n+# file_utils.py\r\n+import os\r\n+import threading\r\n+import sqlite3\r\n+import PyPDF2\r\n+import spacy\r\n+import requests\r\n+from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n+from langchain_core.documents import Document\r\n+from config import FAISS_PATH, RAW_DIR\r\n+from db_utils import add_chunk_if_new, store_content, get_stored_content\r\n+from vectorstore_utils import get_vectorstore\r\n+from utils import lock\r\n+from urllib.parse import quote\r\n+import html\r\n+\r\n+nlp = spacy.load(\"en_core_web_sm\")\r\n+\r\n+def extract_text_from_file(file_path):\r\n+    if file_path.lower().endswith('.txt'):\r\n+        with open(file_path, 'r', encoding='utf-8') as f:\r\n+            text = f.read()\r\n+    elif file_path.lower().endswith('.pdf'):\r\n+        with open(file_path, 'rb') as file:\r\n+            reader = PyPDF2.PdfReader(file)\r\n+            text = ''\r\n+            for page in reader.pages:\r\n+                text += page.extract_text() + '\\n'\r\n+    else:\r\n+        raise ValueError(\"Unsupported file type. Only TXT and PDF are supported.\")\r\n+    return text\r\n+\r\n+def process_file_content(text, use_ollama=False):\r\n+    # NLP Processing\r\n+    doc = nlp(text)\r\n+    processed_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.text.strip()]\r\n+    processed_text = ' '.join(processed_tokens)\r\n+\r\n+    # Chunking\r\n+    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\r\n+    chunk_size = 200  # Words per chunk\r\n+    chunks = []\r\n+    current_chunk = []\r\n+    current_word_count = 0\r\n+    for sent in sentences:\r\n+        word_count = len(sent.split())\r\n+        if current_word_count + word_count > chunk_size:\r\n+            chunks.append(' '.join(current_chunk))\r\n+            current_chunk = [sent]\r\n+            current_word_count = word_count\r\n+        else:\r\n+            current_chunk.append(sent)\r\n+            current_word_count += word_count\r\n+    if current_chunk:\r\n+        chunks.append(' '.join(current_chunk))\r\n+\r\n+    # Optional Ollama enhancement\r\n+    if use_ollama:\r\n+        ollama_url = \"http://localhost:11434/api/generate\"\r\n+        enhanced_chunks = []\r\n+        for i, chunk in enumerate(chunks):\r\n+            payload = {\r\n+                \"model\": \"qwen2.5:7b\",\r\n+                \"prompt\": f\"Enhance and correct this content chunk for clarity and accuracy: {chunk}. Include only the corrected text, do not include a summarization of the changes.\",\r\n+                \"stream\": False\r\n+            }\r\n+            try:\r\n+                response = requests.post(ollama_url, json=payload, timeout=30)\r\n+                if response.status_code == 200:\r\n+                    enhanced_text = response.json()['response']\r\n+                    enhanced_chunks.append(enhanced_text)\r\n+                else:\r\n+                    enhanced_chunks.append(chunk)  # Fallback\r\n+            except requests.exceptions.RequestException as e:\r\n+                enhanced_chunks.append(chunk)\r\n+        processed_text = '\\n\\n'.join(enhanced_chunks)\r\n+    return processed_text, chunks\r\n+\r\n+def run_file_ingestion(task_id, custom_name, file_path, use_ollama, tasks, completed_collections):\r\n+    print(f\"Starting File ingestion task {task_id} for file: {file_path}\")\r\n+    conn = sqlite3.connect('crawled.db')\r\n+    c = conn.cursor()\r\n+    c.execute('''CREATE TABLE IF NOT EXISTS urls\r\n+                 (url TEXT PRIMARY KEY, timestamp DATETIME, cleaned_text TEXT)''')\r\n+    c.execute('''CREATE TABLE IF NOT EXISTS chunks\r\n+                 (hash TEXT PRIMARY KEY, content TEXT, source TEXT, tag TEXT)''')\r\n+    try:\r\n+        c.execute(\"ALTER TABLE chunks ADD COLUMN tag TEXT\")\r\n+        print(\"Added 'tag' column to chunks table.\")\r\n+    except sqlite3.OperationalError as e:\r\n+        if \"duplicate column name\" not in str(e):\r\n+            raise e\r\n+        print(\"'tag' column already exists in chunks table.\")\r\n+    conn.commit()\r\n+    try:\r\n+        response = \"\"\r\n+        tag = custom_name.replace(\" \", \"_\") if custom_name else os.path.basename(file_path).replace(\" \", \"_\")\r\n+        name = custom_name or os.path.basename(file_path)\r\n+\r\n+        # Extract text\r\n+        text = extract_text_from_file(file_path)\r\n+        response += f\"Extracted text from file: {len(text)} characters.\\n\"\r\n+\r\n+        # Process content\r\n+        processed_text, chunks = process_file_content(text, use_ollama)\r\n+        response += f\"Processed text: {len(processed_text)} characters, {len(chunks)} chunks.\\n\"\r\n+\r\n+        # Store and add to vectorstore\r\n+        store_content(conn, file_path, processed_text)  # Use file_path as 'url'\r\n+        new_docs_total = 0\r\n+        new_docs = []  # Initialize new_docs here\r\n+        for chunk in chunks:\r\n+            if add_chunk_if_new(conn, chunk, file_path, tag=tag):\r\n+                metadata = {\"source\": file_path, \"tag\": tag}\r\n+                new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+                new_docs_total += 1\r\n+        if new_docs_total > 0:\r\n+            with lock:\r\n+                vs = get_vectorstore(tag)\r\n+                vs.add_documents(new_docs)\r\n+\r\n+        tasks[task_id]['status'] = 'completed'\r\n+        tasks[task_id]['message'] = f\"Ingestion completed. {new_docs_total} new chunks added. Please refresh sources in the Chat tab.\"\r\n+        tasks[task_id]['tag'] = tag\r\n+        completed_collections.append({'name': name, 'tag': tag})\r\n+        print(f\"File ingestion task {task_id} completed.\")\r\n+    except Exception as e:\r\n+        tasks[task_id]['status'] = 'error'\r\n+        tasks[task_id]['message'] = str(e)\r\n+        print(f\"File ingestion task {task_id} error: {e}\")\r\n+    finally:\r\n+        conn.close()\r\n+\r\n+def start_file_ingestion(custom_name, file_path, use_ollama, tasks, completed_collections):\r\n+    task_id = len(tasks)\r\n+    task = {'id': task_id, 'type': 'file', 'custom_name': custom_name, 'file_path': file_path, 'use_ollama': use_ollama, 'status': 'running', 'message': ''}\r\n+    tasks.append(task)\r\n+    threading.Thread(target=run_file_ingestion, args=(task_id, custom_name, file_path, use_ollama, tasks, completed_collections)).start()\r\n+    return \"File ingestion started in background.\", tasks, completed_collections\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756943120471,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,9 +8,9 @@\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n from config import FAISS_PATH, RAW_DIR\r\n from db_utils import add_chunk_if_new, store_content, get_stored_content\r\n-from vectorstore_utils import get_vectorstore\r\n+from vectorstore_manager import get_vectorstore\r\n from utils import lock\r\n from urllib.parse import quote\r\n import html\r\n \r\n@@ -104,147 +104,25 @@\n         # Process content\r\n         processed_text, chunks = process_file_content(text, use_ollama)\r\n         response += f\"Processed text: {len(processed_text)} characters, {len(chunks)} chunks.\\n\"\r\n \r\n-        # Store and add to vectorstore\r\n-        store_content(conn, file_path, processed_text)  # Use file_path as 'url'\r\n-        new_docs_total = 0\r\n-        new_docs = []  # Initialize new_docs here\r\n-        for chunk in chunks:\r\n-            if add_chunk_if_new(conn, chunk, file_path, tag=tag):\r\n-                metadata = {\"source\": file_path, \"tag\": tag}\r\n-                new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n-                new_docs_total += 1\r\n-        if new_docs_total > 0:\r\n-            with lock:\r\n-                vs = get_vectorstore(tag)\r\n-                vs.add_documents(new_docs)\r\n+        # Save processed text to raw_contents\r\n+        prefix = \"ollama_\" if use_ollama else \"\"\r\n+        safe_filename = prefix + quote(os.path.basename(file_path)[:100]) + \".txt\"\r\n+        filepath = os.path.join(RAW_DIR, safe_filename)\r\n+        with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n+            f.write(processed_text)\r\n+        response += f\"Saved processed text to {filepath}\\n\"\r\n \r\n-        tasks[task_id]['status'] = 'completed'\r\n-        tasks[task_id]['message'] = f\"Ingestion completed. {new_docs_total} new chunks added. Please refresh sources in the Chat tab.\"\r\n-        tasks[task_id]['tag'] = tag\r\n-        completed_collections.append({'name': name, 'tag': tag})\r\n-        print(f\"File ingestion task {task_id} completed.\")\r\n-    except Exception as e:\r\n-        tasks[task_id]['status'] = 'error'\r\n-        tasks[task_id]['message'] = str(e)\r\n-        print(f\"File ingestion task {task_id} error: {e}\")\r\n-    finally:\r\n-        conn.close()\r\n+        # HTML view\r\n+        html_safe_filename = safe_filename.replace(\".txt\", \".html\")\r\n+        html_filepath = os.path.join(RAW_DIR, html_safe_filename)\r\n+        escaped_text = html.escape(processed_text)\r\n+        html_content = f\"\"\"<html><head><style>body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }} pre {{ white-space: pre-wrap; word-wrap: break-word; }}</style></head><body><h1>Processed Content for {os.path.basename(file_path)}</h1><pre>{escaped_text}</pre></body></html>\"\"\"\r\n+        with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n+            f.write(html_content)\r\n+        response += f\"Saved HTML view to {html_filepath}\\n\"\r\n \r\n-def start_file_ingestion(custom_name, file_path, use_ollama, tasks, completed_collections):\r\n-    task_id = len(tasks)\r\n-    task = {'id': task_id, 'type': 'file', 'custom_name': custom_name, 'file_path': file_path, 'use_ollama': use_ollama, 'status': 'running', 'message': ''}\r\n-    tasks.append(task)\r\n-    threading.Thread(target=run_file_ingestion, args=(task_id, custom_name, file_path, use_ollama, tasks, completed_collections)).start()\r\n-    return \"File ingestion started in background.\", tasks, completed_collections\n-# file_utils.py\r\n-import os\r\n-import threading\r\n-import sqlite3\r\n-import PyPDF2\r\n-import spacy\r\n-import requests\r\n-from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n-from langchain_core.documents import Document\r\n-from config import FAISS_PATH, RAW_DIR\r\n-from db_utils import add_chunk_if_new, store_content, get_stored_content\r\n-from vectorstore_manager import get_vectorstore\r\n-from utils import lock\r\n-from urllib.parse import quote\r\n-import html\r\n-\r\n-nlp = spacy.load(\"en_core_web_sm\")\r\n-\r\n-def extract_text_from_file(file_path):\r\n-    if file_path.lower().endswith('.txt'):\r\n-        with open(file_path, 'r', encoding='utf-8') as f:\r\n-            text = f.read()\r\n-    elif file_path.lower().endswith('.pdf'):\r\n-        with open(file_path, 'rb') as file:\r\n-            reader = PyPDF2.PdfReader(file)\r\n-            text = ''\r\n-            for page in reader.pages:\r\n-                text += page.extract_text() + '\\n'\r\n-    else:\r\n-        raise ValueError(\"Unsupported file type. Only TXT and PDF are supported.\")\r\n-    return text\r\n-\r\n-def process_file_content(text, use_ollama=False):\r\n-    # NLP Processing\r\n-    doc = nlp(text)\r\n-    processed_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.text.strip()]\r\n-    processed_text = ' '.join(processed_tokens)\r\n-\r\n-    # Chunking\r\n-    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\r\n-    chunk_size = 200  # Words per chunk\r\n-    chunks = []\r\n-    current_chunk = []\r\n-    current_word_count = 0\r\n-    for sent in sentences:\r\n-        word_count = len(sent.split())\r\n-        if current_word_count + word_count > chunk_size:\r\n-            chunks.append(' '.join(current_chunk))\r\n-            current_chunk = [sent]\r\n-            current_word_count = word_count\r\n-        else:\r\n-            current_chunk.append(sent)\r\n-            current_word_count += word_count\r\n-    if current_chunk:\r\n-        chunks.append(' '.join(current_chunk))\r\n-\r\n-    # Optional Ollama enhancement\r\n-    if use_ollama:\r\n-        ollama_url = \"http://localhost:11434/api/generate\"\r\n-        enhanced_chunks = []\r\n-        for i, chunk in enumerate(chunks):\r\n-            payload = {\r\n-                \"model\": \"qwen2.5:7b\",\r\n-                \"prompt\": f\"Enhance and correct this content chunk for clarity and accuracy: {chunk}. Include only the corrected text, do not include a summarization of the changes.\",\r\n-                \"stream\": False\r\n-            }\r\n-            try:\r\n-                response = requests.post(ollama_url, json=payload, timeout=30)\r\n-                if response.status_code == 200:\r\n-                    enhanced_text = response.json()['response']\r\n-                    enhanced_chunks.append(enhanced_text)\r\n-                else:\r\n-                    enhanced_chunks.append(chunk)  # Fallback\r\n-            except requests.exceptions.RequestException as e:\r\n-                enhanced_chunks.append(chunk)\r\n-        processed_text = '\\n\\n'.join(enhanced_chunks)\r\n-    return processed_text, chunks\r\n-\r\n-def run_file_ingestion(task_id, custom_name, file_path, use_ollama, tasks, completed_collections):\r\n-    print(f\"Starting File ingestion task {task_id} for file: {file_path}\")\r\n-    conn = sqlite3.connect('crawled.db')\r\n-    c = conn.cursor()\r\n-    c.execute('''CREATE TABLE IF NOT EXISTS urls\r\n-                 (url TEXT PRIMARY KEY, timestamp DATETIME, cleaned_text TEXT)''')\r\n-    c.execute('''CREATE TABLE IF NOT EXISTS chunks\r\n-                 (hash TEXT PRIMARY KEY, content TEXT, source TEXT, tag TEXT)''')\r\n-    try:\r\n-        c.execute(\"ALTER TABLE chunks ADD COLUMN tag TEXT\")\r\n-        print(\"Added 'tag' column to chunks table.\")\r\n-    except sqlite3.OperationalError as e:\r\n-        if \"duplicate column name\" not in str(e):\r\n-            raise e\r\n-        print(\"'tag' column already exists in chunks table.\")\r\n-    conn.commit()\r\n-    try:\r\n-        response = \"\"\r\n-        tag = custom_name.replace(\" \", \"_\") if custom_name else os.path.basename(file_path).replace(\" \", \"_\")\r\n-        name = custom_name or os.path.basename(file_path)\r\n-\r\n-        # Extract text\r\n-        text = extract_text_from_file(file_path)\r\n-        response += f\"Extracted text from file: {len(text)} characters.\\n\"\r\n-\r\n-        # Process content\r\n-        processed_text, chunks = process_file_content(text, use_ollama)\r\n-        response += f\"Processed text: {len(processed_text)} characters, {len(chunks)} chunks.\\n\"\r\n-\r\n         # Store and add to vectorstore\r\n         store_content(conn, file_path, processed_text)  # Use file_path as 'url'\r\n         new_docs_total = 0\r\n         new_docs = []  # Initialize new_docs here\r\n@@ -256,9 +134,8 @@\n         if new_docs_total > 0:\r\n             with lock:\r\n                 vs = get_vectorstore(tag)\r\n                 vs.add_documents(new_docs)\r\n-                vs.save_local(os.path.join(FAISS_PATH, tag))\r\n \r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = f\"Ingestion completed. {new_docs_total} new chunks added. Please refresh sources in the Chat tab.\"\r\n         tasks[task_id]['tag'] = tag\r\n"
                },
                {
                    "date": 1756945445043,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,9 +7,9 @@\n import requests\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n from config import FAISS_PATH, RAW_DIR\r\n-from db_utils import add_chunk_if_new, store_content, get_stored_content\r\n+from db_utils import add_chunk_if_new, store_content, get_stored_content, add_collection\r\n from vectorstore_manager import get_vectorstore\r\n from utils import lock\r\n from urllib.parse import quote\r\n import html\r\n@@ -135,8 +135,9 @@\n             with lock:\r\n                 vs = get_vectorstore(tag)\r\n                 vs.add_documents(new_docs)\r\n \r\n+        add_collection(conn, name, tag)\r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = f\"Ingestion completed. {new_docs_total} new chunks added. Please refresh sources in the Chat tab.\"\r\n         tasks[task_id]['tag'] = tag\r\n         completed_collections.append({'name': name, 'tag': tag})\r\n"
                },
                {
                    "date": 1756946360061,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -104,24 +104,23 @@\n         # Process content\r\n         processed_text, chunks = process_file_content(text, use_ollama)\r\n         response += f\"Processed text: {len(processed_text)} characters, {len(chunks)} chunks.\\n\"\r\n \r\n-        # Save processed text to raw_contents\r\n+        # Save consolidated text (since single file, it's the processed_text)\r\n         prefix = \"ollama_\" if use_ollama else \"\"\r\n         safe_filename = prefix + quote(os.path.basename(file_path)[:100]) + \".txt\"\r\n         filepath = os.path.join(RAW_DIR, safe_filename)\r\n         with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n             f.write(processed_text)\r\n-        response += f\"Saved processed text to {filepath}\\n\"\r\n+        response += f\"Saved consolidated text to {filepath}\\n\"\r\n \r\n         # HTML view\r\n         html_safe_filename = safe_filename.replace(\".txt\", \".html\")\r\n         html_filepath = os.path.join(RAW_DIR, html_safe_filename)\r\n         escaped_text = html.escape(processed_text)\r\n-        html_content = f\"\"\"<html><head><style>body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }} pre {{ white-space: pre-wrap; word-wrap: break-word; }}</style></head><body><h1>Processed Content for {os.path.basename(file_path)}</h1><pre>{escaped_text}</pre></body></html>\"\"\"\r\n+        html_content = f\"\"\"<html><head><style>body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }} pre {{ white-space: pre-wrap; word-wrap: break-word; }}</style></head><body><h1>Consolidated Content for {os.path.basename(file_path)}</h1><pre>{escaped_text}</pre></body></html>\"\"\"\r\n         with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n             f.write(html_content)\r\n-        response += f\"Saved HTML view to {html_filepath}\\n\"\r\n \r\n         # Store and add to vectorstore\r\n         store_content(conn, file_path, processed_text)  # Use file_path as 'url'\r\n         new_docs_total = 0\r\n@@ -135,9 +134,10 @@\n             with lock:\r\n                 vs = get_vectorstore(tag)\r\n                 vs.add_documents(new_docs)\r\n \r\n-        add_collection(conn, name, tag)\r\n+        add_collection(conn, name, tag)  # Save to DB\r\n+\r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = f\"Ingestion completed. {new_docs_total} new chunks added. Please refresh sources in the Chat tab.\"\r\n         tasks[task_id]['tag'] = tag\r\n         completed_collections.append({'name': name, 'tag': tag})\r\n"
                },
                {
                    "date": 1756954193648,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -81,10 +81,12 @@\n     conn = sqlite3.connect('crawled.db')\r\n     c = conn.cursor()\r\n     c.execute('''CREATE TABLE IF NOT EXISTS urls\r\n                  (url TEXT PRIMARY KEY, timestamp DATETIME, cleaned_text TEXT)''')\r\n+    print(\"URLs table ensured.\")\r\n     c.execute('''CREATE TABLE IF NOT EXISTS chunks\r\n                  (hash TEXT PRIMARY KEY, content TEXT, source TEXT, tag TEXT)''')\r\n+    print(\"Chunks table ensured.\")\r\n     try:\r\n         c.execute(\"ALTER TABLE chunks ADD COLUMN tag TEXT\")\r\n         print(\"Added 'tag' column to chunks table.\")\r\n     except sqlite3.OperationalError as e:\r\n@@ -133,8 +135,11 @@\n         if new_docs_total > 0:\r\n             with lock:\r\n                 vs = get_vectorstore(tag)\r\n                 vs.add_documents(new_docs)\r\n+                print(f\"Added {new_docs_total} documents to vectorstore for tag {tag}.\")\r\n+        else:\r\n+            print(f\"No new documents added for tag {tag}.\")\r\n \r\n         add_collection(conn, name, tag)  # Save to DB\r\n \r\n         tasks[task_id]['status'] = 'completed'\r\n"
                },
                {
                    "date": 1756959805871,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -135,9 +135,13 @@\n         if new_docs_total > 0:\r\n             with lock:\r\n                 vs = get_vectorstore(tag)\r\n                 vs.add_documents(new_docs)\r\n-                print(f\"Added {new_docs_total} documents to vectorstore for tag {tag}.\")\r\n+                print(f\"Debug: Added {new_docs_total} documents to vectorstore for tag {tag}. ntotal after add: {vs.index.ntotal}\")\r\n+                # Save the vectorstore to disk after adding documents\r\n+                save_path = os.path.join(FAISS_PATH, tag)\r\n+                vs.save_local(save_path)\r\n+                print(f\"Debug: Saved vectorstore for tag {tag} to {save_path}.\")\r\n         else:\r\n             print(f\"No new documents added for tag {tag}.\")\r\n \r\n         add_collection(conn, name, tag)  # Save to DB\r\n"
                },
                {
                    "date": 1756960138212,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,11 +12,22 @@\n from vectorstore_manager import get_vectorstore\r\n from utils import lock\r\n from urllib.parse import quote\r\n import html\r\n+import re  # Added for sanitization\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n+def sanitize_tag(name):\r\n+    # Replace invalid path characters with '_'\r\n+    invalid_chars = r'[<>:\"/\\\\|?*]'\r\n+    sanitized = re.sub(invalid_chars, '_', name)\r\n+    # Strip leading/trailing whitespace\r\n+    sanitized = sanitized.strip()\r\n+    # Collapse multiple '_' into single\r\n+    sanitized = re.sub(r'_+', '_', sanitized)\r\n+    return sanitized\r\n+\r\n def extract_text_from_file(file_path):\r\n     if file_path.lower().endswith('.txt'):\r\n         with open(file_path, 'r', encoding='utf-8') as f:\r\n             text = f.read()\r\n@@ -95,9 +106,11 @@\n         print(\"'tag' column already exists in chunks table.\")\r\n     conn.commit()\r\n     try:\r\n         response = \"\"\r\n-        tag = custom_name.replace(\" \", \"_\") if custom_name else os.path.basename(file_path).replace(\" \", \"_\")\r\n+        raw_tag = custom_name if custom_name else os.path.basename(file_path)\r\n+        tag = sanitize_tag(raw_tag.replace(\" \", \"_\"))  # Sanitize after replacing spaces\r\n+        print(f\"Debug: Sanitized tag from '{raw_tag}' to '{tag}'\")\r\n         name = custom_name or os.path.basename(file_path)\r\n \r\n         # Extract text\r\n         text = extract_text_from_file(file_path)\r\n"
                }
            ],
            "date": 1756939613184,
            "name": "Commit-0",
            "content": "# file_utils.py\r\nimport os\r\nimport threading\r\nimport sqlite3\r\nimport PyPDF2\r\nimport spacy\r\nimport requests\r\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\r\nfrom langchain_core.documents import Document\r\nfrom config import FAISS_PATH, RAW_DIR\r\nfrom db_utils import add_chunk_if_new, store_content, get_stored_content\r\nfrom vectorstore_utils import vectorstore\r\nfrom utils import lock\r\nfrom urllib.parse import quote\r\nimport html\r\n\r\nnlp = spacy.load(\"en_core_web_sm\")\r\n\r\ndef extract_text_from_file(file_path):\r\n    if file_path.lower().endswith('.txt'):\r\n        with open(file_path, 'r', encoding='utf-8') as f:\r\n            text = f.read()\r\n    elif file_path.lower().endswith('.pdf'):\r\n        with open(file_path, 'rb') as file:\r\n            reader = PyPDF2.PdfReader(file)\r\n            text = ''\r\n            for page in reader.pages:\r\n                text += page.extract_text() + '\\n'\r\n    else:\r\n        raise ValueError(\"Unsupported file type. Only TXT and PDF are supported.\")\r\n    return text\r\n\r\ndef process_file_content(text, use_ollama=False):\r\n    # NLP Processing\r\n    doc = nlp(text)\r\n    processed_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.text.strip()]\r\n    processed_text = ' '.join(processed_tokens)\r\n\r\n    # Chunking\r\n    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\r\n    chunk_size = 200  # Words per chunk\r\n    chunks = []\r\n    current_chunk = []\r\n    current_word_count = 0\r\n    for sent in sentences:\r\n        word_count = len(sent.split())\r\n        if current_word_count + word_count > chunk_size:\r\n            chunks.append(' '.join(current_chunk))\r\n            current_chunk = [sent]\r\n            current_word_count = word_count\r\n        else:\r\n            current_chunk.append(sent)\r\n            current_word_count += word_count\r\n    if current_chunk:\r\n        chunks.append(' '.join(current_chunk))\r\n\r\n    # Optional Ollama enhancement\r\n    if use_ollama:\r\n        ollama_url = \"http://localhost:11434/api/generate\"\r\n        enhanced_chunks = []\r\n        for i, chunk in enumerate(chunks):\r\n            payload = {\r\n                \"model\": \"qwen2.5:7b\",\r\n                \"prompt\": f\"Enhance and correct this content chunk for clarity and accuracy: {chunk}. Include only the corrected text, do not include a summarization of the changes.\",\r\n                \"stream\": False\r\n            }\r\n            try:\r\n                response = requests.post(ollama_url, json=payload, timeout=30)\r\n                if response.status_code == 200:\r\n                    enhanced_text = response.json()['response']\r\n                    enhanced_chunks.append(enhanced_text)\r\n                else:\r\n                    enhanced_chunks.append(chunk)  # Fallback\r\n            except requests.exceptions.RequestException as e:\r\n                enhanced_chunks.append(chunk)\r\n        processed_text = '\\n\\n'.join(enhanced_chunks)\r\n    return processed_text, chunks\r\n\r\ndef run_file_ingestion(task_id, custom_name, file_path, use_ollama, tasks, completed_collections):\r\n    print(f\"Starting File ingestion task {task_id} for file: {file_path}\")\r\n    conn = sqlite3.connect('crawled.db')\r\n    c = conn.cursor()\r\n    c.execute('''CREATE TABLE IF NOT EXISTS urls\r\n                 (url TEXT PRIMARY KEY, timestamp DATETIME, cleaned_text TEXT)''')\r\n    c.execute('''CREATE TABLE IF NOT EXISTS chunks\r\n                 (hash TEXT PRIMARY KEY, content TEXT, source TEXT, tag TEXT)''')\r\n    try:\r\n        c.execute(\"ALTER TABLE chunks ADD COLUMN tag TEXT\")\r\n        print(\"Added 'tag' column to chunks table.\")\r\n    except sqlite3.OperationalError as e:\r\n        if \"duplicate column name\" not in str(e):\r\n            raise e\r\n        print(\"'tag' column already exists in chunks table.\")\r\n    conn.commit()\r\n    try:\r\n        response = \"\"\r\n        tag = custom_name.replace(\" \", \"_\") if custom_name else os.path.basename(file_path).replace(\" \", \"_\")\r\n        name = custom_name or os.path.basename(file_path)\r\n\r\n        # Extract text\r\n        text = extract_text_from_file(file_path)\r\n        response += f\"Extracted text from file: {len(text)} characters.\\n\"\r\n\r\n        # Process content\r\n        processed_text, chunks = process_file_content(text, use_ollama)\r\n        response += f\"Processed text: {len(processed_text)} characters, {len(chunks)} chunks.\\n\"\r\n\r\n        # Store and add to vectorstore\r\n        store_content(conn, file_path, processed_text)  # Use file_path as 'url'\r\n        new_docs_total = 0\r\n        for chunk in chunks:\r\n            if add_chunk_if_new(conn, chunk, file_path, tag=tag):\r\n                metadata = {\"source\": file_path, \"tag\": tag}\r\n                new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n                new_docs_total += 1\r\n        if new_docs_total > 0:\r\n            with lock:\r\n                vectorstore.add_documents(new_docs)\r\n                vectorstore.save_local(FAISS_PATH)\r\n\r\n        tasks[task_id]['status'] = 'completed'\r\n        tasks[task_id]['message'] = f\"Ingestion completed. {new_docs_total} new chunks added. Please refresh sources in the Chat tab.\"\r\n        tasks[task_id]['tag'] = tag\r\n        completed_collections.append({'name': name, 'tag': tag})\r\n        print(f\"File ingestion task {task_id} completed.\")\r\n    except Exception as e:\r\n        tasks[task_id]['status'] = 'error'\r\n        tasks[task_id]['message'] = str(e)\r\n        print(f\"File ingestion task {task_id} error: {e}\")\r\n    finally:\r\n        conn.close()\r\n\r\ndef start_file_ingestion(custom_name, file_path, use_ollama, tasks, completed_collections):\r\n    task_id = len(tasks)\r\n    task = {'id': task_id, 'type': 'file', 'custom_name': custom_name, 'file_path': file_path, 'use_ollama': use_ollama, 'status': 'running', 'message': ''}\r\n    tasks.append(task)\r\n    threading.Thread(target=run_file_ingestion, args=(task_id, custom_name, file_path, use_ollama, tasks, completed_collections)).start()\r\n    return \"File ingestion started in background.\", tasks, completed_collections"
        }
    ]
}