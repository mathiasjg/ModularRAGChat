{
    "sourceFile": "reddit_utils.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 10,
            "patches": [
                {
                    "date": 1756935541890,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1756936234189,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,19 +1,33 @@\n # reddit_utils.py\r\n import threading\r\n import requests\r\n+import sqlite3\r\n from config import MAX_URLS, FAISS_PATH\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n from web_utils import search_web\r\n-from process_utils import process_urls\r\n from db_utils import add_chunk_if_new\r\n from vectorstore_utils import vectorstore\r\n from utils import lock\r\n \r\n-def run_reddit_collection(task_id, query, timelimit, use_ollama, max_comments, tasks, completed_collections, conn=None):\r\n+def run_reddit_collection(task_id, query, timelimit, use_ollama, max_comments, tasks, completed_collections):\r\n     print(f\"Starting Reddit collection task {task_id} for query: {query}\")\r\n+    conn = sqlite3.connect('crawled.db')\r\n+    c = conn.cursor()\r\n+    c.execute('''CREATE TABLE IF NOT EXISTS urls\r\n+                 (url TEXT PRIMARY KEY, timestamp DATETIME, cleaned_text TEXT)''')\r\n+    c.execute('''CREATE TABLE IF NOT EXISTS chunks\r\n+                 (hash TEXT PRIMARY KEY, content TEXT, source TEXT, tag TEXT)''')\r\n     try:\r\n+        c.execute(\"ALTER TABLE chunks ADD COLUMN tag TEXT\")\r\n+        print(\"Added 'tag' column to chunks table.\")\r\n+    except sqlite3.OperationalError as e:\r\n+        if \"duplicate column name\" not in str(e):\r\n+            raise e\r\n+        print(\"'tag' column already exists in chunks table.\")\r\n+    conn.commit()\r\n+    try:\r\n         site = \"reddit.com\"\r\n         timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n         urls = search_web(query, site=site, timelimit=timelimit_code)\r\n         all_urls = list(set(urls))[:MAX_URLS]\r\n@@ -33,10 +47,9 @@\n                     post_text = data[0]['data']['children'][0]['data']['selftext']\r\n                     comments = data[1]['data']['children']\r\n                     comment_texts = [comment['data']['body'] for comment in comments[:max_comments] if 'body' in comment['data']]\r\n                     full_text = post_text + \" \".join(comment_texts)\r\n-                    # Process with clean_web_content logic if needed, but since it's JSON, use directly\r\n-                    # Assuming process_urls handles it, but for Reddit, we can store full_text\r\n+                    # Assuming no further processing needed for now, store full_text\r\n                     store_content(conn, url, full_text)\r\n                     response += f\"Fetched post and comments for {url}\\n\"\r\n                 else:\r\n                     response += f\"Failed to fetch for {url}\\n\"\r\n@@ -69,11 +82,13 @@\n     except Exception as e:\r\n         tasks[task_id]['status'] = 'error'\r\n         tasks[task_id]['message'] = str(e)\r\n         print(f\"Reddit collection task {task_id} error: {e}\")\r\n+    finally:\r\n+        conn.close()\r\n \r\n-def start_reddit_collection(query, timelimit, use_ollama, max_comments, tasks, completed_collections, conn=None):\r\n+def start_reddit_collection(query, timelimit, use_ollama, max_comments, tasks, completed_collections):\r\n     task_id = len(tasks)\r\n     task = {'id': task_id, 'type': 'reddit', 'query': query, 'timelimit': timelimit, 'use_ollama': use_ollama, 'max_comments': max_comments, 'status': 'running', 'message': ''}\r\n     tasks.append(task)\r\n-    threading.Thread(target=run_reddit_collection, args=(task_id, query, timelimit, use_ollama, max_comments, tasks, completed_collections, conn)).start()\r\n+    threading.Thread(target=run_reddit_collection, args=(task_id, query, timelimit, use_ollama, max_comments, tasks, completed_collections)).start()\r\n     return \"Reddit collection started in background.\", tasks, completed_collections\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756937069054,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,9 +5,9 @@\n from config import MAX_URLS, FAISS_PATH\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n from web_utils import search_web\r\n-from db_utils import add_chunk_if_new\r\n+from db_utils import add_chunk_if_new, store_content, get_stored_content\r\n from vectorstore_utils import vectorstore\r\n from utils import lock\r\n \r\n def run_reddit_collection(task_id, query, timelimit, use_ollama, max_comments, tasks, completed_collections):\r\n"
                },
                {
                    "date": 1756938050487,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -9,9 +9,9 @@\n from db_utils import add_chunk_if_new, store_content, get_stored_content\r\n from vectorstore_utils import vectorstore\r\n from utils import lock\r\n \r\n-def run_reddit_collection(task_id, query, timelimit, use_ollama, max_comments, tasks, completed_collections):\r\n+def run_reddit_collection(task_id, custom_name, query, timelimit, use_ollama, max_comments, tasks, completed_collections):\r\n     print(f\"Starting Reddit collection task {task_id} for query: {query}\")\r\n     conn = sqlite3.connect('crawled.db')\r\n     c = conn.cursor()\r\n     c.execute('''CREATE TABLE IF NOT EXISTS urls\r\n@@ -32,8 +32,9 @@\n         urls = search_web(query, site=site, timelimit=timelimit_code)\r\n         all_urls = list(set(urls))[:MAX_URLS]\r\n \r\n         tag = f\"reddit_{query}_{timelimit}\"\r\n+        name = custom_name or f\"Reddit - {query} ({timelimit})\"\r\n         history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy\r\n         message = query\r\n         response = \"\"\r\n \r\n@@ -76,19 +77,19 @@\n \r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = f\"Collection completed. {new_docs_total} new chunks added.\"\r\n         tasks[task_id]['tag'] = tag\r\n-        completed_collections.append({'name': f\"Reddit - {query} ({timelimit})\", 'tag': tag})\r\n+        completed_collections.append({'name': name, 'tag': tag})\r\n         print(f\"Reddit collection task {task_id} completed.\")\r\n     except Exception as e:\r\n         tasks[task_id]['status'] = 'error'\r\n         tasks[task_id]['message'] = str(e)\r\n         print(f\"Reddit collection task {task_id} error: {e}\")\r\n     finally:\r\n         conn.close()\r\n \r\n-def start_reddit_collection(query, timelimit, use_ollama, max_comments, tasks, completed_collections):\r\n+def start_reddit_collection(custom_name, query, timelimit, use_ollama, max_comments, tasks, completed_collections):\r\n     task_id = len(tasks)\r\n-    task = {'id': task_id, 'type': 'reddit', 'query': query, 'timelimit': timelimit, 'use_ollama': use_ollama, 'max_comments': max_comments, 'status': 'running', 'message': ''}\r\n+    task = {'id': task_id, 'type': 'reddit', 'custom_name': custom_name, 'query': query, 'timelimit': timelimit, 'use_ollama': use_ollama, 'max_comments': max_comments, 'status': 'running', 'message': ''}\r\n     tasks.append(task)\r\n-    threading.Thread(target=run_reddit_collection, args=(task_id, query, timelimit, use_ollama, max_comments, tasks, completed_collections)).start()\r\n+    threading.Thread(target=run_reddit_collection, args=(task_id, custom_name, query, timelimit, use_ollama, max_comments, tasks, completed_collections)).start()\r\n     return \"Reddit collection started in background.\", tasks, completed_collections\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756943214908,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,15 +1,18 @@\n # reddit_utils.py\r\n import threading\r\n import requests\r\n import sqlite3\r\n-from config import MAX_URLS, FAISS_PATH\r\n+from config import MAX_URLS, FAISS_PATH, RAW_DIR\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n from web_utils import search_web\r\n from db_utils import add_chunk_if_new, store_content, get_stored_content\r\n-from vectorstore_utils import vectorstore\r\n+from vectorstore_manager import get_vectorstore\r\n from utils import lock\r\n+from urllib.parse import quote\r\n+import os\r\n+import html\r\n \r\n def run_reddit_collection(task_id, custom_name, query, timelimit, use_ollama, max_comments, tasks, completed_collections):\r\n     print(f\"Starting Reddit collection task {task_id} for query: {query}\")\r\n     conn = sqlite3.connect('crawled.db')\r\n@@ -48,9 +51,23 @@\n                     post_text = data[0]['data']['children'][0]['data']['selftext']\r\n                     comments = data[1]['data']['children']\r\n                     comment_texts = [comment['data']['body'] for comment in comments[:max_comments] if 'body' in comment['data']]\r\n                     full_text = post_text + \" \".join(comment_texts)\r\n-                    # Assuming no further processing needed for now, store full_text\r\n+                    # Save to raw_contents\r\n+                    prefix = \"ollama_\" if use_ollama else \"\"\r\n+                    safe_filename = prefix + quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n+                    filepath = os.path.join(RAW_DIR, safe_filename)\r\n+                    with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                        f.write(full_text)\r\n+\r\n+                    # HTML view for better viewing\r\n+                    html_safe_filename = safe_filename.replace(\".txt\", \".html\")\r\n+                    html_filepath = os.path.join(RAW_DIR, html_safe_filename)\r\n+                    escaped_text = html.escape(full_text)\r\n+                    html_content = f\"\"\"<html><head><style>body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }} pre {{ white-space: pre-wrap; word-wrap: break-word; }}</style></head><body><h1>Extracted Content for {url}</h1><pre>{escaped_text}</pre></body></html>\"\"\"\r\n+                    with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                        f.write(html_content)\r\n+\r\n                     store_content(conn, url, full_text)\r\n                     response += f\"Fetched post and comments for {url}\\n\"\r\n                 else:\r\n                     response += f\"Failed to fetch for {url}\\n\"\r\n@@ -70,10 +87,10 @@\n                         metadata = {\"source\": url, \"tag\": tag}\r\n                         new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n                 if new_docs:\r\n                     with lock:\r\n-                        vectorstore.add_documents(new_docs)\r\n-                        vectorstore.save_local(FAISS_PATH)\r\n+                        vs = get_vectorstore(tag)\r\n+                        vs.add_documents(new_docs)\r\n                     new_docs_total += len(new_docs)\r\n \r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = f\"Collection completed. {new_docs_total} new chunks added.\"\r\n"
                },
                {
                    "date": 1756945803615,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,18 +1,15 @@\n # reddit_utils.py\r\n import threading\r\n import requests\r\n import sqlite3\r\n-from config import MAX_URLS, FAISS_PATH, RAW_DIR\r\n+from config import MAX_URLS, FAISS_PATH\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n from web_utils import search_web\r\n-from db_utils import add_chunk_if_new, store_content, get_stored_content\r\n+from db_utils import add_chunk_if_new, store_content, get_stored_content, add_collection\r\n from vectorstore_manager import get_vectorstore\r\n from utils import lock\r\n-from urllib.parse import quote\r\n-import os\r\n-import html\r\n \r\n def run_reddit_collection(task_id, custom_name, query, timelimit, use_ollama, max_comments, tasks, completed_collections):\r\n     print(f\"Starting Reddit collection task {task_id} for query: {query}\")\r\n     conn = sqlite3.connect('crawled.db')\r\n@@ -51,23 +48,9 @@\n                     post_text = data[0]['data']['children'][0]['data']['selftext']\r\n                     comments = data[1]['data']['children']\r\n                     comment_texts = [comment['data']['body'] for comment in comments[:max_comments] if 'body' in comment['data']]\r\n                     full_text = post_text + \" \".join(comment_texts)\r\n-                    # Save to raw_contents\r\n-                    prefix = \"ollama_\" if use_ollama else \"\"\r\n-                    safe_filename = prefix + quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n-                    filepath = os.path.join(RAW_DIR, safe_filename)\r\n-                    with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n-                        f.write(full_text)\r\n-\r\n-                    # HTML view for better viewing\r\n-                    html_safe_filename = safe_filename.replace(\".txt\", \".html\")\r\n-                    html_filepath = os.path.join(RAW_DIR, html_safe_filename)\r\n-                    escaped_text = html.escape(full_text)\r\n-                    html_content = f\"\"\"<html><head><style>body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }} pre {{ white-space: pre-wrap; word-wrap: break-word; }}</style></head><body><h1>Extracted Content for {url}</h1><pre>{escaped_text}</pre></body></html>\"\"\"\r\n-                    with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n-                        f.write(html_content)\r\n-\r\n+                    # Assuming no further processing needed for now, store full_text\r\n                     store_content(conn, url, full_text)\r\n                     response += f\"Fetched post and comments for {url}\\n\"\r\n                 else:\r\n                     response += f\"Failed to fetch for {url}\\n\"\r\n@@ -91,8 +74,9 @@\n                         vs = get_vectorstore(tag)\r\n                         vs.add_documents(new_docs)\r\n                     new_docs_total += len(new_docs)\r\n \r\n+        add_collection(conn, name, tag)\r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = f\"Collection completed. {new_docs_total} new chunks added.\"\r\n         tasks[task_id]['tag'] = tag\r\n         completed_collections.append({'name': name, 'tag': tag})\r\n"
                },
                {
                    "date": 1756946370918,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,15 +1,18 @@\n # reddit_utils.py\r\n import threading\r\n import requests\r\n import sqlite3\r\n-from config import MAX_URLS, FAISS_PATH\r\n+from config import MAX_URLS, FAISS_PATH, RAW_DIR\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n from web_utils import search_web\r\n from db_utils import add_chunk_if_new, store_content, get_stored_content, add_collection\r\n from vectorstore_manager import get_vectorstore\r\n from utils import lock\r\n+from urllib.parse import quote\r\n+import os\r\n+import html\r\n \r\n def run_reddit_collection(task_id, custom_name, query, timelimit, use_ollama, max_comments, tasks, completed_collections):\r\n     print(f\"Starting Reddit collection task {task_id} for query: {query}\")\r\n     conn = sqlite3.connect('crawled.db')\r\n@@ -48,9 +51,23 @@\n                     post_text = data[0]['data']['children'][0]['data']['selftext']\r\n                     comments = data[1]['data']['children']\r\n                     comment_texts = [comment['data']['body'] for comment in comments[:max_comments] if 'body' in comment['data']]\r\n                     full_text = post_text + \" \".join(comment_texts)\r\n-                    # Assuming no further processing needed for now, store full_text\r\n+                    # Save to raw_contents\r\n+                    prefix = \"ollama_\" if use_ollama else \"\"\r\n+                    safe_filename = prefix + quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n+                    filepath = os.path.join(RAW_DIR, safe_filename)\r\n+                    with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                        f.write(full_text)\r\n+\r\n+                    # HTML view\r\n+                    html_safe_filename = safe_filename.replace(\".txt\", \".html\")\r\n+                    html_filepath = os.path.join(RAW_DIR, html_safe_filename)\r\n+                    escaped_text = html.escape(full_text)\r\n+                    html_content = f\"\"\"<html><head><style>body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }} pre {{ white-space: pre-wrap; word-wrap: break-word; }}</style></head><body><h1>Extracted Content for {url}</h1><pre>{escaped_text}</pre></body></html>\"\"\"\r\n+                    with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                        f.write(html_content)\r\n+\r\n                     store_content(conn, url, full_text)\r\n                     response += f\"Fetched post and comments for {url}\\n\"\r\n                 else:\r\n                     response += f\"Failed to fetch for {url}\\n\"\r\n@@ -74,9 +91,10 @@\n                         vs = get_vectorstore(tag)\r\n                         vs.add_documents(new_docs)\r\n                     new_docs_total += len(new_docs)\r\n \r\n-        add_collection(conn, name, tag)\r\n+        add_collection(conn, name, tag)  # Save to DB\r\n+\r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = f\"Collection completed. {new_docs_total} new chunks added.\"\r\n         tasks[task_id]['tag'] = tag\r\n         completed_collections.append({'name': name, 'tag': tag})\r\n"
                },
                {
                    "date": 1756958169038,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -89,8 +89,9 @@\n                 if new_docs:\r\n                     with lock:\r\n                         vs = get_vectorstore(tag)\r\n                         vs.add_documents(new_docs)\r\n+                        print(f\"Added {len(new_docs)} documents to vectorstore for tag {tag}.\")\r\n                     new_docs_total += len(new_docs)\r\n \r\n         add_collection(conn, name, tag)  # Save to DB\r\n \r\n"
                },
                {
                    "date": 1756961442471,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,128 @@\n+# reddit_utils.py\r\n+import threading\r\n+import requests\r\n+import sqlite3\r\n+from config import MAX_URLS, FAISS_PATH, RAW_DIR\r\n+from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n+from langchain_core.documents import Document\r\n+from web_utils import search_web\r\n+from db_utils import add_chunk_if_new, store_content, get_stored_content, add_collection\r\n+from vectorstore_manager import get_vectorstore\r\n+from utils import lock\r\n+from urllib.parse import quote\r\n+import os\r\n+import html\r\n+import re  # Added for sanitization\r\n+\r\n+def sanitize_tag(name):\r\n+    # Replace invalid path characters with '_'\r\n+    invalid_chars = r'[<>:\"/\\\\|?*]'\r\n+    sanitized = re.sub(invalid_chars, '_', name)\r\n+    # Strip leading/trailing whitespace\r\n+    sanitized = sanitized.strip()\r\n+    # Collapse multiple '_' into single\r\n+    sanitized = re.sub(r'_+', '_', sanitized)\r\n+    return sanitized\r\n+\r\n+def run_reddit_collection(task_id, custom_name, query, timelimit, max_urls, use_ollama, max_comments, tasks, completed_collections):\r\n+    print(f\"Starting Reddit collection task {task_id} for query: {query}\")\r\n+    conn = sqlite3.connect('crawled.db')\r\n+    c = conn.cursor()\r\n+    c.execute('''CREATE TABLE IF NOT EXISTS urls\r\n+                 (url TEXT PRIMARY KEY, timestamp DATETIME, cleaned_text TEXT)''')\r\n+    c.execute('''CREATE TABLE IF NOT EXISTS chunks\r\n+                 (hash TEXT PRIMARY KEY, content TEXT, source TEXT, tag TEXT)''')\r\n+    try:\r\n+        c.execute(\"ALTER TABLE chunks ADD COLUMN tag TEXT\")\r\n+        print(\"Added 'tag' column to chunks table.\")\r\n+    except sqlite3.OperationalError as e:\r\n+        if \"duplicate column name\" not in str(e):\r\n+            raise e\r\n+        print(\"'tag' column already exists in chunks table.\")\r\n+    conn.commit()\r\n+    try:\r\n+        site = \"reddit.com\"\r\n+        timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n+        urls = search_web(query, site=site, timelimit=timelimit_code)\r\n+        all_urls = list(set(urls))[:max_urls]\r\n+\r\n+        raw_tag = f\"reddit_{query}_{timelimit}\"\r\n+        tag = sanitize_tag(raw_tag)  # Sanitize to prevent invalid path characters\r\n+        print(f\"Debug: Sanitized tag from '{raw_tag}' to '{tag}'\")\r\n+        name = custom_name or f\"Reddit - {query} ({timelimit})\"\r\n+        history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy\r\n+        message = query\r\n+        response = \"\"\r\n+\r\n+        # For Reddit, fetch post + comments\r\n+        for url in all_urls:\r\n+            try:\r\n+                json_url = url + '.json'\r\n+                json_response = requests.get(json_url, headers={'User-Agent': 'Mozilla/5.0'})\r\n+                if json_response.status_code == 200:\r\n+                    data = json_response.json()\r\n+                    post_text = data[0]['data']['children'][0]['data']['selftext']\r\n+                    comments = data[1]['data']['children']\r\n+                    comment_texts = [comment['data']['body'] for comment in comments[:max_comments] if 'body' in comment['data']]\r\n+                    full_text = post_text + \" \".join(comment_texts)\r\n+                    # Save to raw_contents\r\n+                    prefix = \"ollama_\" if use_ollama else \"\"\r\n+                    safe_filename = prefix + quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n+                    filepath = os.path.join(RAW_DIR, safe_filename)\r\n+                    with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                        f.write(full_text)\r\n+\r\n+                    # HTML view\r\n+                    html_safe_filename = safe_filename.replace(\".txt\", \".html\")\r\n+                    html_filepath = os.path.join(RAW_DIR, html_safe_filename)\r\n+                    escaped_text = html.escape(full_text)\r\n+                    html_content = f\"\"\"<html><head><style>body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }} pre {{ white-space: pre-wrap; word-wrap: break-word; }}</style></head><body><h1>Extracted Content for {url}</h1><pre>{escaped_text}</pre></body></html>\"\"\"\r\n+                    with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                        f.write(html_content)\r\n+\r\n+                    store_content(conn, url, full_text)\r\n+                    response += f\"Fetched post and comments for {url}\\n\"\r\n+                else:\r\n+                    response += f\"Failed to fetch for {url}\\n\"\r\n+            except Exception as e:\r\n+                response += f\"Error for {url}: {e}\\n\"\r\n+\r\n+        # Then process to chunks\r\n+        new_docs_total = 0\r\n+        for url in all_urls:\r\n+            content = get_stored_content(conn, url)\r\n+            if content:\r\n+                text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n+                chunks = text_splitter.split_text(content)\r\n+                new_docs = []\r\n+                for chunk in chunks:\r\n+                    if add_chunk_if_new(conn, chunk, url, tag=tag):\r\n+                        metadata = {\"source\": url, \"tag\": tag}\r\n+                        new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+                if new_docs:\r\n+                    with lock:\r\n+                        vs = get_vectorstore(tag)\r\n+                        vs.add_documents(new_docs)\r\n+                        print(f\"Added {len(new_docs)} documents to vectorstore for tag {tag}.\")\r\n+                    new_docs_total += len(new_docs)\r\n+\r\n+        add_collection(conn, name, tag)  # Save to DB\r\n+\r\n+        tasks[task_id]['status'] = 'completed'\r\n+        tasks[task_id]['message'] = f\"Collection completed. {new_docs_total} new chunks added.\"\r\n+        tasks[task_id]['tag'] = tag\r\n+        completed_collections.append({'name': name, 'tag': tag})\r\n+        print(f\"Reddit collection task {task_id} completed.\")\r\n+    except Exception as e:\r\n+        tasks[task_id]['status'] = 'error'\r\n+        tasks[task_id]['message'] = str(e)\r\n+        print(f\"Reddit collection task {task_id} error: {e}\")\r\n+    finally:\r\n+        conn.close()\r\n+\r\n+def start_reddit_collection(custom_name, query, timelimit, max_urls=10, use_ollama=False, max_comments=50, tasks=None, completed_collections=None):\r\n+    task_id = len(tasks)\r\n+    task = {'id': task_id, 'type': 'reddit', 'custom_name': custom_name, 'query': query, 'timelimit': timelimit, 'max_urls': max_urls, 'use_ollama': use_ollama, 'max_comments': max_comments, 'status': 'running', 'message': ''}\r\n+    tasks.append(task)\r\n+    threading.Thread(target=run_reddit_collection, args=(task_id, custom_name, query, timelimit, max_urls, use_ollama, max_comments, tasks, completed_collections)).start()\r\n+    return \"Reddit collection started in background.\", tasks, completed_collections\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756964650253,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -102,9 +102,13 @@\n                 if new_docs:\r\n                     with lock:\r\n                         vs = get_vectorstore(tag)\r\n                         vs.add_documents(new_docs)\r\n-                        print(f\"Added {len(new_docs)} documents to vectorstore for tag {tag}.\")\r\n+                        print(f\"Debug: Added {len(new_docs)} documents to vectorstore for tag {tag}. ntotal after add: {vs.index.ntotal}\")\r\n+                        # Save the vectorstore to disk after adding documents\r\n+                        save_path = os.path.join(FAISS_PATH, tag)\r\n+                        vs.save_local(save_path)\r\n+                        print(f\"Debug: Saved vectorstore for tag {tag} to {save_path}.\")\r\n                     new_docs_total += len(new_docs)\r\n \r\n         add_collection(conn, name, tag)  # Save to DB\r\n \r\n@@ -124,120 +128,5 @@\n     task_id = len(tasks)\r\n     task = {'id': task_id, 'type': 'reddit', 'custom_name': custom_name, 'query': query, 'timelimit': timelimit, 'max_urls': max_urls, 'use_ollama': use_ollama, 'max_comments': max_comments, 'status': 'running', 'message': ''}\r\n     tasks.append(task)\r\n     threading.Thread(target=run_reddit_collection, args=(task_id, custom_name, query, timelimit, max_urls, use_ollama, max_comments, tasks, completed_collections)).start()\r\n-    return \"Reddit collection started in background.\", tasks, completed_collections\n-# reddit_utils.py\r\n-import threading\r\n-import requests\r\n-import sqlite3\r\n-from config import MAX_URLS, FAISS_PATH, RAW_DIR\r\n-from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n-from langchain_core.documents import Document\r\n-from web_utils import search_web\r\n-from db_utils import add_chunk_if_new, store_content, get_stored_content, add_collection\r\n-from vectorstore_manager import get_vectorstore\r\n-from utils import lock\r\n-from urllib.parse import quote\r\n-import os\r\n-import html\r\n-\r\n-def run_reddit_collection(task_id, custom_name, query, timelimit, use_ollama, max_comments, tasks, completed_collections):\r\n-    print(f\"Starting Reddit collection task {task_id} for query: {query}\")\r\n-    conn = sqlite3.connect('crawled.db')\r\n-    c = conn.cursor()\r\n-    c.execute('''CREATE TABLE IF NOT EXISTS urls\r\n-                 (url TEXT PRIMARY KEY, timestamp DATETIME, cleaned_text TEXT)''')\r\n-    c.execute('''CREATE TABLE IF NOT EXISTS chunks\r\n-                 (hash TEXT PRIMARY KEY, content TEXT, source TEXT, tag TEXT)''')\r\n-    try:\r\n-        c.execute(\"ALTER TABLE chunks ADD COLUMN tag TEXT\")\r\n-        print(\"Added 'tag' column to chunks table.\")\r\n-    except sqlite3.OperationalError as e:\r\n-        if \"duplicate column name\" not in str(e):\r\n-            raise e\r\n-        print(\"'tag' column already exists in chunks table.\")\r\n-    conn.commit()\r\n-    try:\r\n-        site = \"reddit.com\"\r\n-        timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n-        urls = search_web(query, site=site, timelimit=timelimit_code)\r\n-        all_urls = list(set(urls))[:MAX_URLS]\r\n-\r\n-        tag = f\"reddit_{query}_{timelimit}\"\r\n-        name = custom_name or f\"Reddit - {query} ({timelimit})\"\r\n-        history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy\r\n-        message = query\r\n-        response = \"\"\r\n-\r\n-        # For Reddit, fetch post + comments\r\n-        for url in all_urls:\r\n-            try:\r\n-                json_url = url + '.json'\r\n-                json_response = requests.get(json_url, headers={'User-Agent': 'Mozilla/5.0'})\r\n-                if json_response.status_code == 200:\r\n-                    data = json_response.json()\r\n-                    post_text = data[0]['data']['children'][0]['data']['selftext']\r\n-                    comments = data[1]['data']['children']\r\n-                    comment_texts = [comment['data']['body'] for comment in comments[:max_comments] if 'body' in comment['data']]\r\n-                    full_text = post_text + \" \".join(comment_texts)\r\n-                    # Save to raw_contents\r\n-                    prefix = \"ollama_\" if use_ollama else \"\"\r\n-                    safe_filename = prefix + quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n-                    filepath = os.path.join(RAW_DIR, safe_filename)\r\n-                    with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n-                        f.write(full_text)\r\n-\r\n-                    # HTML view\r\n-                    html_safe_filename = safe_filename.replace(\".txt\", \".html\")\r\n-                    html_filepath = os.path.join(RAW_DIR, html_safe_filename)\r\n-                    escaped_text = html.escape(full_text)\r\n-                    html_content = f\"\"\"<html><head><style>body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }} pre {{ white-space: pre-wrap; word-wrap: break-word; }}</style></head><body><h1>Extracted Content for {url}</h1><pre>{escaped_text}</pre></body></html>\"\"\"\r\n-                    with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n-                        f.write(html_content)\r\n-\r\n-                    store_content(conn, url, full_text)\r\n-                    response += f\"Fetched post and comments for {url}\\n\"\r\n-                else:\r\n-                    response += f\"Failed to fetch for {url}\\n\"\r\n-            except Exception as e:\r\n-                response += f\"Error for {url}: {e}\\n\"\r\n-\r\n-        # Then process to chunks\r\n-        new_docs_total = 0\r\n-        for url in all_urls:\r\n-            content = get_stored_content(conn, url)\r\n-            if content:\r\n-                text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n-                chunks = text_splitter.split_text(content)\r\n-                new_docs = []\r\n-                for chunk in chunks:\r\n-                    if add_chunk_if_new(conn, chunk, url, tag=tag):\r\n-                        metadata = {\"source\": url, \"tag\": tag}\r\n-                        new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n-                if new_docs:\r\n-                    with lock:\r\n-                        vs = get_vectorstore(tag)\r\n-                        vs.add_documents(new_docs)\r\n-                        print(f\"Added {len(new_docs)} documents to vectorstore for tag {tag}.\")\r\n-                    new_docs_total += len(new_docs)\r\n-\r\n-        add_collection(conn, name, tag)  # Save to DB\r\n-\r\n-        tasks[task_id]['status'] = 'completed'\r\n-        tasks[task_id]['message'] = f\"Collection completed. {new_docs_total} new chunks added.\"\r\n-        tasks[task_id]['tag'] = tag\r\n-        completed_collections.append({'name': name, 'tag': tag})\r\n-        print(f\"Reddit collection task {task_id} completed.\")\r\n-    except Exception as e:\r\n-        tasks[task_id]['status'] = 'error'\r\n-        tasks[task_id]['message'] = str(e)\r\n-        print(f\"Reddit collection task {task_id} error: {e}\")\r\n-    finally:\r\n-        conn.close()\r\n-\r\n-def start_reddit_collection(custom_name, query, timelimit, use_ollama, max_comments, tasks, completed_collections):\r\n-    task_id = len(tasks)\r\n-    task = {'id': task_id, 'type': 'reddit', 'custom_name': custom_name, 'query': query, 'timelimit': timelimit, 'use_ollama': use_ollama, 'max_comments': max_comments, 'status': 'running', 'message': ''}\r\n-    tasks.append(task)\r\n-    threading.Thread(target=run_reddit_collection, args=(task_id, custom_name, query, timelimit, use_ollama, max_comments, tasks, completed_collections)).start()\r\n     return \"Reddit collection started in background.\", tasks, completed_collections\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756964881426,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,132 @@\n+# reddit_utils.py\r\n+import threading\r\n+import requests\r\n+import sqlite3\r\n+from config import MAX_URLS, FAISS_PATH, RAW_DIR\r\n+from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n+from langchain_core.documents import Document\r\n+from web_utils import search_web\r\n+from db_utils import add_chunk_if_new, store_content, get_stored_content, add_collection\r\n+from vectorstore_manager import get_vectorstore\r\n+from utils import lock\r\n+from urllib.parse import quote\r\n+import os\r\n+import html\r\n+import re  # Added for sanitization\r\n+\r\n+def sanitize_tag(name):\r\n+    # Replace invalid path characters with '_'\r\n+    invalid_chars = r'[<>:\"/\\\\|?*]'\r\n+    sanitized = re.sub(invalid_chars, '_', name)\r\n+    # Strip leading/trailing whitespace\r\n+    sanitized = sanitized.strip()\r\n+    # Collapse multiple '_' into single\r\n+    sanitized = re.sub(r'_+', '_', sanitized)\r\n+    return sanitized\r\n+\r\n+def run_reddit_collection(task_id, custom_name, query, timelimit, max_urls, use_ollama, max_comments, tasks, completed_collections):\r\n+    print(f\"Starting Reddit collection task {task_id} for query: {query}\")\r\n+    conn = sqlite3.connect('crawled.db')\r\n+    c = conn.cursor()\r\n+    c.execute('''CREATE TABLE IF NOT EXISTS urls\r\n+                 (url TEXT PRIMARY KEY, timestamp DATETIME, cleaned_text TEXT)''')\r\n+    c.execute('''CREATE TABLE IF NOT EXISTS chunks\r\n+                 (hash TEXT PRIMARY KEY, content TEXT, source TEXT, tag TEXT)''')\r\n+    try:\r\n+        c.execute(\"ALTER TABLE chunks ADD COLUMN tag TEXT\")\r\n+        print(\"Added 'tag' column to chunks table.\")\r\n+    except sqlite3.OperationalError as e:\r\n+        if \"duplicate column name\" not in str(e):\r\n+            raise e\r\n+        print(\"'tag' column already exists in chunks table.\")\r\n+    conn.commit()\r\n+    try:\r\n+        site = \"reddit.com\"\r\n+        timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n+        urls = search_web(query, site=site, timelimit=timelimit_code)\r\n+        all_urls = list(set(urls))[:max_urls]\r\n+\r\n+        raw_tag = f\"reddit_{query}_{timelimit}\"\r\n+        tag = sanitize_tag(raw_tag)  # Sanitize to prevent invalid path characters\r\n+        print(f\"Debug: Sanitized tag from '{raw_tag}' to '{tag}'\")\r\n+        name = custom_name or f\"Reddit - {query} ({timelimit})\"\r\n+        history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy\r\n+        message = query\r\n+        response = \"\"\r\n+\r\n+        # For Reddit, fetch post + comments\r\n+        for url in all_urls:\r\n+            try:\r\n+                json_url = url + '.json'\r\n+                json_response = requests.get(json_url, headers={'User-Agent': 'Mozilla/5.0'})\r\n+                if json_response.status_code == 200:\r\n+                    data = json_response.json()\r\n+                    post_text = data[0]['data']['children'][0]['data']['selftext']\r\n+                    comments = data[1]['data']['children']\r\n+                    comment_texts = [comment['data']['body'] for comment in comments[:max_comments] if 'body' in comment['data']]\r\n+                    full_text = post_text + \" \".join(comment_texts)\r\n+                    # Save to raw_contents\r\n+                    prefix = \"ollama_\" if use_ollama else \"\"\r\n+                    safe_filename = prefix + quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n+                    filepath = os.path.join(RAW_DIR, safe_filename)\r\n+                    with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                        f.write(full_text)\r\n+\r\n+                    # HTML view\r\n+                    html_safe_filename = safe_filename.replace(\".txt\", \".html\")\r\n+                    html_filepath = os.path.join(RAW_DIR, html_safe_filename)\r\n+                    escaped_text = html.escape(full_text)\r\n+                    html_content = f\"\"\"<html><head><style>body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }} pre {{ white-space: pre-wrap; word-wrap: break-word; }}</style></head><body><h1>Extracted Content for {url}</h1><pre>{escaped_text}</pre></body></html>\"\"\"\r\n+                    with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                        f.write(html_content)\r\n+\r\n+                    store_content(conn, url, full_text)\r\n+                    response += f\"Fetched post and comments for {url}\\n\"\r\n+                else:\r\n+                    response += f\"Failed to fetch for {url}\\n\"\r\n+            except Exception as e:\r\n+                response += f\"Error for {url}: {e}\\n\"\r\n+\r\n+        # Then process to chunks\r\n+        new_docs_total = 0\r\n+        for url in all_urls:\r\n+            content = get_stored_content(conn, url)\r\n+            if content:\r\n+                text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n+                chunks = text_splitter.split_text(content)\r\n+                new_docs = []\r\n+                for chunk in chunks:\r\n+                    if add_chunk_if_new(conn, chunk, url, tag=tag):\r\n+                        metadata = {\"source\": url, \"tag\": tag}\r\n+                        new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+                if new_docs:\r\n+                    with lock:\r\n+                        vs = get_vectorstore(tag)\r\n+                        vs.add_documents(new_docs)\r\n+                        print(f\"Debug: Added {len(new_docs)} documents to vectorstore for tag {tag}. ntotal after add: {vs.index.ntotal}\")\r\n+                        # Save the vectorstore to disk after adding documents\r\n+                        save_path = os.path.join(FAISS_PATH, tag)\r\n+                        vs.save_local(save_path)\r\n+                        print(f\"Debug: Saved vectorstore for tag {tag} to {save_path}.\")\r\n+                    new_docs_total += len(new_docs)\r\n+\r\n+        add_collection(conn, name, tag)  # Save to DB\r\n+\r\n+        tasks[task_id]['status'] = 'completed'\r\n+        tasks[task_id]['message'] = f\"Collection completed. {new_docs_total} new chunks added.\"\r\n+        tasks[task_id]['tag'] = tag\r\n+        completed_collections.append({'name': name, 'tag': tag})\r\n+        print(f\"Reddit collection task {task_id} completed.\")\r\n+    except Exception as e:\r\n+        tasks[task_id]['status'] = 'error'\r\n+        tasks[task_id]['message'] = str(e)\r\n+        print(f\"Reddit collection task {task_id} error: {e}\")\r\n+    finally:\r\n+        conn.close()\r\n+\r\n+def start_reddit_collection(custom_name, query, timelimit, max_urls=10, use_ollama=False, max_comments=50, tasks=None, completed_collections=None):\r\n+    task_id = len(tasks)\r\n+    task = {'id': task_id, 'type': 'reddit', 'custom_name': custom_name, 'query': query, 'timelimit': timelimit, 'max_urls': max_urls, 'use_ollama': use_ollama, 'max_comments': max_comments, 'status': 'running', 'message': ''}\r\n+    tasks.append(task)\r\n+    threading.Thread(target=run_reddit_collection, args=(task_id, custom_name, query, timelimit, max_urls, use_ollama, max_comments, tasks, completed_collections)).start()\r\n+    return \"Reddit collection started in background.\", tasks, completed_collections\n\\ No newline at end of file\n"
                }
            ],
            "date": 1756935541890,
            "name": "Commit-0",
            "content": "# reddit_utils.py\r\nimport threading\r\nimport requests\r\nfrom config import MAX_URLS, FAISS_PATH\r\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\r\nfrom langchain_core.documents import Document\r\nfrom web_utils import search_web\r\nfrom process_utils import process_urls\r\nfrom db_utils import add_chunk_if_new\r\nfrom vectorstore_utils import vectorstore\r\nfrom utils import lock\r\n\r\ndef run_reddit_collection(task_id, query, timelimit, use_ollama, max_comments, tasks, completed_collections, conn=None):\r\n    print(f\"Starting Reddit collection task {task_id} for query: {query}\")\r\n    try:\r\n        site = \"reddit.com\"\r\n        timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n        urls = search_web(query, site=site, timelimit=timelimit_code)\r\n        all_urls = list(set(urls))[:MAX_URLS]\r\n\r\n        tag = f\"reddit_{query}_{timelimit}\"\r\n        history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy\r\n        message = query\r\n        response = \"\"\r\n\r\n        # For Reddit, fetch post + comments\r\n        for url in all_urls:\r\n            try:\r\n                json_url = url + '.json'\r\n                json_response = requests.get(json_url, headers={'User-Agent': 'Mozilla/5.0'})\r\n                if json_response.status_code == 200:\r\n                    data = json_response.json()\r\n                    post_text = data[0]['data']['children'][0]['data']['selftext']\r\n                    comments = data[1]['data']['children']\r\n                    comment_texts = [comment['data']['body'] for comment in comments[:max_comments] if 'body' in comment['data']]\r\n                    full_text = post_text + \" \".join(comment_texts)\r\n                    # Process with clean_web_content logic if needed, but since it's JSON, use directly\r\n                    # Assuming process_urls handles it, but for Reddit, we can store full_text\r\n                    store_content(conn, url, full_text)\r\n                    response += f\"Fetched post and comments for {url}\\n\"\r\n                else:\r\n                    response += f\"Failed to fetch for {url}\\n\"\r\n            except Exception as e:\r\n                response += f\"Error for {url}: {e}\\n\"\r\n\r\n        # Then process to chunks\r\n        new_docs_total = 0\r\n        for url in all_urls:\r\n            content = get_stored_content(conn, url)\r\n            if content:\r\n                text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n                chunks = text_splitter.split_text(content)\r\n                new_docs = []\r\n                for chunk in chunks:\r\n                    if add_chunk_if_new(conn, chunk, url, tag=tag):\r\n                        metadata = {\"source\": url, \"tag\": tag}\r\n                        new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n                if new_docs:\r\n                    with lock:\r\n                        vectorstore.add_documents(new_docs)\r\n                        vectorstore.save_local(FAISS_PATH)\r\n                    new_docs_total += len(new_docs)\r\n\r\n        tasks[task_id]['status'] = 'completed'\r\n        tasks[task_id]['message'] = f\"Collection completed. {new_docs_total} new chunks added.\"\r\n        tasks[task_id]['tag'] = tag\r\n        completed_collections.append({'name': f\"Reddit - {query} ({timelimit})\", 'tag': tag})\r\n        print(f\"Reddit collection task {task_id} completed.\")\r\n    except Exception as e:\r\n        tasks[task_id]['status'] = 'error'\r\n        tasks[task_id]['message'] = str(e)\r\n        print(f\"Reddit collection task {task_id} error: {e}\")\r\n\r\ndef start_reddit_collection(query, timelimit, use_ollama, max_comments, tasks, completed_collections, conn=None):\r\n    task_id = len(tasks)\r\n    task = {'id': task_id, 'type': 'reddit', 'query': query, 'timelimit': timelimit, 'use_ollama': use_ollama, 'max_comments': max_comments, 'status': 'running', 'message': ''}\r\n    tasks.append(task)\r\n    threading.Thread(target=run_reddit_collection, args=(task_id, query, timelimit, use_ollama, max_comments, tasks, completed_collections, conn)).start()\r\n    return \"Reddit collection started in background.\", tasks, completed_collections"
        }
    ]
}