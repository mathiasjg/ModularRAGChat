{
    "sourceFile": "chat_utils.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 52,
            "patches": [
                {
                    "date": 1756856986719,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1756857271466,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,8 +6,9 @@\n from web_utils import search_web\r\n from config import MAX_URLS\r\n from process_utils import process_urls\r\n from vectorstore_utils import vectorstore\r\n+import os\r\n \r\n def chat_bot(message, history, conn=None):\r\n     print(f\"Starting chat_bot with message: {message}\")\r\n     history.append({\"role\": \"user\", \"content\": message})\r\n"
                },
                {
                    "date": 1756857463885,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,9 +6,8 @@\n from web_utils import search_web\r\n from config import MAX_URLS\r\n from process_utils import process_urls\r\n from vectorstore_utils import vectorstore\r\n-import os\r\n \r\n def chat_bot(message, history, conn=None):\r\n     print(f\"Starting chat_bot with message: {message}\")\r\n     history.append({\"role\": \"user\", \"content\": message})\r\n"
                },
                {
                    "date": 1756857576011,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,8 +6,9 @@\n from web_utils import search_web\r\n from config import MAX_URLS\r\n from process_utils import process_urls\r\n from vectorstore_utils import vectorstore\r\n+import os\r\n \r\n def chat_bot(message, history, conn=None):\r\n     print(f\"Starting chat_bot with message: {message}\")\r\n     history.append({\"role\": \"user\", \"content\": message})\r\n"
                },
                {
                    "date": 1756858499426,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,9 +6,8 @@\n from web_utils import search_web\r\n from config import MAX_URLS\r\n from process_utils import process_urls\r\n from vectorstore_utils import vectorstore\r\n-import os\r\n \r\n def chat_bot(message, history, conn=None):\r\n     print(f\"Starting chat_bot with message: {message}\")\r\n     history.append({\"role\": \"user\", \"content\": message})\r\n"
                },
                {
                    "date": 1756858877619,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,4 +1,5 @@\n+import os\r\n from langchain_ollama import OllamaLLM\r\n from langchain_core.messages import HumanMessage, AIMessage\r\n from langchain.chains import create_retrieval_chain, create_history_aware_retriever\r\n from langchain.chains.combine_documents import create_stuff_documents_chain\r\n@@ -7,10 +8,10 @@\n from config import MAX_URLS\r\n from process_utils import process_urls\r\n from vectorstore_utils import vectorstore\r\n \r\n-def chat_bot(message, history, conn=None):\r\n-    print(f\"Starting chat_bot with message: {message}\")\r\n+def chat_bot(message, history, conn=None, selected_tag=None):\r\n+    print(f\"Starting chat_bot with message: {message}, selected_tag: {selected_tag}\")\r\n     history.append({\"role\": \"user\", \"content\": message})\r\n     yield \"\", history\r\n \r\n     response = \"**Processing Status:**\\n\"\r\n@@ -85,9 +86,12 @@\n     summary_prompt = ChatPromptTemplate.from_template(\r\n         \"Summarize the following retrieved content related to the question '{input}' in a concise manner:\\n\\n{context}\"\r\n     )\r\n     summary_chain = create_stuff_documents_chain(llm, summary_prompt)\r\n-    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\r\n+    search_kwargs = {\"k\": 5}\r\n+    if selected_tag:\r\n+        search_kwargs[\"filter\"] = {\"tag\": selected_tag}\r\n+    retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n     summary_chain_with_docs = create_retrieval_chain(retriever, summary_chain)\r\n     summary_response = summary_chain_with_docs.invoke({\"input\": message})\r\n     summary = summary_response[\"answer\"]\r\n \r\n"
                },
                {
                    "date": 1756859273803,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,13 +11,13 @@\n \r\n def chat_bot(message, history, conn=None, selected_tag=None):\r\n     print(f\"Starting chat_bot with message: {message}, selected_tag: {selected_tag}\")\r\n     history.append({\"role\": \"user\", \"content\": message})\r\n-    yield \"\", history\r\n+    yield history, \"\"\r\n \r\n     response = \"**Processing Status:**\\n\"\r\n     history.append({\"role\": \"assistant\", \"content\": response})\r\n-    yield \"\", history\r\n+    yield history, \"\"\r\n \r\n     chat_history = []\r\n     for h in history[:-1]:\r\n         if h[\"role\"] == \"user\":\r\n@@ -26,30 +26,30 @@\n             chat_history.append(AIMessage(content=h[\"content\"]))\r\n \r\n     response += f\"Searching web for query: {message}\\n\"\r\n     history[-1][\"content\"] = response\r\n-    yield \"\", history\r\n+    yield history, \"\"\r\n \r\n     google_urls = search_web(message)\r\n \r\n     response += f\"Found {len(google_urls)} Google URLs: {', '.join(google_urls)}\\n\\n\"\r\n     history[-1][\"content\"] = response\r\n-    yield \"\", history\r\n+    yield history, \"\"\r\n \r\n     response += f\"Searching Reddit for query: {message}\\n\"\r\n     history[-1][\"content\"] = response\r\n-    yield \"\", history\r\n+    yield history, \"\"\r\n \r\n     reddit_urls = search_web(message, site=\"reddit.com\")\r\n \r\n     response += f\"Found {len(reddit_urls)} Reddit URLs: {', '.join(reddit_urls)}\\n\\n\"\r\n     history[-1][\"content\"] = response\r\n-    yield \"\", history\r\n+    yield history, \"\"\r\n \r\n     all_urls = list(set(google_urls + reddit_urls))[:MAX_URLS]\r\n     response += f\"All unique URLs to process (limited to {MAX_URLS}): {', '.join(all_urls)}\\n\\n\"\r\n     history[-1][\"content\"] = response\r\n-    yield \"\", history\r\n+    yield history, \"\"\r\n \r\n     process_gen = process_urls(all_urls, response, history, message, conn=conn)\r\n     for upd in process_gen:\r\n         yield upd\r\n@@ -65,24 +65,24 @@\n \r\n     if vectorstore.index.ntotal == 0:\r\n         response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n         history[-1][\"content\"] = response\r\n-        yield \"\", history\r\n+        yield history, \"\"\r\n         return\r\n \r\n     response += \"Creating/Updating vector store...\\n\"\r\n     history[-1][\"content\"] = response\r\n-    yield \"\", history\r\n+    yield history, \"\"\r\n \r\n     response += \"Vector store ready.\\n\\n\"\r\n     history[-1][\"content\"] = response\r\n-    yield \"\", history\r\n+    yield history, \"\"\r\n \r\n     llm = OllamaLLM(model=MODEL_NAME)\r\n \r\n     response += \"Generating summarization of the found content...\\n\"\r\n     history[-1][\"content\"] = response\r\n-    yield \"\", history\r\n+    yield history, \"\"\r\n \r\n     summary_prompt = ChatPromptTemplate.from_template(\r\n         \"Summarize the following retrieved content related to the question '{input}' in a concise manner:\\n\\n{context}\"\r\n     )\r\n@@ -96,13 +96,13 @@\n     summary = summary_response[\"answer\"]\r\n \r\n     response += f\"**Summarization of Found Content:**\\n{summary}\\n\\n\"\r\n     history[-1][\"content\"] = response\r\n-    yield \"\", history\r\n+    yield history, \"\"\r\n \r\n     response += \"Generating specific answer to the prompt...\\n\"\r\n     history[-1][\"content\"] = response\r\n-    yield \"\", history\r\n+    yield history, \"\"\r\n \r\n     rephrase_prompt = ChatPromptTemplate.from_messages(\r\n         [\r\n             MessagesPlaceholder(variable_name=\"chat_history\"),\r\n@@ -121,12 +121,12 @@\n     answer = qa_response[\"answer\"]\r\n \r\n     response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n     history[-1][\"content\"] = response\r\n-    yield \"\", history\r\n+    yield history, \"\"\r\n \r\n     sources_str = \"\\n\".join([f\"- {url}\" for url in sources])\r\n     response += f\"**Sources Crawled:**\\n{sources_str}\"\r\n     history[-1][\"content\"] = response\r\n-    yield \"\", history\r\n+    yield history, \"\"\r\n \r\n     print(\"chat_bot completed.\")\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756860169454,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,5 +1,6 @@\n import os\r\n+from config import MODEL_NAME\r\n from langchain_ollama import OllamaLLM\r\n from langchain_core.messages import HumanMessage, AIMessage\r\n from langchain.chains import create_retrieval_chain, create_history_aware_retriever\r\n from langchain.chains.combine_documents import create_stuff_documents_chain\r\n@@ -8,10 +9,10 @@\n from config import MAX_URLS\r\n from process_utils import process_urls\r\n from vectorstore_utils import vectorstore\r\n \r\n-def chat_bot(message, history, conn=None, selected_tag=None):\r\n-    print(f\"Starting chat_bot with message: {message}, selected_tag: {selected_tag}\")\r\n+def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n+    print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n     history.append({\"role\": \"user\", \"content\": message})\r\n     yield history, \"\"\r\n \r\n     response = \"**Processing Status:**\\n\"\r\n@@ -24,77 +25,84 @@\n             chat_history.append(HumanMessage(content=h[\"content\"]))\r\n         elif h[\"role\"] == \"assistant\":\r\n             chat_history.append(AIMessage(content=h[\"content\"]))\r\n \r\n-    response += f\"Searching web for query: {message}\\n\"\r\n-    history[-1][\"content\"] = response\r\n-    yield history, \"\"\r\n+    llm = OllamaLLM(model=MODEL_NAME)\r\n \r\n-    google_urls = search_web(message)\r\n+    retriever = None\r\n+    if selected_source:\r\n+        if selected_source == \"Web Search\":\r\n+            response += f\"Searching web for query: {message}\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n \r\n-    response += f\"Found {len(google_urls)} Google URLs: {', '.join(google_urls)}\\n\\n\"\r\n-    history[-1][\"content\"] = response\r\n-    yield history, \"\"\r\n+            google_urls = search_web(message)\r\n \r\n-    response += f\"Searching Reddit for query: {message}\\n\"\r\n-    history[-1][\"content\"] = response\r\n-    yield history, \"\"\r\n+            response += f\"Found {len(google_urls)} Google URLs: {', '.join(google_urls)}\\n\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n \r\n-    reddit_urls = search_web(message, site=\"reddit.com\")\r\n+            response += f\"Searching Reddit for query: {message}\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n \r\n-    response += f\"Found {len(reddit_urls)} Reddit URLs: {', '.join(reddit_urls)}\\n\\n\"\r\n-    history[-1][\"content\"] = response\r\n-    yield history, \"\"\r\n+            reddit_urls = search_web(message, site=\"reddit.com\")\r\n \r\n-    all_urls = list(set(google_urls + reddit_urls))[:MAX_URLS]\r\n-    response += f\"All unique URLs to process (limited to {MAX_URLS}): {', '.join(all_urls)}\\n\\n\"\r\n-    history[-1][\"content\"] = response\r\n-    yield history, \"\"\r\n+            response += f\"Found {len(reddit_urls)} Reddit URLs: {', '.join(reddit_urls)}\\n\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n \r\n-    process_gen = process_urls(all_urls, response, history, message, conn=conn)\r\n-    for upd in process_gen:\r\n-        yield upd\r\n+            all_urls = list(set(google_urls + reddit_urls))[:MAX_URLS]\r\n+            response += f\"All unique URLs to process (limited to {MAX_URLS}): {', '.join(all_urls)}\\n\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n \r\n-    # Consume the generator fully to get return value\r\n-    try:\r\n-        sources, response, history = next(process_gen)\r\n-    except StopIteration as e:\r\n-        if e.value:\r\n-            sources, response, history = e.value\r\n+            process_gen = process_urls(all_urls, response, history, message, conn=conn)\r\n+            for upd in process_gen:\r\n+                yield upd\r\n+\r\n+            try:\r\n+                sources, response, history = next(process_gen)\r\n+            except StopIteration as e:\r\n+                if e.value:\r\n+                    sources, response, history = e.value\r\n+                else:\r\n+                    sources = []\r\n+\r\n+            search_kwargs = {\"k\": 5}\r\n+            retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n         else:\r\n-            sources = []\r\n+            search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n+            retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n \r\n-    if vectorstore.index.ntotal == 0:\r\n+    if vectorstore.index.ntotal == 0 and selected_source != None:\r\n         response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n         return\r\n \r\n-    response += \"Creating/Updating vector store...\\n\"\r\n-    history[-1][\"content\"] = response\r\n-    yield history, \"\"\r\n+    if selected_source:\r\n+        response += \"Creating/Updating vector store...\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n \r\n-    response += \"Vector store ready.\\n\\n\"\r\n-    history[-1][\"content\"] = response\r\n-    yield history, \"\"\r\n+        response += \"Vector store ready.\\n\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n \r\n-    llm = OllamaLLM(model=MODEL_NAME)\r\n-\r\n     response += \"Generating summarization of the found content...\\n\"\r\n     history[-1][\"content\"] = response\r\n     yield history, \"\"\r\n \r\n-    summary_prompt = ChatPromptTemplate.from_template(\r\n-        \"Summarize the following retrieved content related to the question '{input}' in a concise manner:\\n\\n{context}\"\r\n-    )\r\n-    summary_chain = create_stuff_documents_chain(llm, summary_prompt)\r\n-    search_kwargs = {\"k\": 5}\r\n-    if selected_tag:\r\n-        search_kwargs[\"filter\"] = {\"tag\": selected_tag}\r\n-    retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n-    summary_chain_with_docs = create_retrieval_chain(retriever, summary_chain)\r\n-    summary_response = summary_chain_with_docs.invoke({\"input\": message})\r\n-    summary = summary_response[\"answer\"]\r\n+    summary = \"No summary available.\"\r\n+    if retriever:\r\n+        summary_prompt = ChatPromptTemplate.from_template(\r\n+            \"Summarize the following retrieved content related to the question '{input}' in a concise manner:\\n\\n{context}\"\r\n+        )\r\n+        summary_chain = create_stuff_documents_chain(llm, summary_prompt)\r\n+        summary_chain_with_docs = create_retrieval_chain(retriever, summary_chain)\r\n+        summary_response = summary_chain_with_docs.invoke({\"input\": message})\r\n+        summary = summary_response[\"answer\"]\r\n \r\n     response += f\"**Summarization of Found Content:**\\n{summary}\\n\\n\"\r\n     history[-1][\"content\"] = response\r\n     yield history, \"\"\r\n@@ -102,31 +110,16 @@\n     response += \"Generating specific answer to the prompt...\\n\"\r\n     history[-1][\"content\"] = response\r\n     yield history, \"\"\r\n \r\n-    rephrase_prompt = ChatPromptTemplate.from_messages(\r\n-        [\r\n-            MessagesPlaceholder(variable_name=\"chat_history\"),\r\n-            (\"human\", \"{input}\"),\r\n-            (\"human\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\"),\r\n-        ]\r\n-    )\r\n-    history_aware_retriever = create_history_aware_retriever(llm, retriever, rephrase_prompt)\r\n+    if retriever:\r\n+        rephrase_prompt = ChatPromptTemplate.from_messages(\r\n+            [\r\n+                MessagesPlaceholder(variable_name=\"chat_history\"),\r\n+                (\"human\", \"{input}\"),\r\n+                (\"human\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\"),\r\n+            ]\r\n+        )\r\n+        history_aware_retriever = create_history_aware_retriever(llm, retriever, rephrase_prompt)\r\n \r\n-    qa_prompt = ChatPromptTemplate.from_template(\r\n-        \"Answer the question based only on the following context:\\n\\n{context}\\n\\nQuestion: {input}\\nIf the context doesn't contain relevant information, say 'I don't know'.\"\r\n-    )\r\n-    qa_chain = create_stuff_documents_chain(llm, qa_prompt)\r\n-    qa_chain_with_docs = create_retrieval_chain(history_aware_retriever, qa_chain)\r\n-    qa_response = qa_chain_with_docs.invoke({\"input\": message, \"chat_history\": chat_history})\r\n-    answer = qa_response[\"answer\"]\r\n-\r\n-    response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n\\ No newline at end of file\n-    history[-1][\"content\"] = response\r\n-    yield history, \"\"\r\n-\r\n-    sources_str = \"\\n\".join([f\"- {url}\" for url in sources])\r\n-    response += f\"**Sources Crawled:**\\n{sources_str}\"\r\n-    history[-1][\"content\"] = response\r\n-    yield history, \"\"\r\n-\r\n-    print(\"chat_bot completed.\")\n+        qa_prompt = ChatPromptTemplate.from_template(\r\n+            \"Answer the question based only on the following context:\\n\\n{context}\\n\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756860176607,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -121,5 +121,29 @@\n         )\r\n         history_aware_retriever = create_history_aware_retriever(llm, retriever, rephrase_prompt)\r\n \r\n         qa_prompt = ChatPromptTemplate.from_template(\r\n-            \"Answer the question based only on the following context:\\n\\n{context}\\n\n\\ No newline at end of file\n+            \"Answer the question based only on the following context:\\n\\n{context}\\n\\nQuestion: {input}\\nIf the context doesn't contain relevant information, say 'I don't know'.\"\r\n+        )\r\n+        qa_chain = create_stuff_documents_chain(llm, qa_prompt)\r\n+        qa_chain_with_docs = create_retrieval_chain(history_aware_retriever, qa_chain)\r\n+        qa_response = qa_chain_with_docs.invoke({\"input\": message, \"chat_history\": chat_history})\r\n+        answer = qa_response[\"answer\"]\r\n+    else:\r\n+        qa_prompt = ChatPromptTemplate.from_template(\r\n+            \"Answer the question:\\n\\nQuestion: {input}\"\r\n+        )\r\n+        qa_chain = create_stuff_documents_chain(llm, qa_prompt)\r\n+        qa_response = qa_chain.invoke({\"input\": message})\r\n+        answer = qa_response\r\n+\r\n+    response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n+    history[-1][\"content\"] = response\r\n+    yield history, \"\"\r\n+\r\n+    if selected_source == \"Web Search\":\r\n+        sources_str = \"\\n\".join([f\"- {url}\" for url in sources])\r\n+        response += f\"**Sources Crawled:**\\n{sources_str}\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+    print(\"chat_bot completed.\")\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756860514623,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,9 +14,9 @@\n     print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n     history.append({\"role\": \"user\", \"content\": message})\r\n     yield history, \"\"\r\n \r\n-    response = \"**Processing Status:**\\n\"\r\n+    response = \"\"\r\n     history.append({\"role\": \"assistant\", \"content\": response})\r\n     yield history, \"\"\r\n \r\n     chat_history = []\r\n@@ -28,10 +28,12 @@\n \r\n     llm = OllamaLLM(model=MODEL_NAME)\r\n \r\n     retriever = None\r\n+    sources = []\r\n     if selected_source:\r\n         if selected_source == \"Web Search\":\r\n+            response += \"**Processing Status:**\\n\"\r\n             response += f\"Searching web for query: {message}\\n\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n \r\n@@ -70,49 +72,54 @@\n \r\n             search_kwargs = {\"k\": 5}\r\n             retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n         else:\r\n+            response += \"**Processing Status:**\\n\"\r\n             search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n             retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n \r\n-    if vectorstore.index.ntotal == 0 and selected_source != None:\r\n-        response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-        return\r\n+    if retriever is None:\r\n+        # Direct LLM call without retrieval\r\n+        qa_prompt = ChatPromptTemplate.from_template(\r\n+            \"Answer the question:\\n\\nQuestion: {input}\"\r\n+        )\r\n+        qa_chain = qa_prompt | llm\r\n+        answer = qa_chain.invoke({\"input\": message})\r\n+    else:\r\n+        if vectorstore.index.ntotal == 0:\r\n+            response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+            return\r\n \r\n-    if selected_source:\r\n         response += \"Creating/Updating vector store...\\n\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n \r\n         response += \"Vector store ready.\\n\\n\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n \r\n-    response += \"Generating summarization of the found content...\\n\"\r\n-    history[-1][\"content\"] = response\r\n-    yield history, \"\"\r\n+        response += \"Generating summarization of the found content...\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n \r\n-    summary = \"No summary available.\"\r\n-    if retriever:\r\n         summary_prompt = ChatPromptTemplate.from_template(\r\n             \"Summarize the following retrieved content related to the question '{input}' in a concise manner:\\n\\n{context}\"\r\n         )\r\n         summary_chain = create_stuff_documents_chain(llm, summary_prompt)\r\n         summary_chain_with_docs = create_retrieval_chain(retriever, summary_chain)\r\n         summary_response = summary_chain_with_docs.invoke({\"input\": message})\r\n         summary = summary_response[\"answer\"]\r\n \r\n-    response += f\"**Summarization of Found Content:**\\n{summary}\\n\\n\"\r\n-    history[-1][\"content\"] = response\r\n-    yield history, \"\"\r\n+        response += f\"**Summarization of Found Content:**\\n{summary}\\n\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n \r\n-    response += \"Generating specific answer to the prompt...\\n\"\r\n-    history[-1][\"content\"] = response\r\n-    yield history, \"\"\r\n+        response += \"Generating specific answer to the prompt...\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n \r\n-    if retriever:\r\n         rephrase_prompt = ChatPromptTemplate.from_messages(\r\n             [\r\n                 MessagesPlaceholder(variable_name=\"chat_history\"),\r\n                 (\"human\", \"{input}\"),\r\n@@ -127,15 +134,8 @@\n         qa_chain = create_stuff_documents_chain(llm, qa_prompt)\r\n         qa_chain_with_docs = create_retrieval_chain(history_aware_retriever, qa_chain)\r\n         qa_response = qa_chain_with_docs.invoke({\"input\": message, \"chat_history\": chat_history})\r\n         answer = qa_response[\"answer\"]\r\n-    else:\r\n-        qa_prompt = ChatPromptTemplate.from_template(\r\n-            \"Answer the question:\\n\\nQuestion: {input}\"\r\n-        )\r\n-        qa_chain = create_stuff_documents_chain(llm, qa_prompt)\r\n-        qa_response = qa_chain.invoke({\"input\": message})\r\n-        answer = qa_response\r\n \r\n     response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n     history[-1][\"content\"] = response\r\n     yield history, \"\"\r\n"
                },
                {
                    "date": 1756861137703,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,8 +4,10 @@\n from langchain_core.messages import HumanMessage, AIMessage\r\n from langchain.chains import create_retrieval_chain, create_history_aware_retriever\r\n from langchain.chains.combine_documents import create_stuff_documents_chain\r\n from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\r\n+from langchain.retrievers import EnsembleRetriever\r\n+from langchain_community.retrievers import BM25Retriever\r\n from web_utils import search_web\r\n from config import MAX_URLS\r\n from process_utils import process_urls\r\n from vectorstore_utils import vectorstore\r\n@@ -70,13 +72,21 @@\n                 else:\r\n                     sources = []\r\n \r\n             search_kwargs = {\"k\": 5}\r\n-            retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n+            if 'lyrics' in message.lower():\r\n+                search_kwargs[\"filter\"] = {\"source_type\": \"lyrics\"}\r\n+            dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n+            bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n+            retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n         else:\r\n             response += \"**Processing Status:**\\n\"\r\n             search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n-            retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n+            if 'lyrics' in message.lower():\r\n+                search_kwargs[\"filter\"][\"source_type\"] = \"lyrics\"\r\n+            dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n+            bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n+            retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n \r\n     if retriever is None:\r\n         # Direct LLM call without retrieval\r\n         qa_prompt = ChatPromptTemplate.from_template(\r\n@@ -128,9 +138,9 @@\n         )\r\n         history_aware_retriever = create_history_aware_retriever(llm, retriever, rephrase_prompt)\r\n \r\n         qa_prompt = ChatPromptTemplate.from_template(\r\n-            \"Answer the question based only on the following context:\\n\\n{context}\\n\\nQuestion: {input}\\nIf the context doesn't contain relevant information, say 'I don't know'.\"\r\n+            \"Use the context to answer the question as accurately as possible. If lyrics are present, extract and format them clearly with verses, chorus, etc., and ignore non-lyric content like discussions. If uncertain or incomplete, note limitations but provide what's available:\\n\\n{context}\\n\\nQuestion: {input}\"\r\n         )\r\n         qa_chain = create_stuff_documents_chain(llm, qa_prompt)\r\n         qa_chain_with_docs = create_retrieval_chain(history_aware_retriever, qa_chain)\r\n         qa_response = qa_chain_with_docs.invoke({\"input\": message, \"chat_history\": chat_history})\r\n"
                },
                {
                    "date": 1756861494357,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,161 @@\n+import os\r\n+from config import MODEL_NAME\r\n+from langchain_ollama import OllamaLLM\r\n+from langchain_core.messages import HumanMessage, AIMessage\r\n+from langchain.chains import create_retrieval_chain, create_history_aware_retriever\r\n+from langchain.chains.combine_documents import create_stuff_documents_chain\r\n+from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\r\n+from langchain.retrievers import EnsembleRetriever\r\n+from langchain_community.retrievers import BM25Retriever\r\n+from web_utils import search_web\r\n+from config import MAX_URLS\r\n+from process_utils import process_urls\r\n+from vectorstore_utils import vectorstore\r\n+\r\n+def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n+    print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n+    history.append({\"role\": \"user\", \"content\": message})\r\n+    yield history, \"\"\r\n+\r\n+    response = \"\"\r\n+    history.append({\"role\": \"assistant\", \"content\": response})\r\n+    yield history, \"\"\r\n+\r\n+    chat_history = []\r\n+    for h in history[:-1]:\r\n+        if h[\"role\"] == \"user\":\r\n+            chat_history.append(HumanMessage(content=h[\"content\"]))\r\n+        elif h[\"role\"] == \"assistant\":\r\n+            chat_history.append(AIMessage(content=h[\"content\"]))\r\n+\r\n+    llm = OllamaLLM(model=MODEL_NAME)\r\n+\r\n+    retriever = None\r\n+    sources = []\r\n+    if selected_source == \"No RAG\":\r\n+        selected_source = None\r\n+    if selected_source:\r\n+        if selected_source == \"Web Search\":\r\n+            response += \"**Processing Status:**\\n\"\r\n+            response += f\"Searching web for query: {message}\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+\r\n+            google_urls = search_web(message)\r\n+\r\n+            response += f\"Found {len(google_urls)} Google URLs: {', '.join(google_urls)}\\n\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+\r\n+            response += f\"Searching Reddit for query: {message}\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+\r\n+            reddit_urls = search_web(message, site=\"reddit.com\")\r\n+\r\n+            response += f\"Found {len(reddit_urls)} Reddit URLs: {', '.join(reddit_urls)}\\n\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+\r\n+            all_urls = list(set(google_urls + reddit_urls))[:MAX_URLS]\r\n+            response += f\"All unique URLs to process (limited to {MAX_URLS}): {', '.join(all_urls)}\\n\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+\r\n+            process_gen = process_urls(all_urls, response, history, message, conn=conn)\r\n+            for upd in process_gen:\r\n+                yield upd\r\n+\r\n+            try:\r\n+                sources, response, history = next(process_gen)\r\n+            except StopIteration as e:\r\n+                if e.value:\r\n+                    sources, response, history = e.value\r\n+                else:\r\n+                    sources = []\r\n+\r\n+            search_kwargs = {\"k\": 5}\r\n+            if 'lyrics' in message.lower():\r\n+                search_kwargs[\"filter\"] = {\"source_type\": \"lyrics\"}\r\n+            dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n+            bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n+            retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n+        else:\r\n+            response += \"**Processing Status:**\\n\"\r\n+            search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n+            if 'lyrics' in message.lower():\r\n+                search_kwargs[\"filter\"][\"source_type\"] = \"lyrics\"\r\n+            dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n+            bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n+            retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n+\r\n+    if retriever is None:\r\n+        # Direct LLM call without retrieval\r\n+        qa_prompt = ChatPromptTemplate.from_template(\r\n+            \"Answer the question:\\n\\nQuestion: {input}\"\r\n+        )\r\n+        qa_chain = qa_prompt | llm\r\n+        answer = qa_chain.invoke({\"input\": message})\r\n+    else:\r\n+        if vectorstore.index.ntotal == 0:\r\n+            response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+            return\r\n+\r\n+        response += \"Creating/Updating vector store...\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+        response += \"Vector store ready.\\n\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+        response += \"Generating summarization of the found content...\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+        summary_prompt = ChatPromptTemplate.from_template(\r\n+            \"Summarize the following retrieved content related to the question '{input}' in a concise manner:\\n\\n{context}\"\r\n+        )\r\n+        summary_chain = create_stuff_documents_chain(llm, summary_prompt)\r\n+        summary_chain_with_docs = create_retrieval_chain(retriever, summary_chain)\r\n+        summary_response = summary_chain_with_docs.invoke({\"input\": message})\r\n+        summary = summary_response[\"answer\"]\r\n+\r\n+        response += f\"**Summarization of Found Content:**\\n{summary}\\n\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+        response += \"Generating specific answer to the prompt...\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+        rephrase_prompt = ChatPromptTemplate.from_messages(\r\n+            [\r\n+                MessagesPlaceholder(variable_name=\"chat_history\"),\r\n+                (\"human\", \"{input}\"),\r\n+                (\"human\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\"),\r\n+            ]\r\n+        )\r\n+        history_aware_retriever = create_history_aware_retriever(llm, retriever, rephrase_prompt)\r\n+\r\n+        qa_prompt = ChatPromptTemplate.from_template(\r\n+            \"Use the context to answer the question as accurately as possible. If lyrics are present, extract and format them clearly with verses, chorus, etc., and ignore non-lyric content like discussions. If uncertain or incomplete, note limitations but provide what's available:\\n\\n{context}\\n\\nQuestion: {input}\"\r\n+        )\r\n+        qa_chain = create_stuff_documents_chain(llm, qa_prompt)\r\n+        qa_chain_with_docs = create_retrieval_chain(history_aware_retriever, qa_chain)\r\n+        qa_response = qa_chain_with_docs.invoke({\"input\": message, \"chat_history\": chat_history})\r\n+        answer = qa_response[\"answer\"]\r\n+\r\n+    response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n+    history[-1][\"content\"] = response\r\n+    yield history, \"\"\r\n+\r\n+    if selected_source == \"Web Search\":\r\n+        sources_str = \"\\n\".join([f\"- {url}\" for url in sources])\r\n+        response += f\"**Sources Crawled:**\\n{sources_str}\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+    print(\"chat_bot completed.\")\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756861577617,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -157,164 +157,5 @@\n         response += f\"**Sources Crawled:**\\n{sources_str}\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n \r\n-    print(\"chat_bot completed.\")\n-import os\r\n-from config import MODEL_NAME\r\n-from langchain_ollama import OllamaLLM\r\n-from langchain_core.messages import HumanMessage, AIMessage\r\n-from langchain.chains import create_retrieval_chain, create_history_aware_retriever\r\n-from langchain.chains.combine_documents import create_stuff_documents_chain\r\n-from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\r\n-from langchain.retrievers import EnsembleRetriever\r\n-from langchain_community.retrievers import BM25Retriever\r\n-from web_utils import search_web\r\n-from config import MAX_URLS\r\n-from process_utils import process_urls\r\n-from vectorstore_utils import vectorstore\r\n-\r\n-def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n-    print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n-    history.append({\"role\": \"user\", \"content\": message})\r\n-    yield history, \"\"\r\n-\r\n-    response = \"\"\r\n-    history.append({\"role\": \"assistant\", \"content\": response})\r\n-    yield history, \"\"\r\n-\r\n-    chat_history = []\r\n-    for h in history[:-1]:\r\n-        if h[\"role\"] == \"user\":\r\n-            chat_history.append(HumanMessage(content=h[\"content\"]))\r\n-        elif h[\"role\"] == \"assistant\":\r\n-            chat_history.append(AIMessage(content=h[\"content\"]))\r\n-\r\n-    llm = OllamaLLM(model=MODEL_NAME)\r\n-\r\n-    retriever = None\r\n-    sources = []\r\n-    if selected_source:\r\n-        if selected_source == \"Web Search\":\r\n-            response += \"**Processing Status:**\\n\"\r\n-            response += f\"Searching web for query: {message}\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            google_urls = search_web(message)\r\n-\r\n-            response += f\"Found {len(google_urls)} Google URLs: {', '.join(google_urls)}\\n\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            response += f\"Searching Reddit for query: {message}\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            reddit_urls = search_web(message, site=\"reddit.com\")\r\n-\r\n-            response += f\"Found {len(reddit_urls)} Reddit URLs: {', '.join(reddit_urls)}\\n\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            all_urls = list(set(google_urls + reddit_urls))[:MAX_URLS]\r\n-            response += f\"All unique URLs to process (limited to {MAX_URLS}): {', '.join(all_urls)}\\n\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            process_gen = process_urls(all_urls, response, history, message, conn=conn)\r\n-            for upd in process_gen:\r\n-                yield upd\r\n-\r\n-            try:\r\n-                sources, response, history = next(process_gen)\r\n-            except StopIteration as e:\r\n-                if e.value:\r\n-                    sources, response, history = e.value\r\n-                else:\r\n-                    sources = []\r\n-\r\n-            search_kwargs = {\"k\": 5}\r\n-            if 'lyrics' in message.lower():\r\n-                search_kwargs[\"filter\"] = {\"source_type\": \"lyrics\"}\r\n-            dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n-            bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n-            retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n-        else:\r\n-            response += \"**Processing Status:**\\n\"\r\n-            search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n-            if 'lyrics' in message.lower():\r\n-                search_kwargs[\"filter\"][\"source_type\"] = \"lyrics\"\r\n-            dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n-            bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n-            retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n-\r\n-    if retriever is None:\r\n-        # Direct LLM call without retrieval\r\n-        qa_prompt = ChatPromptTemplate.from_template(\r\n-            \"Answer the question:\\n\\nQuestion: {input}\"\r\n-        )\r\n-        qa_chain = qa_prompt | llm\r\n-        answer = qa_chain.invoke({\"input\": message})\r\n-    else:\r\n-        if vectorstore.index.ntotal == 0:\r\n-            response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-            return\r\n-\r\n-        response += \"Creating/Updating vector store...\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n-        response += \"Vector store ready.\\n\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n-        response += \"Generating summarization of the found content...\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n-        summary_prompt = ChatPromptTemplate.from_template(\r\n-            \"Summarize the following retrieved content related to the question '{input}' in a concise manner:\\n\\n{context}\"\r\n-        )\r\n-        summary_chain = create_stuff_documents_chain(llm, summary_prompt)\r\n-        summary_chain_with_docs = create_retrieval_chain(retriever, summary_chain)\r\n-        summary_response = summary_chain_with_docs.invoke({\"input\": message})\r\n-        summary = summary_response[\"answer\"]\r\n-\r\n-        response += f\"**Summarization of Found Content:**\\n{summary}\\n\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n-        response += \"Generating specific answer to the prompt...\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n-        rephrase_prompt = ChatPromptTemplate.from_messages(\r\n-            [\r\n-                MessagesPlaceholder(variable_name=\"chat_history\"),\r\n-                (\"human\", \"{input}\"),\r\n-                (\"human\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\"),\r\n-            ]\r\n-        )\r\n-        history_aware_retriever = create_history_aware_retriever(llm, retriever, rephrase_prompt)\r\n-\r\n-        qa_prompt = ChatPromptTemplate.from_template(\r\n-            \"Use the context to answer the question as accurately as possible. If lyrics are present, extract and format them clearly with verses, chorus, etc., and ignore non-lyric content like discussions. If uncertain or incomplete, note limitations but provide what's available:\\n\\n{context}\\n\\nQuestion: {input}\"\r\n-        )\r\n-        qa_chain = create_stuff_documents_chain(llm, qa_prompt)\r\n-        qa_chain_with_docs = create_retrieval_chain(history_aware_retriever, qa_chain)\r\n-        qa_response = qa_chain_with_docs.invoke({\"input\": message, \"chat_history\": chat_history})\r\n-        answer = qa_response[\"answer\"]\r\n-\r\n-    response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n-    history[-1][\"content\"] = response\r\n-    yield history, \"\"\r\n-\r\n-    if selected_source == \"Web Search\":\r\n-        sources_str = \"\\n\".join([f\"- {url}\" for url in sources])\r\n-        response += f\"**Sources Crawled:**\\n{sources_str}\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n     print(\"chat_bot completed.\")\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756861768518,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -95,8 +95,11 @@\n             \"Answer the question:\\n\\nQuestion: {input}\"\r\n         )\r\n         qa_chain = qa_prompt | llm\r\n         answer = qa_chain.invoke({\"input\": message})\r\n+        response = f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n     else:\r\n         if vectorstore.index.ntotal == 0:\r\n             response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n             history[-1][\"content\"] = response\r\n@@ -147,11 +150,11 @@\n         qa_chain_with_docs = create_retrieval_chain(history_aware_retriever, qa_chain)\r\n         qa_response = qa_chain_with_docs.invoke({\"input\": message, \"chat_history\": chat_history})\r\n         answer = qa_response[\"answer\"]\r\n \r\n-    response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n-    history[-1][\"content\"] = response\r\n-    yield history, \"\"\r\n+        response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n \r\n     if selected_source == \"Web Search\":\r\n         sources_str = \"\\n\".join([f\"- {url}\" for url in sources])\r\n         response += f\"**Sources Crawled:**\\n{sources_str}\"\r\n"
                },
                {
                    "date": 1756865704879,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,8 +10,10 @@\n from web_utils import search_web\r\n from config import MAX_URLS\r\n from process_utils import process_urls\r\n from vectorstore_utils import vectorstore\r\n+from youtube_transcript_api import YouTubeTranscriptApi\r\n+import re\r\n \r\n def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n     print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n     history.append({\"role\": \"user\", \"content\": message})\r\n@@ -79,8 +81,60 @@\n                 search_kwargs[\"filter\"] = {\"source_type\": \"lyrics\"}\r\n             dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n             bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n             retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n+        elif selected_source == \"YouTube\":\r\n+            response += \"**Processing Status:**\\n\"\r\n+            response += f\"Searching YouTube for query: {message}\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+\r\n+            youtube_urls = search_web(message, site=\"youtube.com\")\r\n+\r\n+            response += f\"Found {len(youtube_urls)} YouTube URLs: {', '.join(youtube_urls)}\\n\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+\r\n+            all_urls = list(set(youtube_urls))[:MAX_URLS]\r\n+            response += f\"All unique URLs to process (limited to {MAX_URLS}): {', '.join(all_urls)}\\n\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+\r\n+            transcripts = []\r\n+            for url in all_urls:\r\n+                match = re.search(r\"v=([^&]+)\", url)\r\n+                if match:\r\n+                    video_id = match.group(1)\r\n+                    try:\r\n+                        transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\r\n+                        transcript_text = \" \".join([entry['text'] for entry in transcript_list])\r\n+                        transcripts.append(transcript_text)\r\n+                        response += f\"Transcript fetched for {url}\\n\"\r\n+                        history[-1][\"content\"] = response\r\n+                        yield history, \"\"\r\n+                    except Exception as e:\r\n+                        response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n+                        history[-1][\"content\"] = response\r\n+                        yield history, \"\"\r\n+\r\n+            combined_transcript = \" \".join(transcripts)\r\n+            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n+            chunks = text_splitter.split_text(combined_transcript)\r\n+            new_docs = []\r\n+            for chunk in chunks:\r\n+                if add_chunk_if_new(conn, chunk, \"YouTube transcripts\"):\r\n+                    metadata = {\"source\": \"YouTube\", \"tag\": \"youtube_\" + message.replace(\" \", \"_\")}\r\n+                    new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+\r\n+            if new_docs:\r\n+                with lock:\r\n+                    vectorstore.add_documents(new_docs)\r\n+                    vectorstore.save_local(FAISS_PATH)\r\n+\r\n+            search_kwargs = {\"k\": 5, \"filter\": {\"tag\": \"youtube_\" + message.replace(\" \", \"_\")}}\r\n+            dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n+            bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n+            retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n         else:\r\n             response += \"**Processing Status:**\\n\"\r\n             search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n             if 'lyrics' in message.lower():\r\n"
                },
                {
                    "date": 1756865834779,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,17 +1,21 @@\n import os\r\n-from config import MODEL_NAME\r\n+from config import MODEL_NAME, FAISS_PATH\r\n from langchain_ollama import OllamaLLM\r\n from langchain_core.messages import HumanMessage, AIMessage\r\n from langchain.chains import create_retrieval_chain, create_history_aware_retriever\r\n from langchain.chains.combine_documents import create_stuff_documents_chain\r\n from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\r\n from langchain.retrievers import EnsembleRetriever\r\n from langchain_community.retrievers import BM25Retriever\r\n+from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n+from langchain_core.documents import Document\r\n from web_utils import search_web\r\n from config import MAX_URLS\r\n from process_utils import process_urls\r\n from vectorstore_utils import vectorstore\r\n+from db_utils import add_chunk_if_new\r\n+from utils import lock\r\n from youtube_transcript_api import YouTubeTranscriptApi\r\n import re\r\n \r\n def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n@@ -208,9 +212,9 @@\n         response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n \r\n-    if selected_source == \"Web Search\":\r\n+    if selected_source in [\"Web Search\", \"YouTube\"]:\r\n         sources_str = \"\\n\".join([f\"- {url}\" for url in sources])\r\n         response += f\"**Sources Crawled:**\\n{sources_str}\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n"
                },
                {
                    "date": 1756865958642,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,6 +1,6 @@\n import os\r\n-from config import MODEL_NAME, FAISS_PATH\r\n+from config import MODEL_NAME\r\n from langchain_ollama import OllamaLLM\r\n from langchain_core.messages import HumanMessage, AIMessage\r\n from langchain.chains import create_retrieval_chain, create_history_aware_retriever\r\n from langchain.chains.combine_documents import create_stuff_documents_chain\r\n@@ -108,10 +108,11 @@\n                 match = re.search(r\"v=([^&]+)\", url)\r\n                 if match:\r\n                     video_id = match.group(1)\r\n                     try:\r\n-                        transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\r\n-                        transcript_text = \" \".join([entry['text'] for entry in transcript_list])\r\n+                        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\r\n+                        transcript = transcript_list.find_generated_transcript(['en']) or transcript_list.find_transcript(['en'])\r\n+                        transcript_text = \" \".join([entry['text'] for entry in transcript.fetch()])\r\n                         transcripts.append(transcript_text)\r\n                         response += f\"Transcript fetched for {url}\\n\"\r\n                         history[-1][\"content\"] = response\r\n                         yield history, \"\"\r\n"
                },
                {
                    "date": 1756866158513,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -108,11 +108,10 @@\n                 match = re.search(r\"v=([^&]+)\", url)\r\n                 if match:\r\n                     video_id = match.group(1)\r\n                     try:\r\n-                        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\r\n-                        transcript = transcript_list.find_generated_transcript(['en']) or transcript_list.find_transcript(['en'])\r\n-                        transcript_text = \" \".join([entry['text'] for entry in transcript.fetch()])\r\n+                        transcript_list = YouTubeTranscriptApi.get_transcripts(video_id, languages=['en'])\r\n+                        transcript_text = \" \".join([entry['text'] for entry in transcript_list[0]])\r\n                         transcripts.append(transcript_text)\r\n                         response += f\"Transcript fetched for {url}\\n\"\r\n                         history[-1][\"content\"] = response\r\n                         yield history, \"\"\r\n"
                },
                {
                    "date": 1756866300214,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -108,10 +108,11 @@\n                 match = re.search(r\"v=([^&]+)\", url)\r\n                 if match:\r\n                     video_id = match.group(1)\r\n                     try:\r\n-                        transcript_list = YouTubeTranscriptApi.get_transcripts(video_id, languages=['en'])\r\n-                        transcript_text = \" \".join([entry['text'] for entry in transcript_list[0]])\r\n+                        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\r\n+                        transcript = transcript_list.find_generated_transcript(['en']) or transcript_list.find_transcript(['en'])\r\n+                        transcript_text = \" \".join([entry['text'] for entry in transcript.fetch()])\r\n                         transcripts.append(transcript_text)\r\n                         response += f\"Transcript fetched for {url}\\n\"\r\n                         history[-1][\"content\"] = response\r\n                         yield history, \"\"\r\n"
                },
                {
                    "date": 1756870303519,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -108,13 +108,39 @@\n                 match = re.search(r\"v=([^&]+)\", url)\r\n                 if match:\r\n                     video_id = match.group(1)\r\n                     try:\r\n-                        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\r\n-                        transcript = transcript_list.find_generated_transcript(['en']) or transcript_list.find_transcript(['en'])\r\n-                        transcript_text = \" \".join([entry['text'] for entry in transcript.fetch()])\r\n-                        transcripts.append(transcript_text)\r\n-                        response += f\"Transcript fetched for {url}\\n\"\r\n+                        # Debug: Print library version and available methods\r\n+                        print(f\"Debug: youtube_transcript_api version: {YouTubeTranscriptApi.__version__}\")\r\n+                        print(f\"Debug: Available methods on YouTubeTranscriptApi: {dir(YouTubeTranscriptApi)}\")\r\n+\r\n+                        # Approach 1: Direct get_transcript (fallback for older versions)\r\n+                        try:\r\n+                            transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\r\n+                            transcript_text = \" \".join([entry['text'] for entry in transcript_list])\r\n+                            transcripts.append(transcript_text)\r\n+                            response += f\"Transcript fetched for {url} (Approach 1: direct get_transcript)\\n\"\r\n+                        except Exception as e1:\r\n+                            response += f\"Debug: Approach 1 failed: {e1}\\n\"\r\n+                            # Approach 2: list_transcripts and fetch (for newer versions)\r\n+                            try:\r\n+                                transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\r\n+                                transcript = transcript_list.find_generated_transcript(['en']) or transcript_list.find_transcript(['en'])\r\n+                                transcript_text = \" \".join([entry['text'] for entry in transcript.fetch()])\r\n+                                transcripts.append(transcript_text)\r\n+                                response += f\"Transcript fetched for {url} (Approach 2: list_transcripts and fetch)\\n\"\r\n+                            except Exception as e2:\r\n+                                response += f\"Debug: Approach 2 failed: {e2}\\n\"\r\n+                                # Approach 3: Try manual language fallback\r\n+                                try:\r\n+                                    transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\r\n+                                    transcript_text = \" \".join([entry['text'] for entry in transcript_list])\r\n+                                    transcripts.append(transcript_text)\r\n+                                    response += f\"Transcript fetched for {url} (Approach 3: get_transcript without language)\\n\"\r\n+                                except Exception as e3:\r\n+                                    response += f\"Debug: Approach 3 failed: {e3}\\n\"\r\n+                                    response += f\"Error fetching transcript for {url}: All approaches failed.\\n\"\r\n+\r\n                         history[-1][\"content\"] = response\r\n                         yield history, \"\"\r\n                     except Exception as e:\r\n                         response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n"
                },
                {
                    "date": 1756873100613,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -108,35 +108,34 @@\n                 match = re.search(r\"v=([^&]+)\", url)\r\n                 if match:\r\n                     video_id = match.group(1)\r\n                     try:\r\n-                        # Debug: Print library version and available methods\r\n-                        print(f\"Debug: youtube_transcript_api version: {YouTubeTranscriptApi.__version__}\")\r\n+                        # Debug: Available methods on YouTubeTranscriptApi\r\n                         print(f\"Debug: Available methods on YouTubeTranscriptApi: {dir(YouTubeTranscriptApi)}\")\r\n \r\n-                        # Approach 1: Direct get_transcript (fallback for older versions)\r\n+                        # Approach 1: get_transcript with language\r\n                         try:\r\n                             transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\r\n                             transcript_text = \" \".join([entry['text'] for entry in transcript_list])\r\n                             transcripts.append(transcript_text)\r\n-                            response += f\"Transcript fetched for {url} (Approach 1: direct get_transcript)\\n\"\r\n+                            response += f\"Transcript fetched for {url} (Approach 1: get_transcript with en)\\n\"\r\n                         except Exception as e1:\r\n                             response += f\"Debug: Approach 1 failed: {e1}\\n\"\r\n-                            # Approach 2: list_transcripts and fetch (for newer versions)\r\n+                            # Approach 2: get_transcript without language\r\n                             try:\r\n-                                transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\r\n-                                transcript = transcript_list.find_generated_transcript(['en']) or transcript_list.find_transcript(['en'])\r\n-                                transcript_text = \" \".join([entry['text'] for entry in transcript.fetch()])\r\n+                                transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\r\n+                                transcript_text = \" \".join([entry['text'] for entry in transcript_list])\r\n                                 transcripts.append(transcript_text)\r\n-                                response += f\"Transcript fetched for {url} (Approach 2: list_transcripts and fetch)\\n\"\r\n+                                response += f\"Transcript fetched for {url} (Approach 2: get_transcript without language)\\n\"\r\n                             except Exception as e2:\r\n                                 response += f\"Debug: Approach 2 failed: {e2}\\n\"\r\n-                                # Approach 3: Try manual language fallback\r\n+                                # Approach 3: list_transcripts and fetch\r\n                                 try:\r\n-                                    transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\r\n-                                    transcript_text = \" \".join([entry['text'] for entry in transcript_list])\r\n+                                    transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\r\n+                                    transcript = transcript_list.find_generated_transcript(['en']) or transcript_list.find_transcript(['en'])\r\n+                                    transcript_text = \" \".join([entry['text'] for entry in transcript.fetch()])\r\n                                     transcripts.append(transcript_text)\r\n-                                    response += f\"Transcript fetched for {url} (Approach 3: get_transcript without language)\\n\"\r\n+                                    response += f\"Transcript fetched for {url} (Approach 3: list_transcripts and fetch)\\n\"\r\n                                 except Exception as e3:\r\n                                     response += f\"Debug: Approach 3 failed: {e3}\\n\"\r\n                                     response += f\"Error fetching transcript for {url}: All approaches failed.\\n\"\r\n \r\n"
                },
                {
                    "date": 1756873228655,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -129,10 +129,10 @@\n                             except Exception as e2:\r\n                                 response += f\"Debug: Approach 2 failed: {e2}\\n\"\r\n                                 # Approach 3: list_transcripts and fetch\r\n                                 try:\r\n-                                    transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\r\n-                                    transcript = transcript_list.find_generated_transcript(['en']) or transcript_list.find_transcript(['en'])\r\n+                                    transcript_list_obj = YouTubeTranscriptApi.list_transcripts(video_id)\r\n+                                    transcript = transcript_list_obj.find_generated_transcript(['en']) or transcript_list_obj.find_transcript(['en'])\r\n                                     transcript_text = \" \".join([entry['text'] for entry in transcript.fetch()])\r\n                                     transcripts.append(transcript_text)\r\n                                     response += f\"Transcript fetched for {url} (Approach 3: list_transcripts and fetch)\\n\"\r\n                                 except Exception as e3:\r\n"
                },
                {
                    "date": 1756873475777,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,5 +1,7 @@\n import os\r\n+import requests\r\n+import json\r\n from config import MODEL_NAME\r\n from langchain_ollama import OllamaLLM\r\n from langchain_core.messages import HumanMessage, AIMessage\r\n from langchain.chains import create_retrieval_chain, create_history_aware_retriever\r\n@@ -14,9 +16,8 @@\n from process_utils import process_urls\r\n from vectorstore_utils import vectorstore\r\n from db_utils import add_chunk_if_new\r\n from utils import lock\r\n-from youtube_transcript_api import YouTubeTranscriptApi\r\n import re\r\n \r\n def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n     print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n@@ -136,9 +137,23 @@\n                                     transcripts.append(transcript_text)\r\n                                     response += f\"Transcript fetched for {url} (Approach 3: list_transcripts and fetch)\\n\"\r\n                                 except Exception as e3:\r\n                                     response += f\"Debug: Approach 3 failed: {e3}\\n\"\r\n-                                    response += f\"Error fetching transcript for {url}: All approaches failed.\\n\"\r\n+                                    # Approach 4: Direct HTTP to timedtext API (JSON3 format)\r\n+                                    try:\r\n+                                        api_url = f\"https://www.youtube.com/api/timedtext?v={video_id}&lang=en&fmt=json3\"\r\n+                                        api_response = requests.get(api_url)\r\n+                                        print(f\"Debug: Timedtext API status: {api_response.status_code}, content: {api_response.text[:200]}\")\r\n+                                        if api_response.status_code == 200:\r\n+                                            json_data = api_response.json()\r\n+                                            transcript_text = \" \".join([event['segs'][0]['utf8'] for event in json_data['events'] if 'segs' in event and event['segs']])\r\n+                                            transcripts.append(transcript_text)\r\n+                                            response += f\"Transcript fetched for {url} (Approach 4: direct timedtext API)\\n\"\r\n+                                        else:\r\n+                                            response += f\"Debug: Approach 4 failed: Status {api_response.status_code}\\n\"\r\n+                                    except Exception as e4:\r\n+                                        response += f\"Debug: Approach 4 failed: {e4}\\n\"\r\n+                                        response += f\"Error fetching transcript for {url}: All approaches failed.\\n\"\r\n \r\n                         history[-1][\"content\"] = response\r\n                         yield history, \"\"\r\n                     except Exception as e:\r\n"
                },
                {
                    "date": 1756874192863,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,5 @@\n import os\r\n-import requests\r\n-import json\r\n from config import MODEL_NAME\r\n from langchain_ollama import OllamaLLM\r\n from langchain_core.messages import HumanMessage, AIMessage\r\n from langchain.chains import create_retrieval_chain, create_history_aware_retriever\r\n@@ -17,11 +15,13 @@\n from vectorstore_utils import vectorstore\r\n from db_utils import add_chunk_if_new\r\n from utils import lock\r\n import re\r\n+import requests\r\n+import json\r\n \r\n-def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n-    print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n+def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None, selected_embedding=None):\r\n+    print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}, selected_embedding: {selected_embedding}\")\r\n     history.append({\"role\": \"user\", \"content\": message})\r\n     yield history, \"\"\r\n \r\n     response = \"\"\r\n@@ -165,9 +165,9 @@\n             text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n             chunks = text_splitter.split_text(combined_transcript)\r\n             new_docs = []\r\n             for chunk in chunks:\r\n-                if add_chunk_if_new(conn, chunk, \"YouTube transcripts\"):\r\n+                if add_chunk_if_new(conn, chunk, \"YouTube transcripts\", tag=\"youtube_\" + message.replace(\" \", \"_\")):\r\n                     metadata = {\"source\": \"YouTube\", \"tag\": \"youtube_\" + message.replace(\" \", \"_\")}\r\n                     new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n \r\n             if new_docs:\r\n"
                },
                {
                    "date": 1756874507529,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,10 +18,10 @@\n import re\r\n import requests\r\n import json\r\n \r\n-def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None, selected_embedding=None):\r\n-    print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}, selected_embedding: {selected_embedding}\")\r\n+def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n+    print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n     history.append({\"role\": \"user\", \"content\": message})\r\n     yield history, \"\"\r\n \r\n     response = \"\"\r\n@@ -109,52 +109,20 @@\n                 match = re.search(r\"v=([^&]+)\", url)\r\n                 if match:\r\n                     video_id = match.group(1)\r\n                     try:\r\n-                        # Debug: Available methods on YouTubeTranscriptApi\r\n-                        print(f\"Debug: Available methods on YouTubeTranscriptApi: {dir(YouTubeTranscriptApi)}\")\r\n-\r\n-                        # Approach 1: get_transcript with language\r\n-                        try:\r\n-                            transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\r\n-                            transcript_text = \" \".join([entry['text'] for entry in transcript_list])\r\n+                        # Approach 4: Direct HTTP to timedtext API (JSON3 format)\r\n+                        api_url = f\"https://www.youtube.com/api/timedtext?v={video_id}&lang=en&fmt=json3\"\r\n+                        api_response = requests.get(api_url)\r\n+                        print(f\"Debug: Timedtext API status: {api_response.status_code}, content: {api_response.text[:200]}\")\r\n+                        if api_response.status_code == 200:\r\n+                            json_data = api_response.json()\r\n+                            transcript_text = \" \".join([event['segs'][0]['utf8'] for event in json_data['events'] if 'segs' in event and event['segs']])\r\n                             transcripts.append(transcript_text)\r\n-                            response += f\"Transcript fetched for {url} (Approach 1: get_transcript with en)\\n\"\r\n-                        except Exception as e1:\r\n-                            response += f\"Debug: Approach 1 failed: {e1}\\n\"\r\n-                            # Approach 2: get_transcript without language\r\n-                            try:\r\n-                                transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\r\n-                                transcript_text = \" \".join([entry['text'] for entry in transcript_list])\r\n-                                transcripts.append(transcript_text)\r\n-                                response += f\"Transcript fetched for {url} (Approach 2: get_transcript without language)\\n\"\r\n-                            except Exception as e2:\r\n-                                response += f\"Debug: Approach 2 failed: {e2}\\n\"\r\n-                                # Approach 3: list_transcripts and fetch\r\n-                                try:\r\n-                                    transcript_list_obj = YouTubeTranscriptApi.list_transcripts(video_id)\r\n-                                    transcript = transcript_list_obj.find_generated_transcript(['en']) or transcript_list_obj.find_transcript(['en'])\r\n-                                    transcript_text = \" \".join([entry['text'] for entry in transcript.fetch()])\r\n-                                    transcripts.append(transcript_text)\r\n-                                    response += f\"Transcript fetched for {url} (Approach 3: list_transcripts and fetch)\\n\"\r\n-                                except Exception as e3:\r\n-                                    response += f\"Debug: Approach 3 failed: {e3}\\n\"\r\n-                                    # Approach 4: Direct HTTP to timedtext API (JSON3 format)\r\n-                                    try:\r\n-                                        api_url = f\"https://www.youtube.com/api/timedtext?v={video_id}&lang=en&fmt=json3\"\r\n-                                        api_response = requests.get(api_url)\r\n-                                        print(f\"Debug: Timedtext API status: {api_response.status_code}, content: {api_response.text[:200]}\")\r\n-                                        if api_response.status_code == 200:\r\n-                                            json_data = api_response.json()\r\n-                                            transcript_text = \" \".join([event['segs'][0]['utf8'] for event in json_data['events'] if 'segs' in event and event['segs']])\r\n-                                            transcripts.append(transcript_text)\r\n-                                            response += f\"Transcript fetched for {url} (Approach 4: direct timedtext API)\\n\"\r\n-                                        else:\r\n-                                            response += f\"Debug: Approach 4 failed: Status {api_response.status_code}\\n\"\r\n-                                    except Exception as e4:\r\n-                                        response += f\"Debug: Approach 4 failed: {e4}\\n\"\r\n-                                        response += f\"Error fetching transcript for {url}: All approaches failed.\\n\"\r\n-\r\n+                            response += f\"Transcript fetched for {url} (Approach 4: direct timedtext API)\\n\"\r\n+                        else:\r\n+                            response += f\"Debug: Approach 4 failed: Status {api_response.status_code}\\n\"\r\n+                            response += f\"Error fetching transcript for {url}: All approaches failed.\\n\"\r\n                         history[-1][\"content\"] = response\r\n                         yield history, \"\"\r\n                     except Exception as e:\r\n                         response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n@@ -165,9 +133,9 @@\n             text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n             chunks = text_splitter.split_text(combined_transcript)\r\n             new_docs = []\r\n             for chunk in chunks:\r\n-                if add_chunk_if_new(conn, chunk, \"YouTube transcripts\", tag=\"youtube_\" + message.replace(\" \", \"_\")):\r\n+                if add_chunk_if_new(conn, chunk, \"YouTube transcripts\"):\r\n                     metadata = {\"source\": \"YouTube\", \"tag\": \"youtube_\" + message.replace(\" \", \"_\")}\r\n                     new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n \r\n             if new_docs:\r\n"
                },
                {
                    "date": 1756874724799,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -109,20 +109,65 @@\n                 match = re.search(r\"v=([^&]+)\", url)\r\n                 if match:\r\n                     video_id = match.group(1)\r\n                     try:\r\n-                        # Approach 4: Direct HTTP to timedtext API (JSON3 format)\r\n-                        api_url = f\"https://www.youtube.com/api/timedtext?v={video_id}&lang=en&fmt=json3\"\r\n-                        api_response = requests.get(api_url)\r\n-                        print(f\"Debug: Timedtext API status: {api_response.status_code}, content: {api_response.text[:200]}\")\r\n-                        if api_response.status_code == 200:\r\n-                            json_data = api_response.json()\r\n-                            transcript_text = \" \".join([event['segs'][0]['utf8'] for event in json_data['events'] if 'segs' in event and event['segs']])\r\n+                        # Debug: Available methods on YouTubeTranscriptApi\r\n+                        print(f\"Debug: Available methods on YouTubeTranscriptApi: {dir(YouTubeTranscriptApi)}\")\r\n+\r\n+                        # Approach 1: get_transcript with language\r\n+                        try:\r\n+                            transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\r\n+                            transcript_text = \" \".join([entry['text'] for entry in transcript_list])\r\n                             transcripts.append(transcript_text)\r\n-                            response += f\"Transcript fetched for {url} (Approach 4: direct timedtext API)\\n\"\r\n-                        else:\r\n-                            response += f\"Debug: Approach 4 failed: Status {api_response.status_code}\\n\"\r\n-                            response += f\"Error fetching transcript for {url}: All approaches failed.\\n\"\r\n+                            response += f\"Transcript fetched for {url} (Approach 1: get_transcript with en)\\n\"\r\n+                        except Exception as e1:\r\n+                            response += f\"Debug: Approach 1 failed: {e1}\\n\"\r\n+                            # Approach 2: get_transcript without language\r\n+                            try:\r\n+                                transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\r\n+                                transcript_text = \" \".join([entry['text'] for entry in transcript_list])\r\n+                                transcripts.append(transcript_text)\r\n+                                response += f\"Transcript fetched for {url} (Approach 2: get_transcript without language)\\n\"\r\n+                            except Exception as e2:\r\n+                                response += f\"Debug: Approach 2 failed: {e2}\\n\"\r\n+                                # Approach 3: list_transcripts and fetch\r\n+                                try:\r\n+                                    transcript_list_obj = YouTubeTranscriptApi.list_transcripts(video_id)\r\n+                                    transcript = transcript_list_obj.find_generated_transcript(['en']) or transcript_list_obj.find_transcript(['en'])\r\n+                                    transcript_text = \" \".join([entry['text'] for entry in transcript.fetch()])\r\n+                                    transcripts.append(transcript_text)\r\n+                                    response += f\"Transcript fetched for {url} (Approach 3: list_transcripts and fetch)\\n\"\r\n+                                except Exception as e3:\r\n+                                    response += f\"Debug: Approach 3 failed: {e3}\\n\"\r\n+                                    # Approach 4: Direct HTTP to timedtext API (JSON3 format)\r\n+                                    try:\r\n+                                        api_url = f\"https://www.youtube.com/api/timedtext?v={video_id}&lang=en&fmt=json3\"\r\n+                                        api_response = requests.get(api_url)\r\n+                                        print(f\"Debug: Timedtext API status: {api_response.status_code}, content: {api_response.text[:200]}\")\r\n+                                        if api_response.status_code == 200:\r\n+                                            if api_response.text.startswith('{'):\r\n+                                                json_data = api_response.json()\r\n+                                                if 'events' in json_data:\r\n+                                                    transcript_text = \" \".join([event['segs'][0]['utf8'] for event in json_data['events'] if 'segs' in event and event['segs']])\r\n+                                                    transcripts.append(transcript_text)\r\n+                                                    response += f\"Transcript fetched for {url} (Approach 4: direct timedtext API - JSON)\\n\"\r\n+                                                else:\r\n+                                                    response += f\"Debug: Approach 4 failed: No 'events' in JSON.\\n\"\r\n+                                            elif api_response.text.startswith('<'):\r\n+                                                # Handle XML fallback\r\n+                                                from xml.etree import ElementTree as ET\r\n+                                                root = ET.fromstring(api_response.text)\r\n+                                                transcript_text = \" \".join([elem.text for elem in root.findall('.//text') if elem.text])\r\n+                                                transcripts.append(transcript_text)\r\n+                                                response += f\"Transcript fetched for {url} (Approach 4: direct timedtext API - XML fallback)\\n\"\r\n+                                            else:\r\n+                                                response += f\"Debug: Approach 4 failed: Unknown format.\\n\"\r\n+                                        else:\r\n+                                            response += f\"Debug: Approach 4 failed: Status {api_response.status_code}\\n\"\r\n+                                    except Exception as e4:\r\n+                                        response += f\"Debug: Approach 4 failed: {e4}\\n\"\r\n+                                        response += f\"Error fetching transcript for {url}: All approaches failed.\\n\"\r\n+\r\n                         history[-1][\"content\"] = response\r\n                         yield history, \"\"\r\n                     except Exception as e:\r\n                         response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n"
                },
                {
                    "date": 1756875017820,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,8 +17,10 @@\n from utils import lock\r\n import re\r\n import requests\r\n import json\r\n+from xml.etree import ElementTree as ET\r\n+from crawl_utils import fetch_youtube_transcripts\r\n \r\n def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n     print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n     history.append({\"role\": \"user\", \"content\": message})\r\n@@ -103,92 +105,15 @@\n             response += f\"All unique URLs to process (limited to {MAX_URLS}): {', '.join(all_urls)}\\n\\n\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n \r\n-            transcripts = []\r\n-            for url in all_urls:\r\n-                match = re.search(r\"v=([^&]+)\", url)\r\n-                if match:\r\n-                    video_id = match.group(1)\r\n-                    try:\r\n-                        # Debug: Available methods on YouTubeTranscriptApi\r\n-                        print(f\"Debug: Available methods on YouTubeTranscriptApi: {dir(YouTubeTranscriptApi)}\")\r\n+            fetch_gen = fetch_youtube_transcripts(all_urls, response, history, conn=conn, message=message, is_chat=True)\r\n+            for upd in fetch_gen:\r\n+                yield upd\r\n \r\n-                        # Approach 1: get_transcript with language\r\n-                        try:\r\n-                            transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\r\n-                            transcript_text = \" \".join([entry['text'] for entry in transcript_list])\r\n-                            transcripts.append(transcript_text)\r\n-                            response += f\"Transcript fetched for {url} (Approach 1: get_transcript with en)\\n\"\r\n-                        except Exception as e1:\r\n-                            response += f\"Debug: Approach 1 failed: {e1}\\n\"\r\n-                            # Approach 2: get_transcript without language\r\n-                            try:\r\n-                                transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\r\n-                                transcript_text = \" \".join([entry['text'] for entry in transcript_list])\r\n-                                transcripts.append(transcript_text)\r\n-                                response += f\"Transcript fetched for {url} (Approach 2: get_transcript without language)\\n\"\r\n-                            except Exception as e2:\r\n-                                response += f\"Debug: Approach 2 failed: {e2}\\n\"\r\n-                                # Approach 3: list_transcripts and fetch\r\n-                                try:\r\n-                                    transcript_list_obj = YouTubeTranscriptApi.list_transcripts(video_id)\r\n-                                    transcript = transcript_list_obj.find_generated_transcript(['en']) or transcript_list_obj.find_transcript(['en'])\r\n-                                    transcript_text = \" \".join([entry['text'] for entry in transcript.fetch()])\r\n-                                    transcripts.append(transcript_text)\r\n-                                    response += f\"Transcript fetched for {url} (Approach 3: list_transcripts and fetch)\\n\"\r\n-                                except Exception as e3:\r\n-                                    response += f\"Debug: Approach 3 failed: {e3}\\n\"\r\n-                                    # Approach 4: Direct HTTP to timedtext API (JSON3 format)\r\n-                                    try:\r\n-                                        api_url = f\"https://www.youtube.com/api/timedtext?v={video_id}&lang=en&fmt=json3\"\r\n-                                        api_response = requests.get(api_url)\r\n-                                        print(f\"Debug: Timedtext API status: {api_response.status_code}, content: {api_response.text[:200]}\")\r\n-                                        if api_response.status_code == 200:\r\n-                                            if api_response.text.startswith('{'):\r\n-                                                json_data = api_response.json()\r\n-                                                if 'events' in json_data:\r\n-                                                    transcript_text = \" \".join([event['segs'][0]['utf8'] for event in json_data['events'] if 'segs' in event and event['segs']])\r\n-                                                    transcripts.append(transcript_text)\r\n-                                                    response += f\"Transcript fetched for {url} (Approach 4: direct timedtext API - JSON)\\n\"\r\n-                                                else:\r\n-                                                    response += f\"Debug: Approach 4 failed: No 'events' in JSON.\\n\"\r\n-                                            elif api_response.text.startswith('<'):\r\n-                                                # Handle XML fallback\r\n-                                                from xml.etree import ElementTree as ET\r\n-                                                root = ET.fromstring(api_response.text)\r\n-                                                transcript_text = \" \".join([elem.text for elem in root.findall('.//text') if elem.text])\r\n-                                                transcripts.append(transcript_text)\r\n-                                                response += f\"Transcript fetched for {url} (Approach 4: direct timedtext API - XML fallback)\\n\"\r\n-                                            else:\r\n-                                                response += f\"Debug: Approach 4 failed: Unknown format.\\n\"\r\n-                                        else:\r\n-                                            response += f\"Debug: Approach 4 failed: Status {api_response.status_code}\\n\"\r\n-                                    except Exception as e4:\r\n-                                        response += f\"Debug: Approach 4 failed: {e4}\\n\"\r\n-                                        response += f\"Error fetching transcript for {url}: All approaches failed.\\n\"\r\n+            response = fetch_gen[-1][1] if fetch_gen else response  # Update response if needed\r\n+            history = fetch_gen[-1][2] if fetch_gen else history\r\n \r\n-                        history[-1][\"content\"] = response\r\n-                        yield history, \"\"\r\n-                    except Exception as e:\r\n-                        response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n-                        history[-1][\"content\"] = response\r\n-                        yield history, \"\"\r\n-\r\n-            combined_transcript = \" \".join(transcripts)\r\n-            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n-            chunks = text_splitter.split_text(combined_transcript)\r\n-            new_docs = []\r\n-            for chunk in chunks:\r\n-                if add_chunk_if_new(conn, chunk, \"YouTube transcripts\"):\r\n-                    metadata = {\"source\": \"YouTube\", \"tag\": \"youtube_\" + message.replace(\" \", \"_\")}\r\n-                    new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n-\r\n-            if new_docs:\r\n-                with lock:\r\n-                    vectorstore.add_documents(new_docs)\r\n-                    vectorstore.save_local(FAISS_PATH)\r\n-\r\n             search_kwargs = {\"k\": 5, \"filter\": {\"tag\": \"youtube_\" + message.replace(\" \", \"_\")}}\r\n             dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n             bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n             retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n"
                },
                {
                    "date": 1756875228422,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,9 +18,9 @@\n import re\r\n import requests\r\n import json\r\n from xml.etree import ElementTree as ET\r\n-from crawl_utils import fetch_youtube_transcripts\r\n+import time\r\n \r\n def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n     print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n     history.append({\"role\": \"user\", \"content\": message})\r\n@@ -100,20 +100,62 @@\n             response += f\"Found {len(youtube_urls)} YouTube URLs: {', '.join(youtube_urls)}\\n\\n\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n \r\n-            all_urls = list(set(youtube_urls))[:MAX_URLS]\r\n-            response += f\"All unique URLs to process (limited to {MAX_URLS}): {', '.join(all_urls)}\\n\\n\"\r\n+            all_urls = list(set(youtube_urls))[:5]  # Limit to 5 to avoid rate limiting\r\n+            response += f\"All unique URLs to process (limited to 5): {', '.join(all_urls)}\\n\\n\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n \r\n-            fetch_gen = fetch_youtube_transcripts(all_urls, response, history, conn=conn, message=message, is_chat=True)\r\n-            for upd in fetch_gen:\r\n-                yield upd\r\n+            transcripts = []\r\n+            for url in all_urls:\r\n+                match = re.search(r\"v=([^&]+)\", url)\r\n+                if match:\r\n+                    video_id = match.group(1)\r\n+                    try:\r\n+                        api_url = f\"https://www.youtube.com/api/timedtext?v={video_id}&lang=en&fmt=json3\"\r\n+                        api_response = requests.get(api_url)\r\n+                        print(f\"Debug: Timedtext API status: {api_response.status_code}, content: {api_response.text[:200]}\")\r\n+                        if api_response.status_code == 200:\r\n+                            if api_response.text.startswith('{'):\r\n+                                json_data = api_response.json()\r\n+                                if 'events' in json_data:\r\n+                                    transcript_text = \" \".join([event['segs'][0]['utf8'] for event in json_data['events'] if 'segs' in event and event['segs'] and 'utf8' in event['segs'][0]])\r\n+                                    transcripts.append(transcript_text)\r\n+                                    response += f\"Transcript fetched for {url} (direct timedtext API - JSON)\\n\"\r\n+                                else:\r\n+                                    response += f\"Debug: No 'events' in JSON for {url}.\\n\"\r\n+                            elif api_response.text.startswith('<'):\r\n+                                # Handle XML fallback\r\n+                                root = ET.fromstring(api_response.text)\r\n+                                transcript_text = \" \".join([elem.text for elem in root.findall('.//text') if elem.text])\r\n+                                transcripts.append(transcript_text)\r\n+                                response += f\"Transcript fetched for {url} (direct timedtext API - XML fallback)\\n\"\r\n+                            else:\r\n+                                response += f\"Debug: Unknown format for {url}.\\n\"\r\n+                        else:\r\n+                            response += f\"Debug: Status {api_response.status_code} for {url}.\\n\"\r\n+                            response += f\"Error fetching transcript for {url}: Failed to get transcript.\\n\"\r\n+                    except Exception as e:\r\n+                        response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n+                    history[-1][\"content\"] = response\r\n+                    yield history, \"\"\r\n+                    time.sleep(2)  # Delay to avoid rate limiting\r\n \r\n-            response = fetch_gen[-1][1] if fetch_gen else response  # Update response if needed\r\n-            history = fetch_gen[-1][2] if fetch_gen else history\r\n+            combined_transcript = \" \".join(transcripts)\r\n+            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n+            chunks = text_splitter.split_text(combined_transcript)\r\n+            new_docs = []\r\n+            for chunk in chunks:\r\n+                if add_chunk_if_new(conn, chunk, \"YouTube transcripts\"):\r\n+                    metadata = {\"source\": \"YouTube\", \"tag\": \"youtube_\" + message.replace(\" \", \"_\")}\r\n+                    new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n \r\n+            if new_docs:\r\n+                with lock:\r\n+                    vectorstore.add_documents(new_docs)\r\n+                    vectorstore.save_local(FAISS_PATH)\r\n+\r\n             search_kwargs = {\"k\": 5, \"filter\": {\"tag\": \"youtube_\" + message.replace(\" \", \"_\")}}\r\n             dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n             bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n             retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n"
                },
                {
                    "date": 1756877004445,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,6 +1,6 @@\n import os\r\n-from config import MODEL_NAME\r\n+from config import MODEL_NAME, FAISS_PATH\r\n from langchain_ollama import OllamaLLM\r\n from langchain_core.messages import HumanMessage, AIMessage\r\n from langchain.chains import create_retrieval_chain, create_history_aware_retriever\r\n from langchain.chains.combine_documents import create_stuff_documents_chain\r\n"
                },
                {
                    "date": 1756879726368,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,6 +1,6 @@\n import os\r\n-from config import MODEL_NAME, FAISS_PATH\r\n+from config import MODEL_NAME\r\n from langchain_ollama import OllamaLLM\r\n from langchain_core.messages import HumanMessage, AIMessage\r\n from langchain.chains import create_retrieval_chain, create_history_aware_retriever\r\n from langchain.chains.combine_documents import create_stuff_documents_chain\r\n@@ -19,9 +19,13 @@\n import requests\r\n import json\r\n from xml.etree import ElementTree as ET\r\n import time\r\n+import spacy\r\n+from crawl_utils import fetch_youtube_transcripts, run_crawl\r\n \r\n+nlp = spacy.load(\"en_core_web_sm\")\r\n+\r\n def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n     print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n     history.append({\"role\": \"user\", \"content\": message})\r\n     yield history, \"\"\r\n@@ -29,8 +33,15 @@\n     response = \"\"\r\n     history.append({\"role\": \"assistant\", \"content\": response})\r\n     yield history, \"\"\r\n \r\n+    # NLP processing for intent and NER\r\n+    doc = nlp(message)\r\n+    entities = [ent.text for ent in doc.ents]\r\n+    print(f\"Debug: Extracted entities: {entities}\")\r\n+    search_query = \" \".join(entities) if entities else message  # Use entities for search if available\r\n+    print(f\"Debug: Refined search query: {search_query}\")\r\n+\r\n     chat_history = []\r\n     for h in history[:-1]:\r\n         if h[\"role\"] == \"user\":\r\n             chat_history.append(HumanMessage(content=h[\"content\"]))\r\n@@ -45,23 +56,23 @@\n         selected_source = None\r\n     if selected_source:\r\n         if selected_source == \"Web Search\":\r\n             response += \"**Processing Status:**\\n\"\r\n-            response += f\"Searching web for query: {message}\\n\"\r\n+            response += f\"Searching web for query: {search_query}\\n\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n \r\n-            google_urls = search_web(message)\r\n+            google_urls = search_web(search_query)\r\n \r\n             response += f\"Found {len(google_urls)} Google URLs: {', '.join(google_urls)}\\n\\n\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n \r\n-            response += f\"Searching Reddit for query: {message}\\n\"\r\n+            response += f\"Searching Reddit for query: {search_query}\\n\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n \r\n-            reddit_urls = search_web(message, site=\"reddit.com\")\r\n+            reddit_urls = search_web(search_query, site=\"reddit.com\")\r\n \r\n             response += f\"Found {len(reddit_urls)} Reddit URLs: {', '.join(reddit_urls)}\\n\\n\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n@@ -90,13 +101,13 @@\n             bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n             retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n         elif selected_source == \"YouTube\":\r\n             response += \"**Processing Status:**\\n\"\r\n-            response += f\"Searching YouTube for query: {message}\\n\"\r\n+            response += f\"Searching YouTube for query: {search_query}\\n\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n \r\n-            youtube_urls = search_web(message, site=\"youtube.com\")\r\n+            youtube_urls = search_web(search_query, site=\"youtube.com\")\r\n \r\n             response += f\"Found {len(youtube_urls)} YouTube URLs: {', '.join(youtube_urls)}\\n\\n\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n@@ -105,57 +116,14 @@\n             response += f\"All unique URLs to process (limited to 5): {', '.join(all_urls)}\\n\\n\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n \r\n-            transcripts = []\r\n-            for url in all_urls:\r\n-                match = re.search(r\"v=([^&]+)\", url)\r\n-                if match:\r\n-                    video_id = match.group(1)\r\n-                    try:\r\n-                        api_url = f\"https://www.youtube.com/api/timedtext?v={video_id}&lang=en&fmt=json3\"\r\n-                        api_response = requests.get(api_url)\r\n-                        print(f\"Debug: Timedtext API status: {api_response.status_code}, content: {api_response.text[:200]}\")\r\n-                        if api_response.status_code == 200:\r\n-                            if api_response.text.startswith('{'):\r\n-                                json_data = api_response.json()\r\n-                                if 'events' in json_data:\r\n-                                    transcript_text = \" \".join([event['segs'][0]['utf8'] for event in json_data['events'] if 'segs' in event and event['segs'] and 'utf8' in event['segs'][0]])\r\n-                                    transcripts.append(transcript_text)\r\n-                                    response += f\"Transcript fetched for {url} (direct timedtext API - JSON)\\n\"\r\n-                                else:\r\n-                                    response += f\"Debug: No 'events' in JSON for {url}.\\n\"\r\n-                            elif api_response.text.startswith('<'):\r\n-                                # Handle XML fallback\r\n-                                root = ET.fromstring(api_response.text)\r\n-                                transcript_text = \" \".join([elem.text for elem in root.findall('.//text') if elem.text])\r\n-                                transcripts.append(transcript_text)\r\n-                                response += f\"Transcript fetched for {url} (direct timedtext API - XML fallback)\\n\"\r\n-                            else:\r\n-                                response += f\"Debug: Unknown format for {url}.\\n\"\r\n-                        else:\r\n-                            response += f\"Debug: Status {api_response.status_code} for {url}.\\n\"\r\n-                            response += f\"Error fetching transcript for {url}: Failed to get transcript.\\n\"\r\n-                    except Exception as e:\r\n-                        response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n-                    history[-1][\"content\"] = response\r\n-                    yield history, \"\"\r\n-                    time.sleep(2)  # Delay to avoid rate limiting\r\n+            fetch_gen = fetch_youtube_transcripts(all_urls, response, history, conn=conn, message=message, is_chat=True)\r\n+            for upd in fetch_gen:\r\n+                yield upd\r\n \r\n-            combined_transcript = \" \".join(transcripts)\r\n-            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n-            chunks = text_splitter.split_text(combined_transcript)\r\n-            new_docs = []\r\n-            for chunk in chunks:\r\n-                if add_chunk_if_new(conn, chunk, \"YouTube transcripts\"):\r\n-                    metadata = {\"source\": \"YouTube\", \"tag\": \"youtube_\" + message.replace(\" \", \"_\")}\r\n-                    new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+            transcripts, response, history = fetch_gen  # Get final values from generator\r\n \r\n-            if new_docs:\r\n-                with lock:\r\n-                    vectorstore.add_documents(new_docs)\r\n-                    vectorstore.save_local(FAISS_PATH)\r\n-\r\n             search_kwargs = {\"k\": 5, \"filter\": {\"tag\": \"youtube_\" + message.replace(\" \", \"_\")}}\r\n             dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n             bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n             retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n"
                },
                {
                    "date": 1756880672077,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,217 @@\n+import os\r\n+from config import MODEL_NAME\r\n+from langchain_ollama import OllamaLLM\r\n+from langchain_core.messages import HumanMessage, AIMessage\r\n+from langchain.chains import create_retrieval_chain, create_history_aware_retriever\r\n+from langchain.chains.combine_documents import create_stuff_documents_chain\r\n+from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\r\n+from langchain.retrievers import EnsembleRetriever\r\n+from langchain_community.retrievers import BM25Retriever\r\n+from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n+from langchain_core.documents import Document\r\n+from web_utils import search_web\r\n+from config import MAX_URLS\r\n+from process_utils import process_urls\r\n+from vectorstore_utils import vectorstore\r\n+from db_utils import add_chunk_if_new\r\n+from utils import lock\r\n+import re\r\n+import requests\r\n+import json\r\n+from xml.etree import ElementTree as ET\r\n+import time\r\n+import spacy\r\n+from crawl_utils import fetch_youtube_transcripts, run_crawl\r\n+\r\n+nlp = spacy.load(\"en_core_web_sm\")\r\n+\r\n+def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n+    print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n+    history.append({\"role\": \"user\", \"content\": message})\r\n+    yield history, \"\"\r\n+\r\n+    response = \"\"\r\n+    history.append({\"role\": \"assistant\", \"content\": response})\r\n+    yield history, \"\"\r\n+\r\n+    # NLP processing for intent and NER\r\n+    doc = nlp(message)\r\n+    entities = [ent.text for ent in doc.ents]\r\n+    print(f\"Debug: Extracted entities: {entities}\")\r\n+    search_query = message  # Keep full message, but prioritize entities if needed\r\n+    if entities:\r\n+        search_query = \" \".join(entities) + \" \" + message  # Append entities to full message for better search\r\n+    print(f\"Debug: Refined search query: {search_query}\")\r\n+\r\n+    chat_history = []\r\n+    for h in history[:-1]:\r\n+        if h[\"role\"] == \"user\":\r\n+            chat_history.append(HumanMessage(content=h[\"content\"]))\r\n+        elif h[\"role\"] == \"assistant\":\r\n+            chat_history.append(AIMessage(content=h[\"content\"]))\r\n+\r\n+    llm = OllamaLLM(model=MODEL_NAME)\r\n+\r\n+    retriever = None\r\n+    sources = []\r\n+    if selected_source == \"No RAG\":\r\n+        selected_source = None\r\n+    if selected_source:\r\n+        if selected_source == \"Web Search\":\r\n+            response += \"**Processing Status:**\\n\"\r\n+            response += f\"Searching web for query: {search_query}\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+\r\n+            google_urls = search_web(search_query)\r\n+\r\n+            response += f\"Found {len(google_urls)} Google URLs: {', '.join(google_urls)}\\n\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+\r\n+            response += f\"Searching Reddit for query: {search_query}\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+\r\n+            reddit_urls = search_web(search_query, site=\"reddit.com\")\r\n+\r\n+            response += f\"Found {len(reddit_urls)} Reddit URLs: {', '.join(reddit_urls)}\\n\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+\r\n+            all_urls = list(set(google_urls + reddit_urls))[:MAX_URLS]\r\n+            response += f\"All unique URLs to process (limited to {MAX_URLS}): {', '.join(all_urls)}\\n\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+\r\n+            process_gen = process_urls(all_urls, response, history, message, conn=conn)\r\n+            for upd in process_gen:\r\n+                yield upd\r\n+\r\n+            # Consume generator to get return value\r\n+            try:\r\n+                while True:\r\n+                    next(process_gen)\r\n+            except StopIteration as e:\r\n+                sources, response, history = e.value\r\n+\r\n+            search_kwargs = {\"k\": 5}\r\n+            if 'lyrics' in message.lower():\r\n+                search_kwargs[\"filter\"] = {\"source_type\": \"lyrics\"}\r\n+            dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n+            bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n+            retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n+        elif selected_source == \"YouTube\":\r\n+            response += \"**Processing Status:**\\n\"\r\n+            response += f\"Searching YouTube for query: {search_query}\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+\r\n+            youtube_urls = search_web(search_query, site=\"youtube.com\")\r\n+\r\n+            response += f\"Found {len(youtube_urls)} YouTube URLs: {', '.join(youtube_urls)}\\n\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+\r\n+            all_urls = list(set(youtube_urls))[:5]  # Limit to 5 to avoid rate limiting\r\n+            response += f\"All unique URLs to process (limited to 5): {', '.join(all_urls)}\\n\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+\r\n+            fetch_gen = fetch_youtube_transcripts(all_urls, response, history, conn=conn, message=message, is_chat=True)\r\n+            for upd in fetch_gen:\r\n+                yield upd\r\n+\r\n+            # Consume generator to get return value\r\n+            try:\r\n+                while True:\r\n+                    next(fetch_gen)\r\n+            except StopIteration as e:\r\n+                transcripts, response, history = e.value\r\n+\r\n+            search_kwargs = {\"k\": 5, \"filter\": {\"tag\": \"youtube_\" + message.replace(\" \", \"_\")}}\r\n+            dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n+            bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n+            retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n+        else:\r\n+            response += \"**Processing Status:**\\n\"\r\n+            search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n+            if 'lyrics' in message.lower():\r\n+                search_kwargs[\"filter\"][\"source_type\"] = \"lyrics\"\r\n+            dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n+            bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n+            retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n+\r\n+    if retriever is None:\r\n+        # Direct LLM call without retrieval\r\n+        qa_prompt = ChatPromptTemplate.from_template(\r\n+            \"Answer the question:\\n\\nQuestion: {input}\"\r\n+        )\r\n+        qa_chain = qa_prompt | llm\r\n+        answer = qa_chain.invoke({\"input\": message})\r\n+        response = f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+    else:\r\n+        if vectorstore.index.ntotal == 0:\r\n+            response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+            return\r\n+\r\n+        response += \"Creating/Updating vector store...\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+        response += \"Vector store ready.\\n\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+        response += \"Generating summarization of the found content...\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+        summary_prompt = ChatPromptTemplate.from_template(\r\n+            \"Summarize the following retrieved content related to the question '{input}' in a concise manner:\\n\\n{context}\"\r\n+        )\r\n+        summary_chain = create_stuff_documents_chain(llm, summary_prompt)\r\n+        summary_chain_with_docs = create_retrieval_chain(retriever, summary_chain)\r\n+        summary_response = summary_chain_with_docs.invoke({\"input\": message})\r\n+        summary = summary_response[\"answer\"]\r\n+\r\n+        response += f\"**Summarization of Found Content:**\\n{summary}\\n\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+        response += \"Generating specific answer to the prompt...\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+        rephrase_prompt = ChatPromptTemplate.from_messages(\r\n+            [\r\n+                MessagesPlaceholder(variable_name=\"chat_history\"),\r\n+                (\"human\", \"{input}\"),\r\n+                (\"human\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\"),\r\n+            ]\r\n+        )\r\n+        history_aware_retriever = create_history_aware_retriever(llm, retriever, rephrase_prompt)\r\n+\r\n+        qa_prompt = ChatPromptTemplate.from_template(\r\n+            \"Use the context to answer the question as accurately as possible. If lyrics are present, extract and format them clearly with verses, chorus, etc., and ignore non-lyric content like discussions. If uncertain or incomplete, note limitations but provide what's available:\\n\\n{context}\\n\\nQuestion: {input}\"\r\n+        )\r\n+        qa_chain = create_stuff_documents_chain(llm, qa_prompt)\r\n+        qa_chain_with_docs = create_retrieval_chain(history_aware_retriever, qa_chain)\r\n+        qa_response = qa_chain_with_docs.invoke({\"input\": message, \"chat_history\": chat_history})\r\n+        answer = qa_response[\"answer\"]\r\n+\r\n+        response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+    if selected_source in [\"Web Search\", \"YouTube\"]:\r\n+        sources_str = \"\\n\".join([f\"- {url}\" for url in sources])\r\n+        response += f\"**Sources Crawled:**\\n{sources_str}\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+    print(\"chat_bot completed.\")\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756916345353,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,9 +20,9 @@\n import json\r\n from xml.etree import ElementTree as ET\r\n import time\r\n import spacy\r\n-from crawl_utils import fetch_youtube_transcripts, run_crawl\r\n+from crawl_utils import fetch_youtube_transcripts\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n@@ -213,216 +213,5 @@\n         response += f\"**Sources Crawled:**\\n{sources_str}\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n \r\n-    print(\"chat_bot completed.\")\n-import os\r\n-from config import MODEL_NAME\r\n-from langchain_ollama import OllamaLLM\r\n-from langchain_core.messages import HumanMessage, AIMessage\r\n-from langchain.chains import create_retrieval_chain, create_history_aware_retriever\r\n-from langchain.chains.combine_documents import create_stuff_documents_chain\r\n-from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\r\n-from langchain.retrievers import EnsembleRetriever\r\n-from langchain_community.retrievers import BM25Retriever\r\n-from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n-from langchain_core.documents import Document\r\n-from web_utils import search_web\r\n-from config import MAX_URLS\r\n-from process_utils import process_urls\r\n-from vectorstore_utils import vectorstore\r\n-from db_utils import add_chunk_if_new\r\n-from utils import lock\r\n-import re\r\n-import requests\r\n-import json\r\n-from xml.etree import ElementTree as ET\r\n-import time\r\n-import spacy\r\n-from crawl_utils import fetch_youtube_transcripts, run_crawl\r\n-\r\n-nlp = spacy.load(\"en_core_web_sm\")\r\n-\r\n-def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n-    print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n-    history.append({\"role\": \"user\", \"content\": message})\r\n-    yield history, \"\"\r\n-\r\n-    response = \"\"\r\n-    history.append({\"role\": \"assistant\", \"content\": response})\r\n-    yield history, \"\"\r\n-\r\n-    # NLP processing for intent and NER\r\n-    doc = nlp(message)\r\n-    entities = [ent.text for ent in doc.ents]\r\n-    print(f\"Debug: Extracted entities: {entities}\")\r\n-    search_query = \" \".join(entities) if entities else message  # Use entities for search if available\r\n-    print(f\"Debug: Refined search query: {search_query}\")\r\n-\r\n-    chat_history = []\r\n-    for h in history[:-1]:\r\n-        if h[\"role\"] == \"user\":\r\n-            chat_history.append(HumanMessage(content=h[\"content\"]))\r\n-        elif h[\"role\"] == \"assistant\":\r\n-            chat_history.append(AIMessage(content=h[\"content\"]))\r\n-\r\n-    llm = OllamaLLM(model=MODEL_NAME)\r\n-\r\n-    retriever = None\r\n-    sources = []\r\n-    if selected_source == \"No RAG\":\r\n-        selected_source = None\r\n-    if selected_source:\r\n-        if selected_source == \"Web Search\":\r\n-            response += \"**Processing Status:**\\n\"\r\n-            response += f\"Searching web for query: {search_query}\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            google_urls = search_web(search_query)\r\n-\r\n-            response += f\"Found {len(google_urls)} Google URLs: {', '.join(google_urls)}\\n\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            response += f\"Searching Reddit for query: {search_query}\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            reddit_urls = search_web(search_query, site=\"reddit.com\")\r\n-\r\n-            response += f\"Found {len(reddit_urls)} Reddit URLs: {', '.join(reddit_urls)}\\n\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            all_urls = list(set(google_urls + reddit_urls))[:MAX_URLS]\r\n-            response += f\"All unique URLs to process (limited to {MAX_URLS}): {', '.join(all_urls)}\\n\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            process_gen = process_urls(all_urls, response, history, message, conn=conn)\r\n-            for upd in process_gen:\r\n-                yield upd\r\n-\r\n-            try:\r\n-                sources, response, history = next(process_gen)\r\n-            except StopIteration as e:\r\n-                if e.value:\r\n-                    sources, response, history = e.value\r\n-                else:\r\n-                    sources = []\r\n-\r\n-            search_kwargs = {\"k\": 5}\r\n-            if 'lyrics' in message.lower():\r\n-                search_kwargs[\"filter\"] = {\"source_type\": \"lyrics\"}\r\n-            dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n-            bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n-            retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n-        elif selected_source == \"YouTube\":\r\n-            response += \"**Processing Status:**\\n\"\r\n-            response += f\"Searching YouTube for query: {search_query}\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            youtube_urls = search_web(search_query, site=\"youtube.com\")\r\n-\r\n-            response += f\"Found {len(youtube_urls)} YouTube URLs: {', '.join(youtube_urls)}\\n\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            all_urls = list(set(youtube_urls))[:5]  # Limit to 5 to avoid rate limiting\r\n-            response += f\"All unique URLs to process (limited to 5): {', '.join(all_urls)}\\n\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            fetch_gen = fetch_youtube_transcripts(all_urls, response, history, conn=conn, message=message, is_chat=True)\r\n-            for upd in fetch_gen:\r\n-                yield upd\r\n-\r\n-            transcripts, response, history = fetch_gen  # Get final values from generator\r\n-\r\n-            search_kwargs = {\"k\": 5, \"filter\": {\"tag\": \"youtube_\" + message.replace(\" \", \"_\")}}\r\n-            dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n-            bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n-            retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n-        else:\r\n-            response += \"**Processing Status:**\\n\"\r\n-            search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n-            if 'lyrics' in message.lower():\r\n-                search_kwargs[\"filter\"][\"source_type\"] = \"lyrics\"\r\n-            dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n-            bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n-            retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n-\r\n-    if retriever is None:\r\n-        # Direct LLM call without retrieval\r\n-        qa_prompt = ChatPromptTemplate.from_template(\r\n-            \"Answer the question:\\n\\nQuestion: {input}\"\r\n-        )\r\n-        qa_chain = qa_prompt | llm\r\n-        answer = qa_chain.invoke({\"input\": message})\r\n-        response = f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-    else:\r\n-        if vectorstore.index.ntotal == 0:\r\n-            response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-            return\r\n-\r\n-        response += \"Creating/Updating vector store...\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n-        response += \"Vector store ready.\\n\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n-        response += \"Generating summarization of the found content...\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n-        summary_prompt = ChatPromptTemplate.from_template(\r\n-            \"Summarize the following retrieved content related to the question '{input}' in a concise manner:\\n\\n{context}\"\r\n-        )\r\n-        summary_chain = create_stuff_documents_chain(llm, summary_prompt)\r\n-        summary_chain_with_docs = create_retrieval_chain(retriever, summary_chain)\r\n-        summary_response = summary_chain_with_docs.invoke({\"input\": message})\r\n-        summary = summary_response[\"answer\"]\r\n-\r\n-        response += f\"**Summarization of Found Content:**\\n{summary}\\n\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n-        response += \"Generating specific answer to the prompt...\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n-        rephrase_prompt = ChatPromptTemplate.from_messages(\r\n-            [\r\n-                MessagesPlaceholder(variable_name=\"chat_history\"),\r\n-                (\"human\", \"{input}\"),\r\n-                (\"human\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\"),\r\n-            ]\r\n-        )\r\n-        history_aware_retriever = create_history_aware_retriever(llm, retriever, rephrase_prompt)\r\n-\r\n-        qa_prompt = ChatPromptTemplate.from_template(\r\n-            \"Use the context to answer the question as accurately as possible. If lyrics are present, extract and format them clearly with verses, chorus, etc., and ignore non-lyric content like discussions. If uncertain or incomplete, note limitations but provide what's available:\\n\\n{context}\\n\\nQuestion: {input}\"\r\n-        )\r\n-        qa_chain = create_stuff_documents_chain(llm, qa_prompt)\r\n-        qa_chain_with_docs = create_retrieval_chain(history_aware_retriever, qa_chain)\r\n-        qa_response = qa_chain_with_docs.invoke({\"input\": message, \"chat_history\": chat_history})\r\n-        answer = qa_response[\"answer\"]\r\n-\r\n-        response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n-    if selected_source in [\"Web Search\", \"YouTube\"]:\r\n-        sources_str = \"\\n\".join([f\"- {url}\" for url in sources])\r\n-        response += f\"**Sources Crawled:**\\n{sources_str}\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n     print(\"chat_bot completed.\")\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756918960915,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,4 +1,5 @@\n+# chat_utils.py\r\n import os\r\n from config import MODEL_NAME\r\n from langchain_ollama import OllamaLLM\r\n from langchain_core.messages import HumanMessage, AIMessage\r\n@@ -21,8 +22,10 @@\n from xml.etree import ElementTree as ET\r\n import time\r\n import spacy\r\n from crawl_utils import fetch_youtube_transcripts\r\n+from urllib.parse import quote\r\n+from config import RAW_DIR\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n@@ -84,15 +87,12 @@\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n \r\n             process_gen = process_urls(all_urls, response, history, message, conn=conn)\r\n-            for upd in process_gen:\r\n-                yield upd\r\n-\r\n-            # Consume generator to get return value\r\n             try:\r\n                 while True:\r\n-                    next(process_gen)\r\n+                    upd = next(process_gen)\r\n+                    yield upd\r\n             except StopIteration as e:\r\n                 sources, response, history = e.value\r\n \r\n             search_kwargs = {\"k\": 5}\r\n@@ -112,25 +112,60 @@\n             response += f\"Found {len(youtube_urls)} YouTube URLs: {', '.join(youtube_urls)}\\n\\n\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n \r\n-            all_urls = list(set(youtube_urls))[:5]  # Limit to 5 to avoid rate limiting\r\n-            response += f\"All unique URLs to process (limited to 5): {', '.join(all_urls)}\\n\\n\"\r\n+            all_urls = list(set(youtube_urls))[:3]  # Limit to 3 to avoid rate limiting\r\n+            response += f\"All unique URLs to process (limited to 3): {', '.join(all_urls)}\\n\\n\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n \r\n             fetch_gen = fetch_youtube_transcripts(all_urls, response, history, conn=conn, message=message, is_chat=True)\r\n-            for upd in fetch_gen:\r\n-                yield upd\r\n-\r\n-            # Consume generator to get return value\r\n             try:\r\n                 while True:\r\n-                    next(fetch_gen)\r\n+                    upd = next(fetch_gen)\r\n+                    yield upd\r\n             except StopIteration as e:\r\n                 transcripts, response, history = e.value\r\n \r\n-            search_kwargs = {\"k\": 5, \"filter\": {\"tag\": \"youtube_\" + message.replace(\" \", \"_\")}}\r\n+            # Process transcripts (mirror process_urls logic)\r\n+            sources = []\r\n+            documents = []  # Optional, for counting\r\n+            tag = \"youtube_\" + message.replace(\" \", \"_\")\r\n+            for url, transcript in transcripts:\r\n+                if transcript:\r\n+                    sources.append(url)\r\n+                    display_text = transcript[:1000] + \"...\" if len(transcript) > 1000 else transcript\r\n+                    response += f\"Transcript from {url} (preview):\\n{display_text}\\n\\n\"\r\n+                    history[-1][\"content\"] = response\r\n+                    yield history, \"\"\r\n+\r\n+                    safe_filename = quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n+                    filepath = os.path.join(RAW_DIR, safe_filename)\r\n+                    with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                        f.write(transcript)\r\n+                    response += f\"[Download full transcript for {url}](/file={filepath})\\n\\n\"\r\n+                    history[-1][\"content\"] = response\r\n+                    yield history, \"\"\r\n+\r\n+                    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n+                    chunks = text_splitter.split_text(transcript)\r\n+                    new_docs = []\r\n+                    for chunk in chunks:\r\n+                        if add_chunk_if_new(conn, chunk, url, tag=tag):\r\n+                            metadata = {\"source\": url, \"tag\": tag}\r\n+                            if 'lyrics' in message.lower():\r\n+                                metadata[\"source_type\"] = \"lyrics\"\r\n+                            new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+                    if new_docs:\r\n+                        with lock:\r\n+                            vectorstore.add_documents(new_docs)\r\n+                            vectorstore.save_local(FAISS_PATH)\r\n+                        documents.extend(new_docs)\r\n+                    response += f\"Number of new document chunks added from {url}: {len(new_docs)}\\n\\n\"\r\n+                    history[-1][\"content\"] = response\r\n+                    yield history, \"\"\r\n+\r\n+            search_kwargs = {\"k\": 5, \"filter\": {\"tag\": tag}}\r\n             dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n             bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n             retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n         else:\r\n"
                },
                {
                    "date": 1756932172952,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -21,16 +21,16 @@\n import json\r\n from xml.etree import ElementTree as ET\r\n import time\r\n import spacy\r\n-from crawl_utils import fetch_youtube_transcripts\r\n+from ytselenium_utils import fetch_youtube_transcript\r\n from urllib.parse import quote\r\n from config import RAW_DIR\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n-def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n-    print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n+def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None, use_ollama=False, selected_subreddit=None, selected_timelimit=None):\r\n+    print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}, use_ollama: {use_ollama}\")\r\n     history.append({\"role\": \"user\", \"content\": message})\r\n     yield history, \"\"\r\n \r\n     response = \"\"\r\n@@ -59,36 +59,41 @@\n     sources = []\r\n     if selected_source == \"No RAG\":\r\n         selected_source = None\r\n     if selected_source:\r\n-        if selected_source == \"Web Search\":\r\n+        if selected_source in [\"Web Search\", \"Reddit\", \"Subreddit\"]:\r\n+            site = None\r\n+            timelimit_code = None\r\n+            if selected_source == \"Reddit\":\r\n+                site = \"reddit.com\"\r\n+            elif selected_source == \"Subreddit\":\r\n+                if not selected_subreddit:\r\n+                    response += \"Subreddit name required for Subreddit source.\\n\"\r\n+                    history[-1][\"content\"] = response\r\n+                    yield history, \"\"\r\n+                    return\r\n+                site = f\"reddit.com/r/{selected_subreddit}\"\r\n+                timelimit_map = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}\r\n+                timelimit_code = timelimit_map.get(selected_timelimit, None)\r\n+\r\n             response += \"**Processing Status:**\\n\"\r\n-            response += f\"Searching web for query: {search_query}\\n\"\r\n+            response += f\"Searching {selected_source} for query: {search_query}\\n\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n \r\n-            google_urls = search_web(search_query)\r\n+            urls = search_web(search_query, site=site, timelimit=timelimit_code)\r\n \r\n-            response += f\"Found {len(google_urls)} Google URLs: {', '.join(google_urls)}\\n\\n\"\r\n+            response += f\"Found {len(urls)} URLs: {', '.join(urls)}\\n\\n\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n \r\n-            response += f\"Searching Reddit for query: {search_query}\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            reddit_urls = search_web(search_query, site=\"reddit.com\")\r\n-\r\n-            response += f\"Found {len(reddit_urls)} Reddit URLs: {', '.join(reddit_urls)}\\n\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            all_urls = list(set(google_urls + reddit_urls))[:MAX_URLS]\r\n+            all_urls = list(set(urls))[:MAX_URLS]\r\n             response += f\"All unique URLs to process (limited to {MAX_URLS}): {', '.join(all_urls)}\\n\\n\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n \r\n-            process_gen = process_urls(all_urls, response, history, message, conn=conn)\r\n+            tag = f\"{selected_source.lower()}_\" + message.replace(\" \", \"_\")\r\n+            process_gen = process_urls(all_urls, response, history, message, conn=conn, source_tag=tag, use_ollama=use_ollama)\r\n             try:\r\n                 while True:\r\n                     upd = next(process_gen)\r\n                     yield upd\r\n@@ -112,30 +117,21 @@\n             response += f\"Found {len(youtube_urls)} YouTube URLs: {', '.join(youtube_urls)}\\n\\n\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n \r\n-            all_urls = list(set(youtube_urls))[:3]  # Limit to 3 to avoid rate limiting\r\n+            all_urls = list(set(youtube_urls))[:3]  # Limit to 3 to avoid issues\r\n             response += f\"All unique URLs to process (limited to 3): {', '.join(all_urls)}\\n\\n\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n \r\n-            fetch_gen = fetch_youtube_transcripts(all_urls, response, history, conn=conn, message=message, is_chat=True)\r\n-            try:\r\n-                while True:\r\n-                    upd = next(fetch_gen)\r\n-                    yield upd\r\n-            except StopIteration as e:\r\n-                transcripts, response, history = e.value\r\n-\r\n-            # Process transcripts (mirror process_urls logic)\r\n-            sources = []\r\n-            documents = []  # Optional, for counting\r\n+            # Process YouTube transcripts\r\n+            transcripts = []\r\n             tag = \"youtube_\" + message.replace(\" \", \"_\")\r\n-            for url, transcript in transcripts:\r\n+            for url in all_urls:\r\n+                transcript = fetch_youtube_transcript(url, use_ollama=use_ollama)\r\n                 if transcript:\r\n-                    sources.append(url)\r\n-                    display_text = transcript[:1000] + \"...\" if len(transcript) > 1000 else transcript\r\n-                    response += f\"Transcript from {url} (preview):\\n{display_text}\\n\\n\"\r\n+                    transcripts.append((url, transcript))\r\n+                    response += f\"Transcript fetched and processed for {url}\\n\"\r\n                     history[-1][\"content\"] = response\r\n                     yield history, \"\"\r\n \r\n                     safe_filename = quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n@@ -158,12 +154,15 @@\n                     if new_docs:\r\n                         with lock:\r\n                             vectorstore.add_documents(new_docs)\r\n                             vectorstore.save_local(FAISS_PATH)\r\n-                        documents.extend(new_docs)\r\n                     response += f\"Number of new document chunks added from {url}: {len(new_docs)}\\n\\n\"\r\n                     history[-1][\"content\"] = response\r\n                     yield history, \"\"\r\n+                else:\r\n+                    response += f\"Failed to fetch transcript for {url}\\n\"\r\n+                    history[-1][\"content\"] = response\r\n+                    yield history, \"\"\r\n \r\n             search_kwargs = {\"k\": 5, \"filter\": {\"tag\": tag}}\r\n             dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n             bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n@@ -177,9 +176,8 @@\n             bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n             retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n \r\n     if retriever is None:\r\n-        # Direct LLM call without retrieval\r\n         qa_prompt = ChatPromptTemplate.from_template(\r\n             \"Answer the question:\\n\\nQuestion: {input}\"\r\n         )\r\n         qa_chain = qa_prompt | llm\r\n@@ -242,9 +240,9 @@\n         response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n \r\n-    if selected_source in [\"Web Search\", \"YouTube\"]:\r\n+    if selected_source in [\"Web Search\", \"YouTube\", \"Reddit\", \"Subreddit\"]:\r\n         sources_str = \"\\n\".join([f\"- {url}\" for url in sources])\r\n         response += f\"**Sources Crawled:**\\n{sources_str}\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n"
                },
                {
                    "date": 1756932487630,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n # chat_utils.py\r\n import os\r\n-from config import MODEL_NAME\r\n+from config import MODEL_NAME, MAX_URLS, RAW_DIR, FAISS_PATH\r\n from langchain_ollama import OllamaLLM\r\n from langchain_core.messages import HumanMessage, AIMessage\r\n from langchain.chains import create_retrieval_chain, create_history_aware_retriever\r\n from langchain.chains.combine_documents import create_stuff_documents_chain\r\n@@ -10,9 +10,8 @@\n from langchain_community.retrievers import BM25Retriever\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n from web_utils import search_web\r\n-from config import MAX_URLS\r\n from process_utils import process_urls\r\n from vectorstore_utils import vectorstore\r\n from db_utils import add_chunk_if_new\r\n from utils import lock\r\n@@ -23,9 +22,8 @@\n import time\r\n import spacy\r\n from ytselenium_utils import fetch_youtube_transcript\r\n from urllib.parse import quote\r\n-from config import RAW_DIR\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None, use_ollama=False, selected_subreddit=None, selected_timelimit=None):\r\n"
                },
                {
                    "date": 1756933945773,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -22,8 +22,9 @@\n import time\r\n import spacy\r\n from ytselenium_utils import fetch_youtube_transcript\r\n from urllib.parse import quote\r\n+import html\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None, use_ollama=False, selected_subreddit=None, selected_timelimit=None):\r\n@@ -124,9 +125,23 @@\n             # Process YouTube transcripts\r\n             transcripts = []\r\n             tag = \"youtube_\" + message.replace(\" \", \"_\")\r\n             for url in all_urls:\r\n-                transcript = fetch_youtube_transcript(url, use_ollama=use_ollama)\r\n+                response += f\"Processing {url}...\\n\"\r\n+                history[-1][\"content\"] = response\r\n+                yield history, \"\"\r\n+\r\n+                gen = fetch_youtube_transcript(url, use_ollama=use_ollama)\r\n+                transcript = None\r\n+                for item in gen:\r\n+                    item_type, value = item\r\n+                    if item_type == \"status\":\r\n+                        response += value + \"\\n\"\r\n+                        history[-1][\"content\"] = response\r\n+                        yield history, \"\"\r\n+                    elif item_type == \"transcript\":\r\n+                        transcript = value\r\n+\r\n                 if transcript:\r\n                     transcripts.append((url, transcript))\r\n                     response += f\"Transcript fetched and processed for {url}\\n\"\r\n                     history[-1][\"content\"] = response\r\n@@ -135,12 +150,37 @@\n                     safe_filename = quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n                     filepath = os.path.join(RAW_DIR, safe_filename)\r\n                     with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n                         f.write(transcript)\r\n-                    response += f\"[Download full transcript for {url}](/file={filepath})\\n\\n\"\r\n+                    response += f\"[Download full transcript for {url}](/file={safe_filename})\\n\\n\"\r\n                     history[-1][\"content\"] = response\r\n                     yield history, \"\"\r\n \r\n+                    # Save as HTML for viewing\r\n+                    html_safe_filename = safe_filename.replace(\".txt\", \".html\")\r\n+                    html_filepath = os.path.join(RAW_DIR, html_safe_filename)\r\n+                    escaped_text = html.escape(transcript)\r\n+                    html_content = f\"\"\"\r\n+<html>\r\n+<head>\r\n+    <style>\r\n+        body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }}\r\n+        pre {{ white-space: pre-wrap; word-wrap: break-word; }}\r\n+    </style>\r\n+</head>\r\n+<body>\r\n+    <h1>Processed Transcript for {url}</h1>\r\n+    <pre>{escaped_text}</pre>\r\n+</body>\r\n+</html>\r\n+\"\"\"\r\n+                    with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                        f.write(html_content)\r\n+\r\n+                    response += f'<a href=\"/file={html_safe_filename}\" target=\"_blank\">View Processed Transcript for {url} in new tab</a>\\n\\n'\r\n+                    history[-1][\"content\"] = response\r\n+                    yield history, \"\"\r\n+\r\n                     text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n                     chunks = text_splitter.split_text(transcript)\r\n                     new_docs = []\r\n                     for chunk in chunks:\r\n"
                },
                {
                    "date": 1756935149353,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,35 +1,23 @@\n # chat_utils.py\r\n import os\r\n-from config import MODEL_NAME, MAX_URLS, RAW_DIR, FAISS_PATH\r\n+from config import MODEL_NAME, FAISS_PATH\r\n from langchain_ollama import OllamaLLM\r\n from langchain_core.messages import HumanMessage, AIMessage\r\n from langchain.chains import create_retrieval_chain, create_history_aware_retriever\r\n from langchain.chains.combine_documents import create_stuff_documents_chain\r\n from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\r\n from langchain.retrievers import EnsembleRetriever\r\n from langchain_community.retrievers import BM25Retriever\r\n-from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n-from web_utils import search_web\r\n-from process_utils import process_urls\r\n from vectorstore_utils import vectorstore\r\n-from db_utils import add_chunk_if_new\r\n-from utils import lock\r\n-import re\r\n-import requests\r\n-import json\r\n-from xml.etree import ElementTree as ET\r\n import time\r\n import spacy\r\n-from ytselenium_utils import fetch_youtube_transcript\r\n-from urllib.parse import quote\r\n-import html\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n-def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None, use_ollama=False, selected_subreddit=None, selected_timelimit=None):\r\n-    print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}, use_ollama: {use_ollama}\")\r\n+def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n+    print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n     history.append({\"role\": \"user\", \"content\": message})\r\n     yield history, \"\"\r\n \r\n     response = \"\"\r\n@@ -39,12 +27,8 @@\n     # NLP processing for intent and NER\r\n     doc = nlp(message)\r\n     entities = [ent.text for ent in doc.ents]\r\n     print(f\"Debug: Extracted entities: {entities}\")\r\n-    search_query = message  # Keep full message, but prioritize entities if needed\r\n-    if entities:\r\n-        search_query = \" \".join(entities) + \" \" + message  # Append entities to full message for better search\r\n-    print(f\"Debug: Refined search query: {search_query}\")\r\n \r\n     chat_history = []\r\n     for h in history[:-1]:\r\n         if h[\"role\"] == \"user\":\r\n@@ -54,167 +38,17 @@\n \r\n     llm = OllamaLLM(model=MODEL_NAME)\r\n \r\n     retriever = None\r\n-    sources = []\r\n-    if selected_source == \"No RAG\":\r\n-        selected_source = None\r\n-    if selected_source:\r\n-        if selected_source in [\"Web Search\", \"Reddit\", \"Subreddit\"]:\r\n-            site = None\r\n-            timelimit_code = None\r\n-            if selected_source == \"Reddit\":\r\n-                site = \"reddit.com\"\r\n-            elif selected_source == \"Subreddit\":\r\n-                if not selected_subreddit:\r\n-                    response += \"Subreddit name required for Subreddit source.\\n\"\r\n-                    history[-1][\"content\"] = response\r\n-                    yield history, \"\"\r\n-                    return\r\n-                site = f\"reddit.com/r/{selected_subreddit}\"\r\n-                timelimit_map = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}\r\n-                timelimit_code = timelimit_map.get(selected_timelimit, None)\r\n+    if selected_source != \"No RAG\":\r\n+        response += \"**Processing Status:**\\n\"\r\n+        search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n+        if 'lyrics' in message.lower():\r\n+            search_kwargs[\"filter\"][\"source_type\"] = \"lyrics\"\r\n+        dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n+        bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n+        retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n \r\n-            response += \"**Processing Status:**\\n\"\r\n-            response += f\"Searching {selected_source} for query: {search_query}\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            urls = search_web(search_query, site=site, timelimit=timelimit_code)\r\n-\r\n-            response += f\"Found {len(urls)} URLs: {', '.join(urls)}\\n\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            all_urls = list(set(urls))[:MAX_URLS]\r\n-            response += f\"All unique URLs to process (limited to {MAX_URLS}): {', '.join(all_urls)}\\n\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            tag = f\"{selected_source.lower()}_\" + message.replace(\" \", \"_\")\r\n-            process_gen = process_urls(all_urls, response, history, message, conn=conn, source_tag=tag, use_ollama=use_ollama)\r\n-            try:\r\n-                while True:\r\n-                    upd = next(process_gen)\r\n-                    yield upd\r\n-            except StopIteration as e:\r\n-                sources, response, history = e.value\r\n-\r\n-            search_kwargs = {\"k\": 5}\r\n-            if 'lyrics' in message.lower():\r\n-                search_kwargs[\"filter\"] = {\"source_type\": \"lyrics\"}\r\n-            dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n-            bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n-            retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n-        elif selected_source == \"YouTube\":\r\n-            response += \"**Processing Status:**\\n\"\r\n-            response += f\"Searching YouTube for query: {search_query}\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            youtube_urls = search_web(search_query, site=\"youtube.com\")\r\n-\r\n-            response += f\"Found {len(youtube_urls)} YouTube URLs: {', '.join(youtube_urls)}\\n\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            all_urls = list(set(youtube_urls))[:3]  # Limit to 3 to avoid issues\r\n-            response += f\"All unique URLs to process (limited to 3): {', '.join(all_urls)}\\n\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-\r\n-            # Process YouTube transcripts\r\n-            transcripts = []\r\n-            tag = \"youtube_\" + message.replace(\" \", \"_\")\r\n-            for url in all_urls:\r\n-                response += f\"Processing {url}...\\n\"\r\n-                history[-1][\"content\"] = response\r\n-                yield history, \"\"\r\n-\r\n-                gen = fetch_youtube_transcript(url, use_ollama=use_ollama)\r\n-                transcript = None\r\n-                for item in gen:\r\n-                    item_type, value = item\r\n-                    if item_type == \"status\":\r\n-                        response += value + \"\\n\"\r\n-                        history[-1][\"content\"] = response\r\n-                        yield history, \"\"\r\n-                    elif item_type == \"transcript\":\r\n-                        transcript = value\r\n-\r\n-                if transcript:\r\n-                    transcripts.append((url, transcript))\r\n-                    response += f\"Transcript fetched and processed for {url}\\n\"\r\n-                    history[-1][\"content\"] = response\r\n-                    yield history, \"\"\r\n-\r\n-                    safe_filename = quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n-                    filepath = os.path.join(RAW_DIR, safe_filename)\r\n-                    with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n-                        f.write(transcript)\r\n-                    response += f\"[Download full transcript for {url}](/file={safe_filename})\\n\\n\"\r\n-                    history[-1][\"content\"] = response\r\n-                    yield history, \"\"\r\n-\r\n-                    # Save as HTML for viewing\r\n-                    html_safe_filename = safe_filename.replace(\".txt\", \".html\")\r\n-                    html_filepath = os.path.join(RAW_DIR, html_safe_filename)\r\n-                    escaped_text = html.escape(transcript)\r\n-                    html_content = f\"\"\"\r\n-<html>\r\n-<head>\r\n-    <style>\r\n-        body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }}\r\n-        pre {{ white-space: pre-wrap; word-wrap: break-word; }}\r\n-    </style>\r\n-</head>\r\n-<body>\r\n-    <h1>Processed Transcript for {url}</h1>\r\n-    <pre>{escaped_text}</pre>\r\n-</body>\r\n-</html>\r\n-\"\"\"\r\n-                    with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n-                        f.write(html_content)\r\n-\r\n-                    response += f'<a href=\"/file={html_safe_filename}\" target=\"_blank\">View Processed Transcript for {url} in new tab</a>\\n\\n'\r\n-                    history[-1][\"content\"] = response\r\n-                    yield history, \"\"\r\n-\r\n-                    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n-                    chunks = text_splitter.split_text(transcript)\r\n-                    new_docs = []\r\n-                    for chunk in chunks:\r\n-                        if add_chunk_if_new(conn, chunk, url, tag=tag):\r\n-                            metadata = {\"source\": url, \"tag\": tag}\r\n-                            if 'lyrics' in message.lower():\r\n-                                metadata[\"source_type\"] = \"lyrics\"\r\n-                            new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n-                    if new_docs:\r\n-                        with lock:\r\n-                            vectorstore.add_documents(new_docs)\r\n-                            vectorstore.save_local(FAISS_PATH)\r\n-                    response += f\"Number of new document chunks added from {url}: {len(new_docs)}\\n\\n\"\r\n-                    history[-1][\"content\"] = response\r\n-                    yield history, \"\"\r\n-                else:\r\n-                    response += f\"Failed to fetch transcript for {url}\\n\"\r\n-                    history[-1][\"content\"] = response\r\n-                    yield history, \"\"\r\n-\r\n-            search_kwargs = {\"k\": 5, \"filter\": {\"tag\": tag}}\r\n-            dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n-            bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n-            retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n-        else:\r\n-            response += \"**Processing Status:**\\n\"\r\n-            search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n-            if 'lyrics' in message.lower():\r\n-                search_kwargs[\"filter\"][\"source_type\"] = \"lyrics\"\r\n-            dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n-            bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n-            retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n-\r\n     if retriever is None:\r\n         qa_prompt = ChatPromptTemplate.from_template(\r\n             \"Answer the question:\\n\\nQuestion: {input}\"\r\n         )\r\n@@ -229,12 +63,8 @@\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n             return\r\n \r\n-        response += \"Creating/Updating vector store...\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n         response += \"Vector store ready.\\n\\n\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n \r\n@@ -278,11 +108,5 @@\n         response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n \r\n-    if selected_source in [\"Web Search\", \"YouTube\", \"Reddit\", \"Subreddit\"]:\r\n-        sources_str = \"\\n\".join([f\"- {url}\" for url in sources])\r\n-        response += f\"**Sources Crawled:**\\n{sources_str}\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n     print(\"chat_bot completed.\")\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756940290543,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,5 +1,4 @@\n-# chat_utils.py\r\n import os\r\n from config import MODEL_NAME, FAISS_PATH\r\n from langchain_ollama import OllamaLLM\r\n from langchain_core.messages import HumanMessage, AIMessage\r\n@@ -77,8 +76,10 @@\n         )\r\n         summary_chain = create_stuff_documents_chain(llm, summary_prompt)\r\n         summary_chain_with_docs = create_retrieval_chain(retriever, summary_chain)\r\n         summary_response = summary_chain_with_docs.invoke({\"input\": message})\r\n+        summary_docs = summary_response[\"context\"]  # Log retrieved docs\r\n+        print(\"Retrieved docs for summary:\", [doc.metadata for doc in summary_docs])\r\n         summary = summary_response[\"answer\"]\r\n \r\n         response += f\"**Summarization of Found Content:**\\n{summary}\\n\\n\"\r\n         history[-1][\"content\"] = response\r\n@@ -91,9 +92,9 @@\n         rephrase_prompt = ChatPromptTemplate.from_messages(\r\n             [\r\n                 MessagesPlaceholder(variable_name=\"chat_history\"),\r\n                 (\"human\", \"{input}\"),\r\n-                (\"human\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\"),\r\n+                (\"human\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Focus on the current question and ignore unrelated history.\"),\r\n             ]\r\n         )\r\n         history_aware_retriever = create_history_aware_retriever(llm, retriever, rephrase_prompt)\r\n \r\n@@ -102,8 +103,10 @@\n         )\r\n         qa_chain = create_stuff_documents_chain(llm, qa_prompt)\r\n         qa_chain_with_docs = create_retrieval_chain(history_aware_retriever, qa_chain)\r\n         qa_response = qa_chain_with_docs.invoke({\"input\": message, \"chat_history\": chat_history})\r\n+        qa_docs = qa_response[\"context\"]  # Log retrieved docs\r\n+        print(\"Retrieved docs for QA:\", [doc.metadata for doc in qa_docs])\r\n         answer = qa_response[\"answer\"]\r\n \r\n         response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n         history[-1][\"content\"] = response\r\n"
                },
                {
                    "date": 1756940737007,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,4 +1,5 @@\n+# chat_utils.py\r\n import os\r\n from config import MODEL_NAME, FAISS_PATH\r\n from langchain_ollama import OllamaLLM\r\n from langchain_core.messages import HumanMessage, AIMessage\r\n@@ -7,9 +8,9 @@\n from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\r\n from langchain.retrievers import EnsembleRetriever\r\n from langchain_community.retrievers import BM25Retriever\r\n from langchain_core.documents import Document\r\n-from vectorstore_utils import vectorstore\r\n+from vectorstore_manager import get_vectorstore\r\n import time\r\n import spacy\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n@@ -39,13 +40,14 @@\n \r\n     retriever = None\r\n     if selected_source != \"No RAG\":\r\n         response += \"**Processing Status:**\\n\"\r\n+        vs = get_vectorstore(selected_tag)\r\n         search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n         if 'lyrics' in message.lower():\r\n             search_kwargs[\"filter\"][\"source_type\"] = \"lyrics\"\r\n-        dense_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\r\n-        bm25_retriever = BM25Retriever.from_documents(vectorstore.similarity_search(\" \", k=vectorstore.index.ntotal))\r\n+        dense_retriever = vs.as_retriever(search_kwargs=search_kwargs)\r\n+        bm25_retriever = BM25Retriever.from_documents(vs.similarity_search(\" \", k=vs.index.ntotal))\r\n         retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n \r\n     if retriever is None:\r\n         qa_prompt = ChatPromptTemplate.from_template(\r\n@@ -56,9 +58,9 @@\n         response = f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n     else:\r\n-        if vectorstore.index.ntotal == 0:\r\n+        if vs.index.ntotal == 0:\r\n             response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n             return\r\n"
                },
                {
                    "date": 1756941056338,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,9 +8,9 @@\n from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\r\n from langchain.retrievers import EnsembleRetriever\r\n from langchain_community.retrievers import BM25Retriever\r\n from langchain_core.documents import Document\r\n-from vectorstore_manager import get_vectorstore\r\n+from vectorstore_utils import get_vectorstore\r\n import time\r\n import spacy\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n"
                },
                {
                    "date": 1756941439304,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -100,9 +100,9 @@\n         )\r\n         history_aware_retriever = create_history_aware_retriever(llm, retriever, rephrase_prompt)\r\n \r\n         qa_prompt = ChatPromptTemplate.from_template(\r\n-            \"Use the context to answer the question as accurately as possible. If lyrics are present, extract and format them clearly with verses, chorus, etc., and ignore non-lyric content like discussions. If uncertain or incomplete, note limitations but provide what's available:\\n\\n{context}\\n\\nQuestion: {input}\"\r\n+            \"Use the context to answer the question as accurately as possible. If uncertain or incomplete, note limitations but provide what's available:\\n\\n{context}\\n\\nQuestion: {input}\"\r\n         )\r\n         qa_chain = create_stuff_documents_chain(llm, qa_prompt)\r\n         qa_chain_with_docs = create_retrieval_chain(history_aware_retriever, qa_chain)\r\n         qa_response = qa_chain_with_docs.invoke({\"input\": message, \"chat_history\": chat_history})\r\n"
                },
                {
                    "date": 1756941856494,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,16 +8,22 @@\n from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\r\n from langchain.retrievers import EnsembleRetriever\r\n from langchain_community.retrievers import BM25Retriever\r\n from langchain_core.documents import Document\r\n-from vectorstore_utils import get_vectorstore\r\n+from vectorstore_manager import get_vectorstore\r\n import time\r\n import spacy\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n     print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n+    if selected_tag is None and selected_source != \"No RAG\":\r\n+        response = \"No tag found for selected source. Please select a different source or refresh.\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+        return\r\n+\r\n     history.append({\"role\": \"user\", \"content\": message})\r\n     yield history, \"\"\r\n \r\n     response = \"\"\r\n@@ -100,9 +106,9 @@\n         )\r\n         history_aware_retriever = create_history_aware_retriever(llm, retriever, rephrase_prompt)\r\n \r\n         qa_prompt = ChatPromptTemplate.from_template(\r\n-            \"Use the context to answer the question as accurately as possible. If uncertain or incomplete, note limitations but provide what's available:\\n\\n{context}\\n\\nQuestion: {input}\"\r\n+            \"Use the context to answer the question as accurately as possible. If lyrics are present, extract and format them clearly with verses, chorus, etc., and ignore non-lyric content like discussions. If uncertain or incomplete, note limitations but provide what's available:\\n\\n{context}\\n\\nQuestion: {input}\"\r\n         )\r\n         qa_chain = create_stuff_documents_chain(llm, qa_prompt)\r\n         qa_chain_with_docs = create_retrieval_chain(history_aware_retriever, qa_chain)\r\n         qa_response = qa_chain_with_docs.invoke({\"input\": message, \"chat_history\": chat_history})\r\n"
                },
                {
                    "date": 1756942480745,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,123 @@\n+# chat_utils.py\r\n+import os\r\n+from config import MODEL_NAME, FAISS_PATH\r\n+from langchain_ollama import OllamaLLM\r\n+from langchain_core.messages import HumanMessage, AIMessage\r\n+from langchain.chains import create_retrieval_chain, create_history_aware_retriever\r\n+from langchain.chains.combine_documents import create_stuff_documents_chain\r\n+from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\r\n+from langchain.retrievers import EnsembleRetriever\r\n+from langchain_community.retrievers import BM25Retriever\r\n+from langchain_core.documents import Document\r\n+from vectorstore_manager import get_vectorstore\r\n+import time\r\n+import spacy\r\n+\r\n+nlp = spacy.load(\"en_core_web_sm\")\r\n+\r\n+def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n+    print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n+    if selected_tag is None and selected_source != \"No RAG\":\r\n+        response = \"No tag found for selected source. Please select a different source or refresh.\"\r\n+        history.append({\"role\": \"assistant\", \"content\": response})\r\n+        yield history, \"\"\r\n+        return\r\n+\r\n+    history.append({\"role\": \"user\", \"content\": message})\r\n+    yield history, \"\"\r\n+\r\n+    response = \"\"\r\n+    history.append({\"role\": \"assistant\", \"content\": response})\r\n+    yield history, \"\"\r\n+\r\n+    # NLP processing for intent and NER\r\n+    doc = nlp(message)\r\n+    entities = [ent.text for ent in doc.ents]\r\n+    print(f\"Debug: Extracted entities: {entities}\")\r\n+\r\n+    chat_history = []\r\n+    for h in history[:-1]:\r\n+        if h[\"role\"] == \"user\":\r\n+            chat_history.append(HumanMessage(content=h[\"content\"]))\r\n+        elif h[\"role\"] == \"assistant\":\r\n+            chat_history.append(AIMessage(content=h[\"content\"]))\r\n+\r\n+    llm = OllamaLLM(model=MODEL_NAME)\r\n+\r\n+    retriever = None\r\n+    if selected_source != \"No RAG\":\r\n+        response += \"**Processing Status:**\\n\"\r\n+        vs = get_vectorstore(selected_tag)\r\n+        search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n+        if 'lyrics' in message.lower():\r\n+            search_kwargs[\"filter\"][\"source_type\"] = \"lyrics\"\r\n+        dense_retriever = vs.as_retriever(search_kwargs=search_kwargs)\r\n+        bm25_retriever = BM25Retriever.from_documents(vs.similarity_search(\" \", k=vs.index.ntotal))\r\n+        retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n+\r\n+    if retriever is None:\r\n+        qa_prompt = ChatPromptTemplate.from_template(\r\n+            \"Answer the question:\\n\\nQuestion: {input}\"\r\n+        )\r\n+        qa_chain = qa_prompt | llm\r\n+        answer = qa_chain.invoke({\"input\": message})\r\n+        response = f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+    else:\r\n+        if vs.index.ntotal == 0:\r\n+            response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+            return\r\n+\r\n+        response += \"Vector store ready.\\n\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+        response += \"Generating summarization of the found content...\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+        summary_prompt = ChatPromptTemplate.from_template(\r\n+            \"Summarize the following retrieved content related to the question '{input}' in a concise manner:\\n\\n{context}\"\r\n+        )\r\n+        summary_chain = create_stuff_documents_chain(llm, summary_prompt)\r\n+        summary_chain_with_docs = create_retrieval_chain(retriever, summary_chain)\r\n+        summary_response = summary_chain_with_docs.invoke({\"input\": message})\r\n+        summary_docs = summary_response[\"context\"]  # Log retrieved docs\r\n+        print(\"Retrieved docs for summary:\", [doc.metadata for doc in summary_docs])\r\n+        summary = summary_response[\"answer\"]\r\n+\r\n+        response += f\"**Summarization of Found Content:**\\n{summary}\\n\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+        response += \"Generating specific answer to the prompt...\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+        rephrase_prompt = ChatPromptTemplate.from_messages(\r\n+            [\r\n+                MessagesPlaceholder(variable_name=\"chat_history\"),\r\n+                (\"human\", \"{input}\"),\r\n+                (\"human\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Focus on the current question and ignore unrelated history.\"),\r\n+            ]\r\n+        )\r\n+        history_aware_retriever = create_history_aware_retriever(llm, retriever, rephrase_prompt)\r\n+\r\n+        qa_prompt = ChatPromptTemplate.from_template(\r\n+            \"Use the context to answer the question as accurately as possible. If lyrics are present, extract and format them clearly with verses, chorus, etc., and ignore non-lyric content like discussions. If uncertain or incomplete, note limitations but provide what's available:\\n\\n{context}\\n\\nQuestion: {input}\"\r\n+        )\r\n+        qa_chain = create_stuff_documents_chain(llm, qa_prompt)\r\n+        qa_chain_with_docs = create_retrieval_chain(history_aware_retriever, qa_chain)\r\n+        qa_response = qa_chain_with_docs.invoke({\"input\": message, \"chat_history\": chat_history})\r\n+        qa_docs = qa_response[\"context\"]  # Log retrieved docs\r\n+        print(\"Retrieved docs for QA:\", [doc.metadata for doc in qa_docs])\r\n+        answer = qa_response[\"answer\"]\r\n+\r\n+        response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+\r\n+    print(\"chat_bot completed.\")\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756942736508,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,14 +16,8 @@\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n     print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n-    if selected_tag is None and selected_source != \"No RAG\":\r\n-        response = \"No tag found for selected source. Please select a different source or refresh.\"\r\n-        history.append({\"role\": \"assistant\", \"content\": response})\r\n-        yield history, \"\"\r\n-        return\r\n-\r\n     history.append({\"role\": \"user\", \"content\": message})\r\n     yield history, \"\"\r\n \r\n     response = \"\"\r\n@@ -47,9 +41,11 @@\n     retriever = None\r\n     if selected_source != \"No RAG\":\r\n         response += \"**Processing Status:**\\n\"\r\n         vs = get_vectorstore(selected_tag)\r\n-        search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n+        search_kwargs = {\"k\": 5}\r\n+        if selected_tag:\r\n+            search_kwargs[\"filter\"] = {\"tag\": selected_tag}\r\n         if 'lyrics' in message.lower():\r\n             search_kwargs[\"filter\"][\"source_type\"] = \"lyrics\"\r\n         dense_retriever = vs.as_retriever(search_kwargs=search_kwargs)\r\n         bm25_retriever = BM25Retriever.from_documents(vs.similarity_search(\" \", k=vs.index.ntotal))\r\n@@ -119,128 +115,5 @@\n         response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n \r\n-    print(\"chat_bot completed.\")\n-# chat_utils.py\r\n-import os\r\n-from config import MODEL_NAME, FAISS_PATH\r\n-from langchain_ollama import OllamaLLM\r\n-from langchain_core.messages import HumanMessage, AIMessage\r\n-from langchain.chains import create_retrieval_chain, create_history_aware_retriever\r\n-from langchain.chains.combine_documents import create_stuff_documents_chain\r\n-from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\r\n-from langchain.retrievers import EnsembleRetriever\r\n-from langchain_community.retrievers import BM25Retriever\r\n-from langchain_core.documents import Document\r\n-from vectorstore_manager import get_vectorstore\r\n-import time\r\n-import spacy\r\n-\r\n-nlp = spacy.load(\"en_core_web_sm\")\r\n-\r\n-def chat_bot(message, history, conn=None, selected_source=None, selected_tag=None):\r\n-    print(f\"Starting chat_bot with message: {message}, selected_source: {selected_source}, selected_tag: {selected_tag}\")\r\n-    if selected_tag is None and selected_source != \"No RAG\":\r\n-        response = \"No tag found for selected source. Please select a different source or refresh.\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-        return\r\n-\r\n-    history.append({\"role\": \"user\", \"content\": message})\r\n-    yield history, \"\"\r\n-\r\n-    response = \"\"\r\n-    history.append({\"role\": \"assistant\", \"content\": response})\r\n-    yield history, \"\"\r\n-\r\n-    # NLP processing for intent and NER\r\n-    doc = nlp(message)\r\n-    entities = [ent.text for ent in doc.ents]\r\n-    print(f\"Debug: Extracted entities: {entities}\")\r\n-\r\n-    chat_history = []\r\n-    for h in history[:-1]:\r\n-        if h[\"role\"] == \"user\":\r\n-            chat_history.append(HumanMessage(content=h[\"content\"]))\r\n-        elif h[\"role\"] == \"assistant\":\r\n-            chat_history.append(AIMessage(content=h[\"content\"]))\r\n-\r\n-    llm = OllamaLLM(model=MODEL_NAME)\r\n-\r\n-    retriever = None\r\n-    if selected_source != \"No RAG\":\r\n-        response += \"**Processing Status:**\\n\"\r\n-        vs = get_vectorstore(selected_tag)\r\n-        search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n-        if 'lyrics' in message.lower():\r\n-            search_kwargs[\"filter\"][\"source_type\"] = \"lyrics\"\r\n-        dense_retriever = vs.as_retriever(search_kwargs=search_kwargs)\r\n-        bm25_retriever = BM25Retriever.from_documents(vs.similarity_search(\" \", k=vs.index.ntotal))\r\n-        retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n-\r\n-    if retriever is None:\r\n-        qa_prompt = ChatPromptTemplate.from_template(\r\n-            \"Answer the question:\\n\\nQuestion: {input}\"\r\n-        )\r\n-        qa_chain = qa_prompt | llm\r\n-        answer = qa_chain.invoke({\"input\": message})\r\n-        response = f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-    else:\r\n-        if vs.index.ntotal == 0:\r\n-            response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-            return\r\n-\r\n-        response += \"Vector store ready.\\n\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n-        response += \"Generating summarization of the found content...\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n-        summary_prompt = ChatPromptTemplate.from_template(\r\n-            \"Summarize the following retrieved content related to the question '{input}' in a concise manner:\\n\\n{context}\"\r\n-        )\r\n-        summary_chain = create_stuff_documents_chain(llm, summary_prompt)\r\n-        summary_chain_with_docs = create_retrieval_chain(retriever, summary_chain)\r\n-        summary_response = summary_chain_with_docs.invoke({\"input\": message})\r\n-        summary_docs = summary_response[\"context\"]  # Log retrieved docs\r\n-        print(\"Retrieved docs for summary:\", [doc.metadata for doc in summary_docs])\r\n-        summary = summary_response[\"answer\"]\r\n-\r\n-        response += f\"**Summarization of Found Content:**\\n{summary}\\n\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n-        response += \"Generating specific answer to the prompt...\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n-        rephrase_prompt = ChatPromptTemplate.from_messages(\r\n-            [\r\n-                MessagesPlaceholder(variable_name=\"chat_history\"),\r\n-                (\"human\", \"{input}\"),\r\n-                (\"human\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Focus on the current question and ignore unrelated history.\"),\r\n-            ]\r\n-        )\r\n-        history_aware_retriever = create_history_aware_retriever(llm, retriever, rephrase_prompt)\r\n-\r\n-        qa_prompt = ChatPromptTemplate.from_template(\r\n-            \"Use the context to answer the question as accurately as possible. If lyrics are present, extract and format them clearly with verses, chorus, etc., and ignore non-lyric content like discussions. If uncertain or incomplete, note limitations but provide what's available:\\n\\n{context}\\n\\nQuestion: {input}\"\r\n-        )\r\n-        qa_chain = create_stuff_documents_chain(llm, qa_prompt)\r\n-        qa_chain_with_docs = create_retrieval_chain(history_aware_retriever, qa_chain)\r\n-        qa_response = qa_chain_with_docs.invoke({\"input\": message, \"chat_history\": chat_history})\r\n-        qa_docs = qa_response[\"context\"]  # Log retrieved docs\r\n-        print(\"Retrieved docs for QA:\", [doc.metadata for doc in qa_docs])\r\n-        answer = qa_response[\"answer\"]\r\n-\r\n-        response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-\r\n     print(\"chat_bot completed.\")\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756944100215,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,18 +39,20 @@\n     llm = OllamaLLM(model=MODEL_NAME)\r\n \r\n     retriever = None\r\n     if selected_source != \"No RAG\":\r\n-        response += \"**Processing Status:**\\n\"\r\n-        vs = get_vectorstore(selected_tag)\r\n-        search_kwargs = {\"k\": 5}\r\n-        if selected_tag:\r\n-            search_kwargs[\"filter\"] = {\"tag\": selected_tag}\r\n-        if 'lyrics' in message.lower():\r\n-            search_kwargs[\"filter\"][\"source_type\"] = \"lyrics\"\r\n-        dense_retriever = vs.as_retriever(search_kwargs=search_kwargs)\r\n-        bm25_retriever = BM25Retriever.from_documents(vs.similarity_search(\" \", k=vs.index.ntotal))\r\n-        retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n+        if selected_tag is None:\r\n+            response += \"No tag found for selected source. Using No RAG mode.\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+        else:\r\n+            vs = get_vectorstore(selected_tag)\r\n+            search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n+            if 'lyrics' in message.lower():\r\n+                search_kwargs[\"filter\"][\"source_type\"] = \"lyrics\"\r\n+            dense_retriever = vs.as_retriever(search_kwargs=search_kwargs)\r\n+            bm25_retriever = BM25Retriever.from_documents(vs.similarity_search(\" \", k=vs.index.ntotal))\r\n+            retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n \r\n     if retriever is None:\r\n         qa_prompt = ChatPromptTemplate.from_template(\r\n             \"Answer the question:\\n\\nQuestion: {input}\"\r\n"
                },
                {
                    "date": 1756945424363,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,8 +23,14 @@\n     response = \"\"\r\n     history.append({\"role\": \"assistant\", \"content\": response})\r\n     yield history, \"\"\r\n \r\n+    if selected_tag is None and selected_source != \"No RAG\":\r\n+        response += \"No tag found for selected source. Using No RAG mode.\\n\"\r\n+        history[-1][\"content\"] = response\r\n+        yield history, \"\"\r\n+        selected_source = \"No RAG\"\r\n+\r\n     # NLP processing for intent and NER\r\n     doc = nlp(message)\r\n     entities = [ent.text for ent in doc.ents]\r\n     print(f\"Debug: Extracted entities: {entities}\")\r\n@@ -39,20 +45,16 @@\n     llm = OllamaLLM(model=MODEL_NAME)\r\n \r\n     retriever = None\r\n     if selected_source != \"No RAG\":\r\n-        if selected_tag is None:\r\n-            response += \"No tag found for selected source. Using No RAG mode.\\n\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-        else:\r\n-            vs = get_vectorstore(selected_tag)\r\n-            search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n-            if 'lyrics' in message.lower():\r\n-                search_kwargs[\"filter\"][\"source_type\"] = \"lyrics\"\r\n-            dense_retriever = vs.as_retriever(search_kwargs=search_kwargs)\r\n-            bm25_retriever = BM25Retriever.from_documents(vs.similarity_search(\" \", k=vs.index.ntotal))\r\n-            retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n+        response += \"**Processing Status:**\\n\"\r\n+        vs = get_vectorstore(selected_tag)\r\n+        search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n+        if 'lyrics' in message.lower():\r\n+            search_kwargs[\"filter\"][\"source_type\"] = \"lyrics\"\r\n+        dense_retriever = vs.as_retriever(search_kwargs=search_kwargs)\r\n+        bm25_retriever = BM25Retriever.from_documents(vs.similarity_search(\" \", k=vs.index.ntotal))\r\n+        retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n \r\n     if retriever is None:\r\n         qa_prompt = ChatPromptTemplate.from_template(\r\n             \"Answer the question:\\n\\nQuestion: {input}\"\r\n"
                },
                {
                    "date": 1756952278640,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,14 +23,8 @@\n     response = \"\"\r\n     history.append({\"role\": \"assistant\", \"content\": response})\r\n     yield history, \"\"\r\n \r\n-    if selected_tag is None and selected_source != \"No RAG\":\r\n-        response += \"No tag found for selected source. Using No RAG mode.\\n\"\r\n-        history[-1][\"content\"] = response\r\n-        yield history, \"\"\r\n-        selected_source = \"No RAG\"\r\n-\r\n     # NLP processing for intent and NER\r\n     doc = nlp(message)\r\n     entities = [ent.text for ent in doc.ents]\r\n     print(f\"Debug: Extracted entities: {entities}\")\r\n@@ -47,13 +41,19 @@\n     retriever = None\r\n     if selected_source != \"No RAG\":\r\n         response += \"**Processing Status:**\\n\"\r\n         vs = get_vectorstore(selected_tag)\r\n+        if vs.index.ntotal == 0:\r\n+            response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+            return\r\n         search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n         if 'lyrics' in message.lower():\r\n             search_kwargs[\"filter\"][\"source_type\"] = \"lyrics\"\r\n         dense_retriever = vs.as_retriever(search_kwargs=search_kwargs)\r\n-        bm25_retriever = BM25Retriever.from_documents(vs.similarity_search(\" \", k=vs.index.ntotal))\r\n+        bm_docs = vs.similarity_search(\" \", k=min(vs.index.ntotal, 100))  # Limit to avoid large empty stores\r\n+        bm25_retriever = BM25Retriever.from_documents(bm_docs)\r\n         retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n \r\n     if retriever is None:\r\n         qa_prompt = ChatPromptTemplate.from_template(\r\n@@ -64,14 +64,8 @@\n         response = f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n     else:\r\n-        if vs.index.ntotal == 0:\r\n-            response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-            return\r\n-\r\n         response += \"Vector store ready.\\n\\n\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n \r\n"
                },
                {
                    "date": 1756952648748,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -41,13 +41,8 @@\n     retriever = None\r\n     if selected_source != \"No RAG\":\r\n         response += \"**Processing Status:**\\n\"\r\n         vs = get_vectorstore(selected_tag)\r\n-        if vs.index.ntotal == 0:\r\n-            response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-            return\r\n         search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n         if 'lyrics' in message.lower():\r\n             search_kwargs[\"filter\"][\"source_type\"] = \"lyrics\"\r\n         dense_retriever = vs.as_retriever(search_kwargs=search_kwargs)\r\n@@ -64,8 +59,14 @@\n         response = f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n     else:\r\n+        if vs.index.ntotal == 0:\r\n+            response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+            return\r\n+\r\n         response += \"Vector store ready.\\n\\n\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n \r\n"
                },
                {
                    "date": 1756953402738,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -41,8 +41,14 @@\n     retriever = None\r\n     if selected_source != \"No RAG\":\r\n         response += \"**Processing Status:**\\n\"\r\n         vs = get_vectorstore(selected_tag)\r\n+        print(f\"Vector store for tag {selected_tag} has {vs.index.ntotal} documents.\")  # Debug nt\r\n+        if vs.index.ntotal == 0:\r\n+            response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+            return\r\n         search_kwargs = {\"k\": 5, \"filter\": {\"tag\": selected_tag}}\r\n         if 'lyrics' in message.lower():\r\n             search_kwargs[\"filter\"][\"source_type\"] = \"lyrics\"\r\n         dense_retriever = vs.as_retriever(search_kwargs=search_kwargs)\r\n@@ -59,14 +65,8 @@\n         response = f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n     else:\r\n-        if vs.index.ntotal == 0:\r\n-            response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-            return\r\n-\r\n         response += \"Vector store ready.\\n\\n\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n \r\n"
                },
                {
                    "date": 1756958008451,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -41,9 +41,8 @@\n     retriever = None\r\n     if selected_source != \"No RAG\":\r\n         response += \"**Processing Status:**\\n\"\r\n         vs = get_vectorstore(selected_tag)\r\n-        print(f\"Vector store for tag {selected_tag} has {vs.index.ntotal} documents.\")  # Debug nt\r\n         if vs.index.ntotal == 0:\r\n             response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n"
                },
                {
                    "date": 1756958084403,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -41,8 +41,9 @@\n     retriever = None\r\n     if selected_source != \"No RAG\":\r\n         response += \"**Processing Status:**\\n\"\r\n         vs = get_vectorstore(selected_tag)\r\n+        print(f\"Vector store for tag {selected_tag} has {vs.index.ntotal} documents.\")  # Debug nt\r\n         if vs.index.ntotal == 0:\r\n             response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n"
                },
                {
                    "date": 1756959812654,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -41,9 +41,9 @@\n     retriever = None\r\n     if selected_source != \"No RAG\":\r\n         response += \"**Processing Status:**\\n\"\r\n         vs = get_vectorstore(selected_tag)\r\n-        print(f\"Vector store for tag {selected_tag} has {vs.index.ntotal} documents.\")  # Debug nt\r\n+        print(f\"Debug: Vector store for tag {selected_tag} loaded with ntotal: {vs.index.ntotal}\")\r\n         if vs.index.ntotal == 0:\r\n             response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n@@ -54,8 +54,9 @@\n         dense_retriever = vs.as_retriever(search_kwargs=search_kwargs)\r\n         bm_docs = vs.similarity_search(\" \", k=min(vs.index.ntotal, 100))  # Limit to avoid large empty stores\r\n         bm25_retriever = BM25Retriever.from_documents(bm_docs)\r\n         retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.7, 0.3])\r\n+        print(f\"Debug: Created ensemble retriever for tag {selected_tag}. BM25 docs loaded: {len(bm_docs)}\")\r\n \r\n     if retriever is None:\r\n         qa_prompt = ChatPromptTemplate.from_template(\r\n             \"Answer the question:\\n\\nQuestion: {input}\"\r\n"
                },
                {
                    "date": 1756963196361,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -115,5 +115,18 @@\n         response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n         history[-1][\"content\"] = response\r\n         yield history, \"\"\r\n \r\n+        # Collect unique sources from retrieved documents\r\n+        all_docs = set()\r\n+        for doc in summary_docs + qa_docs:\r\n+            if 'source' in doc.metadata:\r\n+                all_docs.add(doc.metadata['source'])\r\n+\r\n+        if all_docs:\r\n+            response += \"**Referenced Sources:**\\n\"\r\n+            for source in all_docs:\r\n+                response += f\"- {source}\\n\"\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+\r\n     print(\"chat_bot completed.\")\n\\ No newline at end of file\n"
                }
            ],
            "date": 1756856986719,
            "name": "Commit-0",
            "content": "from langchain_ollama import OllamaLLM\r\nfrom langchain_core.messages import HumanMessage, AIMessage\r\nfrom langchain.chains import create_retrieval_chain, create_history_aware_retriever\r\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\r\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\r\nfrom web_utils import search_web\r\nfrom config import MAX_URLS\r\nfrom process_utils import process_urls\r\nfrom vectorstore_utils import vectorstore\r\n\r\ndef chat_bot(message, history, conn=None):\r\n    print(f\"Starting chat_bot with message: {message}\")\r\n    history.append({\"role\": \"user\", \"content\": message})\r\n    yield \"\", history\r\n\r\n    response = \"**Processing Status:**\\n\"\r\n    history.append({\"role\": \"assistant\", \"content\": response})\r\n    yield \"\", history\r\n\r\n    chat_history = []\r\n    for h in history[:-1]:\r\n        if h[\"role\"] == \"user\":\r\n            chat_history.append(HumanMessage(content=h[\"content\"]))\r\n        elif h[\"role\"] == \"assistant\":\r\n            chat_history.append(AIMessage(content=h[\"content\"]))\r\n\r\n    response += f\"Searching web for query: {message}\\n\"\r\n    history[-1][\"content\"] = response\r\n    yield \"\", history\r\n\r\n    google_urls = search_web(message)\r\n\r\n    response += f\"Found {len(google_urls)} Google URLs: {', '.join(google_urls)}\\n\\n\"\r\n    history[-1][\"content\"] = response\r\n    yield \"\", history\r\n\r\n    response += f\"Searching Reddit for query: {message}\\n\"\r\n    history[-1][\"content\"] = response\r\n    yield \"\", history\r\n\r\n    reddit_urls = search_web(message, site=\"reddit.com\")\r\n\r\n    response += f\"Found {len(reddit_urls)} Reddit URLs: {', '.join(reddit_urls)}\\n\\n\"\r\n    history[-1][\"content\"] = response\r\n    yield \"\", history\r\n\r\n    all_urls = list(set(google_urls + reddit_urls))[:MAX_URLS]\r\n    response += f\"All unique URLs to process (limited to {MAX_URLS}): {', '.join(all_urls)}\\n\\n\"\r\n    history[-1][\"content\"] = response\r\n    yield \"\", history\r\n\r\n    process_gen = process_urls(all_urls, response, history, message, conn=conn)\r\n    for upd in process_gen:\r\n        yield upd\r\n\r\n    # Consume the generator fully to get return value\r\n    try:\r\n        sources, response, history = next(process_gen)\r\n    except StopIteration as e:\r\n        if e.value:\r\n            sources, response, history = e.value\r\n        else:\r\n            sources = []\r\n\r\n    if vectorstore.index.ntotal == 0:\r\n        response += \"No relevant content in vectorstore.\\n\\n**Specific Answer:**\\nSorry, I couldn't find any information.\"\r\n        history[-1][\"content\"] = response\r\n        yield \"\", history\r\n        return\r\n\r\n    response += \"Creating/Updating vector store...\\n\"\r\n    history[-1][\"content\"] = response\r\n    yield \"\", history\r\n\r\n    response += \"Vector store ready.\\n\\n\"\r\n    history[-1][\"content\"] = response\r\n    yield \"\", history\r\n\r\n    llm = OllamaLLM(model=MODEL_NAME)\r\n\r\n    response += \"Generating summarization of the found content...\\n\"\r\n    history[-1][\"content\"] = response\r\n    yield \"\", history\r\n\r\n    summary_prompt = ChatPromptTemplate.from_template(\r\n        \"Summarize the following retrieved content related to the question '{input}' in a concise manner:\\n\\n{context}\"\r\n    )\r\n    summary_chain = create_stuff_documents_chain(llm, summary_prompt)\r\n    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\r\n    summary_chain_with_docs = create_retrieval_chain(retriever, summary_chain)\r\n    summary_response = summary_chain_with_docs.invoke({\"input\": message})\r\n    summary = summary_response[\"answer\"]\r\n\r\n    response += f\"**Summarization of Found Content:**\\n{summary}\\n\\n\"\r\n    history[-1][\"content\"] = response\r\n    yield \"\", history\r\n\r\n    response += \"Generating specific answer to the prompt...\\n\"\r\n    history[-1][\"content\"] = response\r\n    yield \"\", history\r\n\r\n    rephrase_prompt = ChatPromptTemplate.from_messages(\r\n        [\r\n            MessagesPlaceholder(variable_name=\"chat_history\"),\r\n            (\"human\", \"{input}\"),\r\n            (\"human\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\"),\r\n        ]\r\n    )\r\n    history_aware_retriever = create_history_aware_retriever(llm, retriever, rephrase_prompt)\r\n\r\n    qa_prompt = ChatPromptTemplate.from_template(\r\n        \"Answer the question based only on the following context:\\n\\n{context}\\n\\nQuestion: {input}\\nIf the context doesn't contain relevant information, say 'I don't know'.\"\r\n    )\r\n    qa_chain = create_stuff_documents_chain(llm, qa_prompt)\r\n    qa_chain_with_docs = create_retrieval_chain(history_aware_retriever, qa_chain)\r\n    qa_response = qa_chain_with_docs.invoke({\"input\": message, \"chat_history\": chat_history})\r\n    answer = qa_response[\"answer\"]\r\n\r\n    response += f\"**Specific Answer:**\\n{answer}\\n\\n\"\r\n    history[-1][\"content\"] = response\r\n    yield \"\", history\r\n\r\n    sources_str = \"\\n\".join([f\"- {url}\" for url in sources])\r\n    response += f\"**Sources Crawled:**\\n{sources_str}\"\r\n    history[-1][\"content\"] = response\r\n    yield \"\", history\r\n\r\n    print(\"chat_bot completed.\")"
        }
    ]
}