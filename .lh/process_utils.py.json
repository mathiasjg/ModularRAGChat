{
    "sourceFile": "process_utils.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 33,
            "patches": [
                {
                    "date": 1756856980340,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1756857253292,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,8 +6,9 @@\n from db_utils import get_stored_content, store_content, add_chunk_if_new\r\n from vectorstore_utils import vectorstore\r\n from utils import lock\r\n from web_utils import clean_web_content\r\n+import os\r\n \r\n def process_urls(all_urls, response, history, message, is_chat=True, conn=None):\r\n     print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}\")\r\n     documents = []\r\n"
                },
                {
                    "date": 1756857432936,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,14 +1,13 @@\n-from config import RAW_DIR, MAX_DISPLAY_CHARS\r\n+from config import RAW_DIR, MAX_DISPLAY_CHARS, FAISS_PATH\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n from concurrent.futures import ThreadPoolExecutor, as_completed\r\n from urllib.parse import quote\r\n from db_utils import get_stored_content, store_content, add_chunk_if_new\r\n from vectorstore_utils import vectorstore\r\n from utils import lock\r\n from web_utils import clean_web_content\r\n-import os\r\n \r\n def process_urls(all_urls, response, history, message, is_chat=True, conn=None):\r\n     print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}\")\r\n     documents = []\r\n@@ -52,34 +51,5 @@\n                 if is_chat:\r\n                     history[-1]['content'] = response\r\n                     yield \"\", history\r\n \r\n-                safe_filename = quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n-                filepath = os.path.join(RAW_DIR, safe_filename)\r\n-                with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n-                    f.write(cleaned_text)\r\n-\r\n-                response += f\"[Download full raw text for {url}](/file={filepath})\\n\\n\"\r\n-                if is_chat:\r\n-                    history[-1]['content'] = response\r\n-                    yield \"\", history\r\n-\r\n-                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\r\n-                chunks = text_splitter.split_text(cleaned_text)\r\n-                new_docs = []\r\n-                for chunk in chunks:\r\n-                    if add_chunk_if_new(conn, chunk, url):\r\n-                        new_docs.append(Document(page_content=chunk, metadata={\"source\": url}))\r\n-\r\n-                if new_docs:\r\n-                    with lock:\r\n-                        vectorstore.add_documents(new_docs)\r\n-                        vectorstore.save_local(FAISS_PATH)\r\n-                    documents.extend(new_docs)\r\n-\r\n-    response += f\"Number of new document chunks added: {len(documents)}\\n\\n\"\r\n-    if is_chat:\r\n-        history[-1]['content'] = response\r\n-        yield \"\", history\r\n-\r\n-    print(\"process_urls completed.\")\r\n-    return sources, response, history\n\\ No newline at end of file\n+                safe_filename = quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756857440128,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -51,5 +51,34 @@\n                 if is_chat:\r\n                     history[-1]['content'] = response\r\n                     yield \"\", history\r\n \r\n-                safe_filename = quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"\n\\ No newline at end of file\n+                safe_filename = quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n+                filepath = os.path.join(RAW_DIR, safe_filename)\r\n+                with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                    f.write(cleaned_text)\r\n+\r\n+                response += f\"[Download full raw text for {url}](/file={filepath})\\n\\n\"\r\n+                if is_chat:\r\n+                    history[-1]['content'] = response\r\n+                    yield \"\", history\r\n+\r\n+                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\r\n+                chunks = text_splitter.split_text(cleaned_text)\r\n+                new_docs = []\r\n+                for chunk in chunks:\r\n+                    if add_chunk_if_new(conn, chunk, url):\r\n+                        new_docs.append(Document(page_content=chunk, metadata={\"source\": url}))\r\n+\r\n+                if new_docs:\r\n+                    with lock:\r\n+                        vectorstore.add_documents(new_docs)\r\n+                        vectorstore.save_local(FAISS_PATH)\r\n+                    documents.extend(new_docs)\r\n+\r\n+    response += f\"Number of new document chunks added: {len(documents)}\\n\\n\"\r\n+    if is_chat:\r\n+        history[-1]['content'] = response\r\n+        yield \"\", history\r\n+\r\n+    print(\"process_urls completed.\")\r\n+    return sources, response, history\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756857630800,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,8 +6,9 @@\n from db_utils import get_stored_content, store_content, add_chunk_if_new\r\n from vectorstore_utils import vectorstore\r\n from utils import lock\r\n from web_utils import clean_web_content\r\n+import os\r\n \r\n def process_urls(all_urls, response, history, message, is_chat=True, conn=None):\r\n     print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}\")\r\n     documents = []\r\n"
                },
                {
                    "date": 1756858482092,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,9 +6,8 @@\n from db_utils import get_stored_content, store_content, add_chunk_if_new\r\n from vectorstore_utils import vectorstore\r\n from utils import lock\r\n from web_utils import clean_web_content\r\n-import os\r\n \r\n def process_urls(all_urls, response, history, message, is_chat=True, conn=None):\r\n     print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}\")\r\n     documents = []\r\n"
                },
                {
                    "date": 1756858491475,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,8 +6,9 @@\n from db_utils import get_stored_content, store_content, add_chunk_if_new\r\n from vectorstore_utils import vectorstore\r\n from utils import lock\r\n from web_utils import clean_web_content\r\n+import os \r\n \r\n def process_urls(all_urls, response, history, message, is_chat=True, conn=None):\r\n     print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}\")\r\n     documents = []\r\n"
                },
                {
                    "date": 1756858862973,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,4 +1,5 @@\n+import os\r\n from config import RAW_DIR, MAX_DISPLAY_CHARS, FAISS_PATH\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n from concurrent.futures import ThreadPoolExecutor, as_completed\r\n@@ -6,12 +7,11 @@\n from db_utils import get_stored_content, store_content, add_chunk_if_new\r\n from vectorstore_utils import vectorstore\r\n from utils import lock\r\n from web_utils import clean_web_content\r\n-import os \r\n \r\n-def process_urls(all_urls, response, history, message, is_chat=True, conn=None):\r\n-    print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}\")\r\n+def process_urls(all_urls, response, history, message, is_chat=True, conn=None, source_tag=None):\r\n+    print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}, source_tag: {source_tag}\")\r\n     documents = []\r\n     sources = []\r\n     with ThreadPoolExecutor(max_workers=5) as executor:\r\n         future_to_url = {}\r\n@@ -67,9 +67,12 @@\n                 chunks = text_splitter.split_text(cleaned_text)\r\n                 new_docs = []\r\n                 for chunk in chunks:\r\n                     if add_chunk_if_new(conn, chunk, url):\r\n-                        new_docs.append(Document(page_content=chunk, metadata={\"source\": url}))\r\n+                        metadata = {\"source\": url}\r\n+                        if source_tag:\r\n+                            metadata[\"tag\"] = source_tag\r\n+                        new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n \r\n                 if new_docs:\r\n                     with lock:\r\n                         vectorstore.add_documents(new_docs)\r\n"
                },
                {
                    "date": 1756859256976,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,88 @@\n+import os\r\n+from config import RAW_DIR, MAX_DISPLAY_CHARS, FAISS_PATH\r\n+from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n+from langchain_core.documents import Document\r\n+from concurrent.futures import ThreadPoolExecutor, as_completed\r\n+from urllib.parse import quote\r\n+from db_utils import get_stored_content, store_content, add_chunk_if_new\r\n+from vectorstore_utils import vectorstore\r\n+from utils import lock\r\n+from web_utils import clean_web_content\r\n+\r\n+def process_urls(all_urls, response, history, message, is_chat=True, conn=None, source_tag=None):\r\n+    print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}, source_tag: {source_tag}\")\r\n+    documents = []\r\n+    sources = []\r\n+    with ThreadPoolExecutor(max_workers=5) as executor:\r\n+        future_to_url = {}\r\n+        for url in all_urls:\r\n+            stored = get_stored_content(conn, url)\r\n+            if stored:\r\n+                response += f\"Using stored content for {url}\\n\"\r\n+                if is_chat:\r\n+                    history[-1]['content'] = response\r\n+                    yield \"\", history\r\n+                cleaned_text = stored\r\n+            else:\r\n+                future = executor.submit(clean_web_content, url)\r\n+                future_to_url[future] = url\r\n+                cleaned_text = None\r\n+\r\n+        for future in as_completed(future_to_url):\r\n+            url = future_to_url[future]\r\n+            response += f\"Processed future for {url}\\n\"\r\n+            if is_chat:\r\n+                history[-1]['content'] = response\r\n+                yield \"\", history\r\n+            try:\r\n+                cleaned_text = future.result()\r\n+                if cleaned_text:\r\n+                    store_content(conn, url, cleaned_text)\r\n+            except Exception as e:\r\n+                print(f\"Exception in future result for {url}: {e}\")\r\n+                response += f\"Error processing {url}: {e}\\n\"\r\n+                if is_chat:\r\n+                    history[-1]['content'] = response\r\n+                    yield \"\", history\r\n+\r\n+            if cleaned_text:\r\n+                sources.append(url)\r\n+                display_text = cleaned_text[:MAX_DISPLAY_CHARS] + \"...\" if len(cleaned_text) > MAX_DISPLAY_CHARS else cleaned_text\r\n+                response += f\"Cleaned content from {url} (preview):\\n{display_text}\\n\\n\"\r\n+                if is_chat:\r\n+                    history[-1]['content'] = response\r\n+                    yield \"\", history\r\n+\r\n+                safe_filename = quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n+                filepath = os.path.join(RAW_DIR, safe_filename)\r\n+                with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                    f.write(cleaned_text)\r\n+\r\n+                response += f\"[Download full raw text for {url}](/file={filepath})\\n\\n\"\r\n+                if is_chat:\r\n+                    history[-1]['content'] = response\r\n+                    yield \"\", history\r\n+\r\n+                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\r\n+                chunks = text_splitter.split_text(cleaned_text)\r\n+                new_docs = []\r\n+                for chunk in chunks:\r\n+                    if add_chunk_if_new(conn, chunk, url):\r\n+                        metadata = {\"source\": url}\r\n+                        if source_tag:\r\n+                            metadata[\"tag\"] = source_tag\r\n+                        new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+\r\n+                if new_docs:\r\n+                    with lock:\r\n+                        vectorstore.add_documents(new_docs)\r\n+                        vectorstore.save_local(FAISS_PATH)\r\n+                    documents.extend(new_docs)\r\n+\r\n+    response += f\"Number of new document chunks added: {len(documents)}\\n\\n\"\r\n+    if is_chat:\r\n+        history[-1]['content'] = response\r\n+        yield \"\", history\r\n+\r\n+    print(\"process_urls completed.\")\r\n+    return sources, response, history\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756859805288,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,9 +20,9 @@\n             if stored:\r\n                 response += f\"Using stored content for {url}\\n\"\r\n                 if is_chat:\r\n                     history[-1]['content'] = response\r\n-                    yield \"\", history\r\n+                    yield history, \"\"\r\n                 cleaned_text = stored\r\n             else:\r\n                 future = executor.submit(clean_web_content, url)\r\n                 future_to_url[future] = url\r\n@@ -32,9 +32,9 @@\n             url = future_to_url[future]\r\n             response += f\"Processed future for {url}\\n\"\r\n             if is_chat:\r\n                 history[-1]['content'] = response\r\n-                yield \"\", history\r\n+                yield history, \"\"\r\n             try:\r\n                 cleaned_text = future.result()\r\n                 if cleaned_text:\r\n                     store_content(conn, url, cleaned_text)\r\n@@ -42,17 +42,17 @@\n                 print(f\"Exception in future result for {url}: {e}\")\r\n                 response += f\"Error processing {url}: {e}\\n\"\r\n                 if is_chat:\r\n                     history[-1]['content'] = response\r\n-                    yield \"\", history\r\n+                    yield history, \"\"\r\n \r\n             if cleaned_text:\r\n                 sources.append(url)\r\n                 display_text = cleaned_text[:MAX_DISPLAY_CHARS] + \"...\" if len(cleaned_text) > MAX_DISPLAY_CHARS else cleaned_text\r\n                 response += f\"Cleaned content from {url} (preview):\\n{display_text}\\n\\n\"\r\n                 if is_chat:\r\n                     history[-1]['content'] = response\r\n-                    yield \"\", history\r\n+                    yield history, \"\"\r\n \r\n                 safe_filename = quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n                 filepath = os.path.join(RAW_DIR, safe_filename)\r\n                 with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n@@ -60,9 +60,9 @@\n \r\n                 response += f\"[Download full raw text for {url}](/file={filepath})\\n\\n\"\r\n                 if is_chat:\r\n                     history[-1]['content'] = response\r\n-                    yield \"\", history\r\n+                    yield history, \"\"\r\n \r\n                 text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\r\n                 chunks = text_splitter.split_text(cleaned_text)\r\n                 new_docs = []\r\n@@ -81,96 +81,8 @@\n \r\n     response += f\"Number of new document chunks added: {len(documents)}\\n\\n\"\r\n     if is_chat:\r\n         history[-1]['content'] = response\r\n-        yield \"\", history\r\n+        yield history, \"\"\r\n \r\n     print(\"process_urls completed.\")\r\n-    return sources, response, history\n-import os\r\n-from config import RAW_DIR, MAX_DISPLAY_CHARS, FAISS_PATH\r\n-from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n-from langchain_core.documents import Document\r\n-from concurrent.futures import ThreadPoolExecutor, as_completed\r\n-from urllib.parse import quote\r\n-from db_utils import get_stored_content, store_content, add_chunk_if_new\r\n-from vectorstore_utils import vectorstore\r\n-from utils import lock\r\n-from web_utils import clean_web_content\r\n-\r\n-def process_urls(all_urls, response, history, message, is_chat=True, conn=None, source_tag=None):\r\n-    print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}, source_tag: {source_tag}\")\r\n-    documents = []\r\n-    sources = []\r\n-    with ThreadPoolExecutor(max_workers=5) as executor:\r\n-        future_to_url = {}\r\n-        for url in all_urls:\r\n-            stored = get_stored_content(conn, url)\r\n-            if stored:\r\n-                response += f\"Using stored content for {url}\\n\"\r\n-                if is_chat:\r\n-                    history[-1]['content'] = response\r\n-                    yield \"\", history\r\n-                cleaned_text = stored\r\n-            else:\r\n-                future = executor.submit(clean_web_content, url)\r\n-                future_to_url[future] = url\r\n-                cleaned_text = None\r\n-\r\n-        for future in as_completed(future_to_url):\r\n-            url = future_to_url[future]\r\n-            response += f\"Processed future for {url}\\n\"\r\n-            if is_chat:\r\n-                history[-1]['content'] = response\r\n-                yield \"\", history\r\n-            try:\r\n-                cleaned_text = future.result()\r\n-                if cleaned_text:\r\n-                    store_content(conn, url, cleaned_text)\r\n-            except Exception as e:\r\n-                print(f\"Exception in future result for {url}: {e}\")\r\n-                response += f\"Error processing {url}: {e}\\n\"\r\n-                if is_chat:\r\n-                    history[-1]['content'] = response\r\n-                    yield \"\", history\r\n-\r\n-            if cleaned_text:\r\n-                sources.append(url)\r\n-                display_text = cleaned_text[:MAX_DISPLAY_CHARS] + \"...\" if len(cleaned_text) > MAX_DISPLAY_CHARS else cleaned_text\r\n-                response += f\"Cleaned content from {url} (preview):\\n{display_text}\\n\\n\"\r\n-                if is_chat:\r\n-                    history[-1]['content'] = response\r\n-                    yield \"\", history\r\n-\r\n-                safe_filename = quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n-                filepath = os.path.join(RAW_DIR, safe_filename)\r\n-                with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n-                    f.write(cleaned_text)\r\n-\r\n-                response += f\"[Download full raw text for {url}](/file={filepath})\\n\\n\"\r\n-                if is_chat:\r\n-                    history[-1]['content'] = response\r\n-                    yield \"\", history\r\n-\r\n-                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\r\n-                chunks = text_splitter.split_text(cleaned_text)\r\n-                new_docs = []\r\n-                for chunk in chunks:\r\n-                    if add_chunk_if_new(conn, chunk, url):\r\n-                        metadata = {\"source\": url}\r\n-                        if source_tag:\r\n-                            metadata[\"tag\"] = source_tag\r\n-                        new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n-\r\n-                if new_docs:\r\n-                    with lock:\r\n-                        vectorstore.add_documents(new_docs)\r\n-                        vectorstore.save_local(FAISS_PATH)\r\n-                    documents.extend(new_docs)\r\n-\r\n-    response += f\"Number of new document chunks added: {len(documents)}\\n\\n\"\r\n-    if is_chat:\r\n-        history[-1]['content'] = response\r\n-        yield \"\", history\r\n-\r\n-    print(\"process_urls completed.\")\r\n     return sources, response, history\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756861084728,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -62,16 +62,18 @@\n                 if is_chat:\r\n                     history[-1]['content'] = response\r\n                     yield history, \"\"\r\n \r\n-                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\r\n+                text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n                 chunks = text_splitter.split_text(cleaned_text)\r\n                 new_docs = []\r\n                 for chunk in chunks:\r\n                     if add_chunk_if_new(conn, chunk, url):\r\n                         metadata = {\"source\": url}\r\n                         if source_tag:\r\n                             metadata[\"tag\"] = source_tag\r\n+                        if 'lyrics' in message.lower():\r\n+                            metadata[\"source_type\"] = \"lyrics\"\r\n                         new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n \r\n                 if new_docs:\r\n                     with lock:\r\n"
                },
                {
                    "date": 1756863197905,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,8 +7,10 @@\n from db_utils import get_stored_content, store_content, add_chunk_if_new\r\n from vectorstore_utils import vectorstore\r\n from utils import lock\r\n from web_utils import clean_web_content\r\n+from langchain_community.document_loaders import PyPDFLoader, TextLoader\r\n+from youtube_transcript_api import YouTubeTranscriptApi\r\n \r\n def process_urls(all_urls, response, history, message, is_chat=True, conn=None, source_tag=None):\r\n     print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}, source_tag: {source_tag}\")\r\n     documents = []\r\n@@ -86,5 +88,59 @@\n         history[-1]['content'] = response\r\n         yield history, \"\"\r\n \r\n     print(\"process_urls completed.\")\r\n-    return sources, response, history\n\\ No newline at end of file\n+    return sources, response, history\r\n+\r\n+def ingest_local_file(file_path, source_tag=None, conn=None):\r\n+    print(f\"Ingesting local file: {file_path}\")\r\n+    if file_path.endswith('.pdf'):\r\n+        loader = PyPDFLoader(file_path)\r\n+    elif file_path.endswith('.txt'):\r\n+        loader = TextLoader(file_path)\r\n+    else:\r\n+        return \"Unsupported file type\"\r\n+    \r\n+    docs = loader.load()\r\n+    text = \" \".join([doc.page_content for doc in docs])\r\n+    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n+    chunks = text_splitter.split_text(text)\r\n+    new_docs = []\r\n+    for chunk in chunks:\r\n+        if add_chunk_if_new(conn, chunk, file_path):\r\n+            metadata = {\"source\": file_path}\r\n+            if source_tag:\r\n+                metadata[\"tag\"] = source_tag\r\n+            new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+\r\n+    if new_docs:\r\n+        with lock:\r\n+            vectorstore.add_documents(new_docs)\r\n+            vectorstore.save_local(FAISS_PATH)\r\n+    print(f\"File ingested: {len(new_docs)} chunks added.\")\r\n+    return f\"File ingested successfully with tag {source_tag}.\"\r\n+\r\n+def ingest_youtube(url, source_tag=None, conn=None):\r\n+    print(f\"Ingesting YouTube video: {url}\")\r\n+    video_id = url.split(\"v=\")[-1].split(\"&\")[0]\r\n+    try:\r\n+        transcript = YouTubeTranscriptApi.get_transcript(video_id)\r\n+        text = \" \".join([entry['text'] for entry in transcript])\r\n+        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n+        chunks = text_splitter.split_text(text)\r\n+        new_docs = []\r\n+        for chunk in chunks:\r\n+            if add_chunk_if_new(conn, chunk, url):\r\n+                metadata = {\"source\": url}\r\n+                if source_tag:\r\n+                    metadata[\"tag\"] = source_tag\r\n+                new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+\r\n+        if new_docs:\r\n+            with lock:\r\n+                vectorstore.add_documents(new_docs)\r\n+                vectorstore.save_local(FAISS_PATH)\r\n+        print(f\"YouTube transcript ingested: {len(new_docs)} chunks added.\")\r\n+        return f\"YouTube transcript ingested successfully with tag {source_tag}.\"\r\n+    except Exception as e:\r\n+        print(f\"Error ingesting YouTube: {e}\")\r\n+        return \"Failed to ingest YouTube transcript.\"\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756874187163,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,10 +7,8 @@\n from db_utils import get_stored_content, store_content, add_chunk_if_new\r\n from vectorstore_utils import vectorstore\r\n from utils import lock\r\n from web_utils import clean_web_content\r\n-from langchain_community.document_loaders import PyPDFLoader, TextLoader\r\n-from youtube_transcript_api import YouTubeTranscriptApi\r\n \r\n def process_urls(all_urls, response, history, message, is_chat=True, conn=None, source_tag=None):\r\n     print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}, source_tag: {source_tag}\")\r\n     documents = []\r\n@@ -68,9 +66,9 @@\n                 text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n                 chunks = text_splitter.split_text(cleaned_text)\r\n                 new_docs = []\r\n                 for chunk in chunks:\r\n-                    if add_chunk_if_new(conn, chunk, url):\r\n+                    if add_chunk_if_new(conn, chunk, url, tag=source_tag):\r\n                         metadata = {\"source\": url}\r\n                         if source_tag:\r\n                             metadata[\"tag\"] = source_tag\r\n                         if 'lyrics' in message.lower():\r\n@@ -88,59 +86,5 @@\n         history[-1]['content'] = response\r\n         yield history, \"\"\r\n \r\n     print(\"process_urls completed.\")\r\n-    return sources, response, history\r\n-\r\n-def ingest_local_file(file_path, source_tag=None, conn=None):\r\n-    print(f\"Ingesting local file: {file_path}\")\r\n-    if file_path.endswith('.pdf'):\r\n-        loader = PyPDFLoader(file_path)\r\n-    elif file_path.endswith('.txt'):\r\n-        loader = TextLoader(file_path)\r\n-    else:\r\n-        return \"Unsupported file type\"\r\n-    \r\n-    docs = loader.load()\r\n-    text = \" \".join([doc.page_content for doc in docs])\r\n-    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n-    chunks = text_splitter.split_text(text)\r\n-    new_docs = []\r\n-    for chunk in chunks:\r\n-        if add_chunk_if_new(conn, chunk, file_path):\r\n-            metadata = {\"source\": file_path}\r\n-            if source_tag:\r\n-                metadata[\"tag\"] = source_tag\r\n-            new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n-\r\n-    if new_docs:\r\n-        with lock:\r\n-            vectorstore.add_documents(new_docs)\r\n-            vectorstore.save_local(FAISS_PATH)\r\n-    print(f\"File ingested: {len(new_docs)} chunks added.\")\r\n-    return f\"File ingested successfully with tag {source_tag}.\"\r\n-\r\n-def ingest_youtube(url, source_tag=None, conn=None):\r\n-    print(f\"Ingesting YouTube video: {url}\")\r\n-    video_id = url.split(\"v=\")[-1].split(\"&\")[0]\r\n-    try:\r\n-        transcript = YouTubeTranscriptApi.get_transcript(video_id)\r\n-        text = \" \".join([entry['text'] for entry in transcript])\r\n-        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n-        chunks = text_splitter.split_text(text)\r\n-        new_docs = []\r\n-        for chunk in chunks:\r\n-            if add_chunk_if_new(conn, chunk, url):\r\n-                metadata = {\"source\": url}\r\n-                if source_tag:\r\n-                    metadata[\"tag\"] = source_tag\r\n-                new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n-\r\n-        if new_docs:\r\n-            with lock:\r\n-                vectorstore.add_documents(new_docs)\r\n-                vectorstore.save_local(FAISS_PATH)\r\n-        print(f\"YouTube transcript ingested: {len(new_docs)} chunks added.\")\r\n-        return f\"YouTube transcript ingested successfully with tag {source_tag}.\"\r\n-    except Exception as e:\r\n-        print(f\"Error ingesting YouTube: {e}\")\r\n-        return \"Failed to ingest YouTube transcript.\"\n\\ No newline at end of file\n+    return sources, response, history\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756932181095,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,4 +1,5 @@\n+# process_utils.py\r\n import os\r\n from config import RAW_DIR, MAX_DISPLAY_CHARS, FAISS_PATH\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n@@ -8,10 +9,10 @@\n from vectorstore_utils import vectorstore\r\n from utils import lock\r\n from web_utils import clean_web_content\r\n \r\n-def process_urls(all_urls, response, history, message, is_chat=True, conn=None, source_tag=None):\r\n-    print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}, source_tag: {source_tag}\")\r\n+def process_urls(all_urls, response, history, message, is_chat=True, conn=None, source_tag=None, use_ollama=False):\r\n+    print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}, source_tag: {source_tag}, use_ollama: {use_ollama}\")\r\n     documents = []\r\n     sources = []\r\n     with ThreadPoolExecutor(max_workers=5) as executor:\r\n         future_to_url = {}\r\n@@ -23,9 +24,9 @@\n                     history[-1]['content'] = response\r\n                     yield history, \"\"\r\n                 cleaned_text = stored\r\n             else:\r\n-                future = executor.submit(clean_web_content, url)\r\n+                future = executor.submit(clean_web_content, url, use_ollama=use_ollama)\r\n                 future_to_url[future] = url\r\n                 cleaned_text = None\r\n \r\n         for future in as_completed(future_to_url):\r\n"
                },
                {
                    "date": 1756933898887,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,90 +2,87 @@\n import os\r\n from config import RAW_DIR, MAX_DISPLAY_CHARS, FAISS_PATH\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n-from concurrent.futures import ThreadPoolExecutor, as_completed\r\n from urllib.parse import quote\r\n from db_utils import get_stored_content, store_content, add_chunk_if_new\r\n from vectorstore_utils import vectorstore\r\n from utils import lock\r\n from web_utils import clean_web_content\r\n+import html\r\n \r\n def process_urls(all_urls, response, history, message, is_chat=True, conn=None, source_tag=None, use_ollama=False):\r\n     print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}, source_tag: {source_tag}, use_ollama: {use_ollama}\")\r\n     documents = []\r\n     sources = []\r\n-    with ThreadPoolExecutor(max_workers=5) as executor:\r\n-        future_to_url = {}\r\n-        for url in all_urls:\r\n-            stored = get_stored_content(conn, url)\r\n-            if stored:\r\n-                response += f\"Using stored content for {url}\\n\"\r\n-                if is_chat:\r\n-                    history[-1]['content'] = response\r\n-                    yield history, \"\"\r\n-                cleaned_text = stored\r\n-            else:\r\n-                future = executor.submit(clean_web_content, url, use_ollama=use_ollama)\r\n-                future_to_url[future] = url\r\n-                cleaned_text = None\r\n-\r\n-        for future in as_completed(future_to_url):\r\n-            url = future_to_url[future]\r\n-            response += f\"Processed future for {url}\\n\"\r\n+    for url in all_urls:\r\n+        stored = get_stored_content(conn, url)\r\n+        if stored:\r\n+            response += f\"Using stored content for {url}\\n\"\r\n             if is_chat:\r\n                 history[-1]['content'] = response\r\n                 yield history, \"\"\r\n-            try:\r\n-                cleaned_text = future.result()\r\n-                if cleaned_text:\r\n-                    store_content(conn, url, cleaned_text)\r\n-            except Exception as e:\r\n-                print(f\"Exception in future result for {url}: {e}\")\r\n-                response += f\"Error processing {url}: {e}\\n\"\r\n-                if is_chat:\r\n-                    history[-1]['content'] = response\r\n-                    yield history, \"\"\r\n+            cleaned_text = stored\r\n+        else:\r\n+            response += f\"Fetching and processing {url}...\\n\"\r\n+            if is_chat:\r\n+                history[-1]['content'] = response\r\n+                yield history, \"\"\r\n+            gen = clean_web_content(url, use_ollama=use_ollama)\r\n+            cleaned_text = None\r\n+            for item in gen:\r\n+                item_type, value = item\r\n+                if item_type == \"status\":\r\n+                    response += value + \"\\n\"\r\n+                    if is_chat:\r\n+                        history[-1]['content'] = response\r\n+                        yield history, \"\"\r\n+                elif item_type == \"content\":\r\n+                    cleaned_text = value\r\n \r\n-            if cleaned_text:\r\n-                sources.append(url)\r\n-                display_text = cleaned_text[:MAX_DISPLAY_CHARS] + \"...\" if len(cleaned_text) > MAX_DISPLAY_CHARS else cleaned_text\r\n-                response += f\"Cleaned content from {url} (preview):\\n{display_text}\\n\\n\"\r\n-                if is_chat:\r\n-                    history[-1]['content'] = response\r\n-                    yield history, \"\"\r\n+        if cleaned_text:\r\n+            sources.append(url)\r\n+            store_content(conn, url, cleaned_text)\r\n \r\n-                safe_filename = quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n-                filepath = os.path.join(RAW_DIR, safe_filename)\r\n-                with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n-                    f.write(cleaned_text)\r\n+            safe_filename = quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n+            filepath = os.path.join(RAW_DIR, safe_filename)\r\n+            with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                f.write(cleaned_text)\r\n \r\n-                response += f\"[Download full raw text for {url}](/file={filepath})\\n\\n\"\r\n\\ No newline at end of file\n-                if is_chat:\r\n-                    history[-1]['content'] = response\r\n-                    yield history, \"\"\r\n+            response += f\"[Download full raw text for {url}](/file={safe_filename})\\n\\n\"\r\n+            if is_chat:\r\n+                history[-1]['content'] = response\r\n+                yield history, \"\"\r\n \r\n-                text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n-                chunks = text_splitter.split_text(cleaned_text)\r\n-                new_docs = []\r\n-                for chunk in chunks:\r\n-                    if add_chunk_if_new(conn, chunk, url, tag=source_tag):\r\n-                        metadata = {\"source\": url}\r\n-                        if source_tag:\r\n-                            metadata[\"tag\"] = source_tag\r\n-                        if 'lyrics' in message.lower():\r\n-                            metadata[\"source_type\"] = \"lyrics\"\r\n-                        new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+            # Save as HTML for viewing\r\n+            html_safe_filename = safe_filename.replace(\".txt\", \".html\")\r\n+            html_filepath = os.path.join(RAW_DIR, html_safe_filename)\r\n+            escaped_text = html.escape(cleaned_text)\r\n+            html_content = f\"\"\"\r\n+<html>\r\n+<head>\r\n+    <style>\r\n+        body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }}\r\n+        pre {{ white-space: pre-wrap; word-wrap: break-word; }}\r\n+    </style>\r\n+</head>\r\n+<body>\r\n+    <h1>Processed Text for {url}</h1>\r\n+    <pre>{escaped_text}</pre>\r\n+</body>\r\n+</html>\r\n+\"\"\"\r\n+            with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                f.write(html_content)\r\n \r\n-                if new_docs:\r\n-                    with lock:\r\n-                        vectorstore.add_documents(new_docs)\r\n-                        vectorstore.save_local(FAISS_PATH)\r\n-                    documents.extend(new_docs)\r\n+            response += f'<a href=\"/file={html_safe_filename}\" target=\"_blank\">View Processed Text for {url} in new tab</a>\\n\\n'\r\n+            if is_chat:\r\n+                history[-1]['content'] = response\r\n+                yield history, \"\"\r\n \r\n-    response += f\"Number of new document chunks added: {len(documents)}\\n\\n\"\r\n-    if is_chat:\r\n-        history[-1]['content'] = response\r\n-        yield history, \"\"\r\n+            display_text = cleaned_text[:MAX_DISPLAY_CHARS] + \"...\" if len(cleaned_text) > MAX_DISPLAY_CHARS else cleaned_text\r\n+            response += f\"Cleaned content from {url} (preview):\\n{display_text}\\n\\n\"\r\n+            if is_chat:\r\n+                history[-1]['content'] = response\r\n+                yield history, \"\"\r\n \r\n-    print(\"process_urls completed.\")\r\n-    return sources, response, history\n+            text_splitter = RecursiveCharacter\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756933905267,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -84,5 +84,29 @@\n             if is_chat:\r\n                 history[-1]['content'] = response\r\n                 yield history, \"\"\r\n \r\n-            text_splitter = RecursiveCharacter\n\\ No newline at end of file\n+            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n+            chunks = text_splitter.split_text(cleaned_text)\r\n+            new_docs = []\r\n+            for chunk in chunks:\r\n+                if add_chunk_if_new(conn, chunk, url, tag=source_tag):\r\n+                    metadata = {\"source\": url}\r\n+                    if source_tag:\r\n+                        metadata[\"tag\"] = source_tag\r\n+                    if 'lyrics' in message.lower():\r\n+                        metadata[\"source_type\"] = \"lyrics\"\r\n+                    new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+\r\n+            if new_docs:\r\n+                with lock:\r\n+                    vectorstore.add_documents(new_docs)\r\n+                    vectorstore.save_local(FAISS_PATH)\r\n+                documents.extend(new_docs)\r\n+\r\n+    response += f\"Number of new document chunks added: {len(documents)}\\n\\n\"\r\n+    if is_chat:\r\n+        history[-1]['content'] = response\r\n+        yield history, \"\"\r\n+\r\n+    print(\"process_urls completed.\")\r\n+    return sources, response, history\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756935362458,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,107 +6,91 @@\n from urllib.parse import quote\r\n from db_utils import get_stored_content, store_content, add_chunk_if_new\r\n from vectorstore_utils import vectorstore\r\n from utils import lock\r\n-from web_utils import clean_web_content\r\n import html\r\n+import requests\r\n+from bs4 import BeautifulSoup\r\n+import re\r\n+import spacy\r\n \r\n-def process_urls(all_urls, response, history, message, is_chat=True, conn=None, source_tag=None, use_ollama=False):\r\n-    print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}, source_tag: {source_tag}, use_ollama: {use_ollama}\")\r\n-    documents = []\r\n-    sources = []\r\n-    for url in all_urls:\r\n-        stored = get_stored_content(conn, url)\r\n-        if stored:\r\n-            response += f\"Using stored content for {url}\\n\"\r\n-            if is_chat:\r\n-                history[-1]['content'] = response\r\n-                yield history, \"\"\r\n-            cleaned_text = stored\r\n-        else:\r\n-            response += f\"Fetching and processing {url}...\\n\"\r\n-            if is_chat:\r\n-                history[-1]['content'] = response\r\n-                yield history, \"\"\r\n-            gen = clean_web_content(url, use_ollama=use_ollama)\r\n-            cleaned_text = None\r\n-            for item in gen:\r\n-                item_type, value = item\r\n-                if item_type == \"status\":\r\n-                    response += value + \"\\n\"\r\n-                    if is_chat:\r\n-                        history[-1]['content'] = response\r\n-                        yield history, \"\"\r\n-                elif item_type == \"content\":\r\n-                    cleaned_text = value\r\n+nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n-        if cleaned_text:\r\n-            sources.append(url)\r\n-            store_content(conn, url, cleaned_text)\r\n+def clean_web_content(url, use_ollama=False):\r\n+    yield (\"status\", f\"Fetching and cleaning URL: {url} with Ollama: {use_ollama}\")\r\n+    try:\r\n+        yield (\"status\", \"Step 1: Sending request to URL...\")\r\n+        response = requests.get(url, timeout=10)\r\n+        response.raise_for_status()\r\n+        html = response.text\r\n+        yield (\"status\", \"Step 1 completed: Response received.\")\r\n \r\n-            safe_filename = quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n-            filepath = os.path.join(RAW_DIR, safe_filename)\r\n-            with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n-                f.write(cleaned_text)\r\n+        yield (\"status\", \"Step 2: Parsing HTML with BeautifulSoup...\")\r\n+        soup = BeautifulSoup(html, 'html.parser')\r\n+        for elem in soup.select('script, style, nav, header, footer, .ad, .advert, iframe, noscript'):\r\n+            elem.extract()\r\n+        main_content = soup.find('main') or soup.find('article') or soup\r\n+        text = main_content.get_text(separator='\\n', strip=True)\r\n+        text = re.sub(r'\\s+', ' ', text)\r\n+        text = re.sub(r'[\\n\\t\\r]+', ' ', text)\r\n+        text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\r\n+        lines = [line.strip() for line in text.split('.') if len(line.strip()) > 20]\r\n+        cleaned_text = '. '.join(lines)\r\n+        yield (\"status\", f\"Step 2 completed: Cleaned text length: {len(cleaned_text)} characters.\")\r\n \r\n-            response += f\"[Download full raw text for {url}](/file={safe_filename})\\n\\n\"\r\n-            if is_chat:\r\n-                history[-1]['content'] = response\r\n-                yield history, \"\"\r\n+        # NLP Processing\r\n+        yield (\"status\", \"Step 3: Processing with NLP...\")\r\n+        doc = nlp(cleaned_text)\r\n+        processed_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.text.strip()]\r\n+        processed_text = ' '.join(processed_tokens)\r\n+        yield (\"status\", \"Step 3 completed: NLP processing done.\")\r\n \r\n-            # Save as HTML for viewing\r\n-            html_safe_filename = safe_filename.replace(\".txt\", \".html\")\r\n-            html_filepath = os.path.join(RAW_DIR, html_safe_filename)\r\n-            escaped_text = html.escape(cleaned_text)\r\n-            html_content = f\"\"\"\r\n-<html>\r\n-<head>\r\n-    <style>\r\n-        body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }}\r\n-        pre {{ white-space: pre-wrap; word-wrap: break-word; }}\r\n-    </style>\r\n-</head>\r\n-<body>\r\n-    <h1>Processed Text for {url}</h1>\r\n-    <pre>{escaped_text}</pre>\r\n-</body>\r\n-</html>\r\n-\"\"\"\r\n-            with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n-                f.write(html_content)\r\n+        # Chunking\r\n+        yield (\"status\", \"Step 4: Chunking content...\")\r\n+        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\r\n+        chunk_size = 200  # Words per chunk\r\n+        chunks = []\r\n+        current_chunk = []\r\n\\ No newline at end of file\n+        current_word_count = 0\r\n+        for sent in sentences:\r\n+            word_count = len(sent.split())\r\n+            if current_word_count + word_count > chunk_size:\r\n+                chunks.append(' '.join(current_chunk))\r\n+                current_chunk = [sent]\r\n+                current_word_count = word_count\r\n+            else:\r\n+                current_chunk.append(sent)\r\n+                current_word_count += word_count\r\n+        if current_chunk:\r\n+            chunks.append(' '.join(current_chunk))\r\n+        yield (\"status\", f\"Step 4 completed: Created {len(chunks)} chunks.\")\r\n \r\n-            response += f'<a href=\"/file={html_safe_filename}\" target=\"_blank\">View Processed Text for {url} in new tab</a>\\n\\n'\r\n-            if is_chat:\r\n-                history[-1]['content'] = response\r\n-                yield history, \"\"\r\n+        # Optional Ollama enhancement\r\n+        if use_ollama:\r\n+            yield (\"status\", \"Step 5: Enhancing chunks with Ollama...\")\r\n+            ollama_url = \"http://localhost:11434/api/generate\"\r\n+            enhanced_chunks = []\r\n+            for i, chunk in enumerate(chunks):\r\n+                yield (\"status\", f\"Enhancing chunk {i+1}/{len(chunks)}...\")\r\n+                payload = {\r\n+                    \"model\": \"qwen2.5:7b\",\r\n+                    \"prompt\": f\"Enhance and correct this web content chunk for clarity and accuracy: {chunk}. Include only the corrected text, do not include a summarization of the changes.\",\r\n+                    \"stream\": False\r\n+                }\r\n+                try:\r\n+                    resp = requests.post(ollama_url, json=payload, timeout=30)\r\n+                    if resp.status_code == 200:\r\n+                        enhanced_text = resp.json()['response']\r\n+                        enhanced_chunks.append(enhanced_text)\r\n+                        yield (\"status\", f\"Chunk {i+1} enhanced.\")\r\n+                    else:\r\n+                        yield (\"status\", f\"Error enhancing chunk {i+1}: {resp.text}\")\r\n+                        enhanced_chunks.append(chunk)\r\n+                except requests.exceptions.RequestException as e:\r\n+                    yield (\"status\", f\"Ollama request failed for chunk {i+1}: {e}. Falling back.\")\r\n+                    enhanced_chunks.append(chunk)\r\n+            processed_text = '\\n\\n'.join(enhanced_chunks)\r\n+            yield (\"status\", \"Step 5 completed: Ollama enhancement done.\")\r\n \r\n-            display_text = cleaned_text[:MAX_DISPLAY_CHARS] + \"...\" if len(cleaned_text) > MAX_DISPLAY_CHARS else cleaned_text\r\n-            response += f\"Cleaned content from {url} (preview):\\n{display_text}\\n\\n\"\r\n-            if is_chat:\r\n-                history[-1]['content'] = response\r\n-                yield history, \"\"\r\n-\r\n-            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n-            chunks = text_splitter.split_text(cleaned_text)\r\n-            new_docs = []\r\n-            for chunk in chunks:\r\n-                if add_chunk_if_new(conn, chunk, url, tag=source_tag):\r\n-                    metadata = {\"source\": url}\r\n-                    if source_tag:\r\n-                        metadata[\"tag\"] = source_tag\r\n-                    if 'lyrics' in message.lower():\r\n-                        metadata[\"source_type\"] = \"lyrics\"\r\n-                    new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n-\r\n-            if new_docs:\r\n-                with lock:\r\n-                    vectorstore.add_documents(new_docs)\r\n-                    vectorstore.save_local(FAISS_PATH)\r\n-                documents.extend(new_docs)\r\n-\r\n-    response += f\"Number of new document chunks added: {len(documents)}\\n\\n\"\r\n-    if is_chat:\r\n-        history[-1]['content'] = response\r\n-        yield history, \"\"\r\n-\r\n-    print(\"process_urls completed.\")\r\n-    return sources, response, history\n+        yield (\"content\", processed_text)\r\n+    except Exception as e:\r\n+        yield (\"status\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756935375297,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -92,5 +92,108 @@\n             yield (\"status\", \"Step 5 completed: Ollama enhancement done.\")\r\n \r\n         yield (\"content\", processed_text)\r\n     except Exception as e:\r\n-        yield (\"status\n\\ No newline at end of file\n+        yield (\"status\", f\"Error cleaning {url}: {e}\")\r\n+        yield (\"content\", None)\r\n+\r\n+def process_urls(all_urls, response, history, message, is_chat=True, conn=None, source_tag=None, use_ollama=False):\r\n+    print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}, source_tag: {source_tag}, use_ollama: {use_ollama}\")\r\n+    documents = []\r\n+    sources = []\r\n+    for url in all_urls:\r\n+        stored = get_stored_content(conn, url)\r\n+        if stored:\r\n+            response += f\"Using stored content for {url}\\n\"\r\n+            if is_chat:\r\n+                history[-1]['content'] = response\r\n+                yield history, \"\"\r\n+            cleaned_text = stored\r\n+        else:\r\n+            response += f\"Fetching and processing {url}...\\n\"\r\n+            if is_chat:\r\n+                history[-1]['content'] = response\r\n+                yield history, \"\"\r\n+            gen = clean_web_content(url, use_ollama=use_ollama)\r\n+            cleaned_text = None\r\n+            for item in gen:\r\n+                item_type, value = item\r\n+                if item_type == \"status\":\r\n+                    response += value + \"\\n\"\r\n+                    if is_chat:\r\n+                        history[-1]['content'] = response\r\n+                        yield history, \"\"\r\n+                elif item_type == \"content\":\r\n+                    cleaned_text = value\r\n+\r\n+        if cleaned_text:\r\n+            sources.append(url)\r\n+            store_content(conn, url, cleaned_text)\r\n+\r\n+            prefix = \"ollama_\" if use_ollama else \"\"\r\n+            safe_filename = prefix + quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n+            filepath = os.path.join(RAW_DIR, safe_filename)\r\n+            with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                f.write(cleaned_text)\r\n+\r\n+            response += f\"[Download full raw text for {url}](/file={safe_filename})\\n\\n\"\r\n+            if is_chat:\r\n+                history[-1]['content'] = response\r\n+                yield history, \"\"\r\n+\r\n+            # Save as HTML for viewing\r\n+            html_safe_filename = safe_filename.replace(\".txt\", \".html\")\r\n+            html_filepath = os.path.join(RAW_DIR, html_safe_filename)\r\n+            escaped_text = html.escape(cleaned_text)\r\n+            html_content = f\"\"\"\r\n+<html>\r\n+<head>\r\n+    <style>\r\n+        body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }}\r\n+        pre {{ white-space: pre-wrap; word-wrap: break-word; }}\r\n+    </style>\r\n+</head>\r\n+<body>\r\n+    <h1>Processed Text for {url}</h1>\r\n+    <pre>{escaped_text}</pre>\r\n+</body>\r\n+</html>\r\n+\"\"\"\r\n+            with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                f.write(html_content)\r\n+\r\n+            response += f'<a href=\"/file={html_safe_filename}\" target=\"_blank\">View Processed Text for {url} in new tab</a>\\n\\n'\r\n+            if is_chat:\r\n+                history[-1]['content'] = response\r\n+                yield history, \"\"\r\n+\r\n+            display_text = cleaned_text[:MAX_DISPLAY_CHARS] + \"...\" if len(cleaned_text) > MAX_DISPLAY_CHARS else cleaned_text\r\n+            response += f\"Cleaned content from {url} (preview):\\n{display_text}\\n\\n\"\r\n+            if is_chat:\r\n+                history[-1]['content'] = response\r\n+                yield history, \"\"\r\n+\r\n+            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n+            chunks = text_splitter.split_text(cleaned_text)\r\n+            new_docs = []\r\n+            for chunk in chunks:\r\n+                if add_chunk_if_new(conn, chunk, url, tag=source_tag):\r\n+                    metadata = {\"source\": url}\r\n+                    if source_tag:\r\n+                        metadata[\"tag\"] = source_tag\r\n+                    if 'lyrics' in message.lower():\r\n+                        metadata[\"source_type\"] = \"lyrics\"\r\n+                    new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+\r\n+            if new_docs:\r\n+                with lock:\r\n+                    vectorstore.add_documents(new_docs)\r\n+                    vectorstore.save_local(FAISS_PATH)\r\n+                documents.extend(new_docs)\r\n+\r\n+    response += f\"Number of new document chunks added: {len(documents)}\\n\\n\"\r\n+    if is_chat:\r\n+        history[-1]['content'] = response\r\n+        yield history, \"\"\r\n+\r\n+    print(\"process_urls completed.\")\r\n+    return sources, response, history\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756957938831,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,9 +4,9 @@\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n from urllib.parse import quote\r\n from db_utils import get_stored_content, store_content, add_chunk_if_new\r\n-from vectorstore_utils import vectorstore\r\n+from vectorstore_manager import get_vectorstore\r\n from utils import lock\r\n import html\r\n import requests\r\n from bs4 import BeautifulSoup\r\n@@ -176,19 +176,18 @@\n             chunks = text_splitter.split_text(cleaned_text)\r\n             new_docs = []\r\n             for chunk in chunks:\r\n                 if add_chunk_if_new(conn, chunk, url, tag=source_tag):\r\n-                    metadata = {\"source\": url}\r\n-                    if source_tag:\r\n-                        metadata[\"tag\"] = source_tag\r\n+                    metadata = {\"source\": url, \"tag\": source_tag}\r\n                     if 'lyrics' in message.lower():\r\n                         metadata[\"source_type\"] = \"lyrics\"\r\n                     new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n \r\n             if new_docs:\r\n                 with lock:\r\n-                    vectorstore.add_documents(new_docs)\r\n-                    vectorstore.save_local(FAISS_PATH)\r\n+                    vs = get_vectorstore(source_tag)\r\n+                    vs.add_documents(new_docs)\r\n+                    print(f\"Added {len(new_docs)} documents to vectorstore for tag {source_tag}.\")\r\n                 documents.extend(new_docs)\r\n \r\n     response += f\"Number of new document chunks added: {len(documents)}\\n\\n\"\r\n     if is_chat:\r\n"
                },
                {
                    "date": 1756958155747,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,198 @@\n+# process_utils.py\r\n+import os\r\n+from config import RAW_DIR, MAX_DISPLAY_CHARS, FAISS_PATH\r\n+from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n+from langchain_core.documents import Document\r\n+from urllib.parse import quote\r\n+from db_utils import get_stored_content, store_content, add_chunk_if_new\r\n+from vectorstore_manager import get_vectorstore\r\n+from utils import lock\r\n+import html\r\n+import requests\r\n+from bs4 import BeautifulSoup\r\n+import re\r\n+import spacy\r\n+\r\n+nlp = spacy.load(\"en_core_web_sm\")\r\n+\r\n+def clean_web_content(url, use_ollama=False):\r\n+    yield (\"status\", f\"Fetching and cleaning URL: {url} with Ollama: {use_ollama}\")\r\n+    try:\r\n+        yield (\"status\", \"Step 1: Sending request to URL...\")\r\n+        response = requests.get(url, timeout=10)\r\n+        response.raise_for_status()\r\n+        html = response.text\r\n+        yield (\"status\", \"Step 1 completed: Response received.\")\r\n+\r\n+        yield (\"status\", \"Step 2: Parsing HTML with BeautifulSoup...\")\r\n+        soup = BeautifulSoup(html, 'html.parser')\r\n+        for elem in soup.select('script, style, nav, header, footer, .ad, .advert, iframe, noscript'):\r\n+            elem.extract()\r\n+        main_content = soup.find('main') or soup.find('article') or soup\r\n+        text = main_content.get_text(separator='\\n', strip=True)\r\n+        text = re.sub(r'\\s+', ' ', text)\r\n+        text = re.sub(r'[\\n\\t\\r]+', ' ', text)\r\n+        text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\r\n+        lines = [line.strip() for line in text.split('.') if len(line.strip()) > 20]\r\n+        cleaned_text = '. '.join(lines)\r\n+        yield (\"status\", f\"Step 2 completed: Cleaned text length: {len(cleaned_text)} characters.\")\r\n+\r\n+        # NLP Processing\r\n+        yield (\"status\", \"Step 3: Processing with NLP...\")\r\n+        doc = nlp(cleaned_text)\r\n+        processed_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.text.strip()]\r\n+        processed_text = ' '.join(processed_tokens)\r\n+        yield (\"status\", \"Step 3 completed: NLP processing done.\")\r\n+\r\n+        # Chunking\r\n+        yield (\"status\", \"Step 4: Chunking content...\")\r\n+        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\r\n+        chunk_size = 200  # Words per chunk\r\n+        chunks = []\r\n+        current_chunk = []\r\n+        current_word_count = 0\r\n+        for sent in sentences:\r\n+            word_count = len(sent.split())\r\n+            if current_word_count + word_count > chunk_size:\r\n+                chunks.append(' '.join(current_chunk))\r\n+                current_chunk = [sent]\r\n+                current_word_count = word_count\r\n+            else:\r\n+                current_chunk.append(sent)\r\n+                current_word_count += word_count\r\n+        if current_chunk:\r\n+            chunks.append(' '.join(current_chunk))\r\n+        yield (\"status\", f\"Step 4 completed: Created {len(chunks)} chunks.\")\r\n+\r\n+        # Optional Ollama enhancement\r\n+        if use_ollama:\r\n+            yield (\"status\", \"Step 5: Enhancing chunks with Ollama...\")\r\n+            ollama_url = \"http://localhost:11434/api/generate\"\r\n+            enhanced_chunks = []\r\n+            for i, chunk in enumerate(chunks):\r\n+                yield (\"status\", f\"Enhancing chunk {i+1}/{len(chunks)}...\")\r\n+                payload = {\r\n+                    \"model\": \"qwen2.5:7b\",\r\n+                    \"prompt\": f\"Enhance and correct this web content chunk for clarity and accuracy: {chunk}. Include only the corrected text, do not include a summarization of the changes.\",\r\n+                    \"stream\": False\r\n+                }\r\n+                try:\r\n+                    resp = requests.post(ollama_url, json=payload, timeout=30)\r\n+                    if resp.status_code == 200:\r\n+                        enhanced_text = resp.json()['response']\r\n+                        enhanced_chunks.append(enhanced_text)\r\n+                        yield (\"status\", f\"Chunk {i+1} enhanced.\")\r\n+                    else:\r\n+                        yield (\"status\", f\"Error enhancing chunk {i+1}: {resp.text}\")\r\n+                        enhanced_chunks.append(chunk)\r\n+                except requests.exceptions.RequestException as e:\r\n+                    yield (\"status\", f\"Ollama request failed for chunk {i+1}: {e}. Falling back.\")\r\n+                    enhanced_chunks.append(chunk)\r\n+            processed_text = '\\n\\n'.join(enhanced_chunks)\r\n+            yield (\"status\", \"Step 5 completed: Ollama enhancement done.\")\r\n+\r\n+        yield (\"content\", processed_text)\r\n+    except Exception as e:\r\n+        yield (\"status\", f\"Error cleaning {url}: {e}\")\r\n+        yield (\"content\", None)\r\n+\r\n+def process_urls(all_urls, response, history, message, is_chat=True, conn=None, source_tag=None, use_ollama=False):\r\n+    print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}, source_tag: {source_tag}, use_ollama: {use_ollama}\")\r\n+    documents = []\r\n+    sources = []\r\n+    for url in all_urls:\r\n+        stored = get_stored_content(conn, url)\r\n+        if stored:\r\n+            response += f\"Using stored content for {url}\\n\"\r\n+            if is_chat:\r\n+                history[-1]['content'] = response\r\n+                yield history, \"\"\r\n+            cleaned_text = stored\r\n+        else:\r\n+            response += f\"Fetching and processing {url}...\\n\"\r\n+            if is_chat:\r\n+                history[-1]['content'] = response\r\n+                yield history, \"\"\r\n+            gen = clean_web_content(url, use_ollama=use_ollama)\r\n+            cleaned_text = None\r\n+            for item in gen:\r\n+                item_type, value = item\r\n+                if item_type == \"status\":\r\n+                    response += value + \"\\n\"\r\n+                    if is_chat:\r\n+                        history[-1]['content'] = response\r\n+                        yield history, \"\"\r\n+                elif item_type == \"content\":\r\n+                    cleaned_text = value\r\n+\r\n+        if cleaned_text:\r\n+            sources.append(url)\r\n+            store_content(conn, url, cleaned_text)\r\n+\r\n+            prefix = \"ollama_\" if use_ollama else \"\"\r\n+            safe_filename = prefix + quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n+            filepath = os.path.join(RAW_DIR, safe_filename)\r\n+            with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                f.write(cleaned_text)\r\n+\r\n+            response += f\"[Download full raw text for {url}](/file={safe_filename})\\n\\n\"\r\n+            if is_chat:\r\n+                history[-1]['content'] = response\r\n+                yield history, \"\"\r\n+\r\n+            # Save as HTML for viewing\r\n+            html_safe_filename = safe_filename.replace(\".txt\", \".html\")\r\n+            html_filepath = os.path.join(RAW_DIR, html_safe_filename)\r\n+            escaped_text = html.escape(cleaned_text)\r\n+            html_content = f\"\"\"\r\n+<html>\r\n+<head>\r\n+    <style>\r\n+        body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }}\r\n+        pre {{ white-space: pre-wrap; word-wrap: break-word; }}\r\n+    </style>\r\n+</head>\r\n+<body>\r\n+    <h1>Processed Text for {url}</h1>\r\n+    <pre>{escaped_text}</pre>\r\n+</body>\r\n+</html>\r\n+\"\"\"\r\n+            with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                f.write(html_content)\r\n+\r\n+            response += f'<a href=\"/file={html_safe_filename}\" target=\"_blank\">View Processed Text for {url} in new tab</a>\\n\\n'\r\n+            if is_chat:\r\n+                history[-1]['content'] = response\r\n+                yield history, \"\"\r\n+\r\n+            display_text = cleaned_text[:MAX_DISPLAY_CHARS] + \"...\" if len(cleaned_text) > MAX_DISPLAY_CHARS else cleaned_text\r\n+            response += f\"Cleaned content from {url} (preview):\\n{display_text}\\n\\n\"\r\n+            if is_chat:\r\n+                history[-1]['content'] = response\r\n+                yield history, \"\"\r\n+\r\n+            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n+            chunks = text_splitter.split_text(cleaned_text)\r\n+            new_docs = []\r\n+            for chunk in chunks:\r\n+                if add_chunk_if_new(conn, chunk, url, tag=source_tag):\r\n+                    metadata = {\"source\": url, \"tag\": source_tag}\r\n+                    if 'lyrics' in message.lower():\r\n+                        metadata[\"source_type\"] = \"lyrics\"\r\n+                    new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+\r\n+            if new_docs:\r\n+                with lock:\r\n+                    vs = get_vectorstore(source_tag)\r\n+                    vs.add_documents(new_docs)\r\n+                    print(f\"Added {len(new_docs)} documents to vectorstore for tag {source_tag}.\")\r\n+                documents.extend(new_docs)\r\n+\r\n+    response += f\"Number of new document chunks added: {len(documents)}\\n\\n\"\r\n+    if is_chat:\r\n+        history[-1]['content'] = response\r\n+        yield history, \"\"\r\n+\r\n+    print(\"process_urls completed.\")\r\n+    return sources, response, history\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756959799044,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -185,9 +185,13 @@\n             if new_docs:\r\n                 with lock:\r\n                     vs = get_vectorstore(source_tag)\r\n                     vs.add_documents(new_docs)\r\n-                    print(f\"Added {len(new_docs)} documents to vectorstore for tag {source_tag}.\")\r\n+                    print(f\"Debug: Added {len(new_docs)} documents to vectorstore for tag {source_tag}. ntotal after add: {vs.index.ntotal}\")\r\n+                    # Save the vectorstore to disk after adding documents\r\n+                    save_path = os.path.join(FAISS_PATH, source_tag)\r\n+                    vs.save_local(save_path)\r\n+                    print(f\"Debug: Saved vectorstore for tag {source_tag} to {save_path}.\")\r\n                 documents.extend(new_docs)\r\n \r\n     response += f\"Number of new document chunks added: {len(documents)}\\n\\n\"\r\n     if is_chat:\r\n"
                },
                {
                    "date": 1756967474289,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,21 +11,22 @@\n import requests\r\n from bs4 import BeautifulSoup\r\n import re\r\n import spacy\r\n+from augment_utils import augment_chunk  # Import for augmentation\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n def clean_web_content(url, use_ollama=False):\r\n-    yield (\"status\", f\"Fetching and cleaning URL: {url} with Ollama: {use_ollama}\")\r\n+    yield (\"status\", f\"Debug: Fetching and cleaning URL: {url} with Ollama: {use_ollama}\")\r\n     try:\r\n-        yield (\"status\", \"Step 1: Sending request to URL...\")\r\n+        yield (\"status\", \"Debug: Step 1: Sending request to URL...\")\r\n         response = requests.get(url, timeout=10)\r\n         response.raise_for_status()\r\n         html = response.text\r\n-        yield (\"status\", \"Step 1 completed: Response received.\")\r\n+        yield (\"status\", \"Debug: Step 1 completed: Response received.\")\r\n \r\n-        yield (\"status\", \"Step 2: Parsing HTML with BeautifulSoup...\")\r\n+        yield (\"status\", \"Debug: Step 2: Parsing HTML with BeautifulSoup...\")\r\n         soup = BeautifulSoup(html, 'html.parser')\r\n         for elem in soup.select('script, style, nav, header, footer, .ad, .advert, iframe, noscript'):\r\n             elem.extract()\r\n         main_content = soup.find('main') or soup.find('article') or soup\r\n@@ -34,19 +35,13 @@\n         text = re.sub(r'[\\n\\t\\r]+', ' ', text)\r\n         text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\r\n         lines = [line.strip() for line in text.split('.') if len(line.strip()) > 20]\r\n         cleaned_text = '. '.join(lines)\r\n-        yield (\"status\", f\"Step 2 completed: Cleaned text length: {len(cleaned_text)} characters.\")\r\n+        yield (\"status\", f\"Debug: Step 2 completed: Cleaned text length: {len(cleaned_text)} characters.\")\r\n \r\n-        # NLP Processing\r\n-        yield (\"status\", \"Step 3: Processing with NLP...\")\r\n-        doc = nlp(cleaned_text)\r\n-        processed_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.text.strip()]\r\n-        processed_text = ' '.join(processed_tokens)\r\n-        yield (\"status\", \"Step 3 completed: NLP processing done.\")\r\n-\r\n-        # Chunking\r\n-        yield (\"status\", \"Step 4: Chunking content...\")\r\n+        # Chunking (before augmentation)\r\n+        yield (\"status\", \"Debug: Step 3: Chunking content...\")\r\n+        doc = nlp(cleaned_text)  # Use NLP for sentence splitting\r\n         sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\r\n         chunk_size = 200  # Words per chunk\r\n         chunks = []\r\n         current_chunk = []\r\n@@ -61,44 +56,32 @@\n                 current_chunk.append(sent)\r\n                 current_word_count += word_count\r\n         if current_chunk:\r\n             chunks.append(' '.join(current_chunk))\r\n-        yield (\"status\", f\"Step 4 completed: Created {len(chunks)} chunks.\")\r\n+        yield (\"status\", f\"Debug: Step 3 completed: Created {len(chunks)} chunks.\")\r\n \r\n-        # Optional Ollama enhancement\r\n+        # Augment chunks if use_ollama is True\r\n         if use_ollama:\r\n-            yield (\"status\", \"Step 5: Enhancing chunks with Ollama...\")\r\n-            ollama_url = \"http://localhost:11434/api/generate\"\r\n-            enhanced_chunks = []\r\n+            yield (\"status\", \"Debug: Step 4: Augmenting chunks with NLP and Ollama...\")\r\n+            augmented_chunks = []\r\n             for i, chunk in enumerate(chunks):\r\n-                yield (\"status\", f\"Enhancing chunk {i+1}/{len(chunks)}...\")\r\n-                payload = {\r\n-                    \"model\": \"qwen2.5:7b\",\r\n-                    \"prompt\": f\"Enhance and correct this web content chunk for clarity and accuracy: {chunk}. Include only the corrected text, do not include a summarization of the changes.\",\r\n-                    \"stream\": False\r\n-                }\r\n-                try:\r\n-                    resp = requests.post(ollama_url, json=payload, timeout=30)\r\n-                    if resp.status_code == 200:\r\n-                        enhanced_text = resp.json()['response']\r\n-                        enhanced_chunks.append(enhanced_text)\r\n-                        yield (\"status\", f\"Chunk {i+1} enhanced.\")\r\n-                    else:\r\n-                        yield (\"status\", f\"Error enhancing chunk {i+1}: {resp.text}\")\r\n-                        enhanced_chunks.append(chunk)\r\n-                except requests.exceptions.RequestException as e:\r\n-                    yield (\"status\", f\"Ollama request failed for chunk {i+1}: {e}. Falling back.\")\r\n-                    enhanced_chunks.append(chunk)\r\n-            processed_text = '\\n\\n'.join(enhanced_chunks)\r\n-            yield (\"status\", \"Step 5 completed: Ollama enhancement done.\")\r\n+                yield (\"status\", f\"Debug: Augmenting chunk {i+1}/{len(chunks)}...\")\r\n+                augmented_text = augment_chunk(chunk)\r\n+                augmented_chunks.append(augmented_text)\r\n+            processed_text = '\\n\\n'.join(augmented_chunks)\r\n+            yield (\"status\", \"Debug: Step 4 completed: Augmentation done.\")\r\n+        else:\r\n+            # If no augmentation, just join chunks\r\n+            processed_text = '\\n\\n'.join(chunks)\r\n+            yield (\"status\", \"Debug: Step 4 skipped: No augmentation requested.\")\r\n \r\n         yield (\"content\", processed_text)\r\n     except Exception as e:\r\n-        yield (\"status\", f\"Error cleaning {url}: {e}\")\r\n+        yield (\"status\", f\"Debug: Error cleaning {url}: {e}\")\r\n         yield (\"content\", None)\r\n \r\n def process_urls(all_urls, response, history, message, is_chat=True, conn=None, source_tag=None, use_ollama=False):\r\n-    print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}, source_tag: {source_tag}, use_ollama: {use_ollama}\")\r\n+    print(f\"Debug: Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}, source_tag: {source_tag}, use_ollama: {use_ollama}\")\r\n     documents = []\r\n     sources = []\r\n     for url in all_urls:\r\n         stored = get_stored_content(conn, url)\r\n@@ -108,9 +91,9 @@\n                 history[-1]['content'] = response\r\n                 yield history, \"\"\r\n             cleaned_text = stored\r\n         else:\r\n-            response += f\"Fetching and processing {url}...\\n\"\r\n+            response += f\"Debug: Fetching and processing {url}...\\n\"\r\n             if is_chat:\r\n                 history[-1]['content'] = response\r\n                 yield history, \"\"\r\n             gen = clean_web_content(url, use_ollama=use_ollama)\r\n@@ -197,204 +180,6 @@\n     if is_chat:\r\n         history[-1]['content'] = response\r\n         yield history, \"\"\r\n \r\n-    print(\"process_urls completed.\")\r\n-    return sources, response, history\n-# process_utils.py\r\n-import os\r\n-from config import RAW_DIR, MAX_DISPLAY_CHARS, FAISS_PATH\r\n-from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n-from langchain_core.documents import Document\r\n-from urllib.parse import quote\r\n-from db_utils import get_stored_content, store_content, add_chunk_if_new\r\n-from vectorstore_manager import get_vectorstore\r\n-from utils import lock\r\n-import html\r\n-import requests\r\n-from bs4 import BeautifulSoup\r\n-import re\r\n-import spacy\r\n-\r\n-nlp = spacy.load(\"en_core_web_sm\")\r\n-\r\n-def clean_web_content(url, use_ollama=False):\r\n-    yield (\"status\", f\"Fetching and cleaning URL: {url} with Ollama: {use_ollama}\")\r\n-    try:\r\n-        yield (\"status\", \"Step 1: Sending request to URL...\")\r\n-        response = requests.get(url, timeout=10)\r\n-        response.raise_for_status()\r\n-        html = response.text\r\n-        yield (\"status\", \"Step 1 completed: Response received.\")\r\n-\r\n-        yield (\"status\", \"Step 2: Parsing HTML with BeautifulSoup...\")\r\n-        soup = BeautifulSoup(html, 'html.parser')\r\n-        for elem in soup.select('script, style, nav, header, footer, .ad, .advert, iframe, noscript'):\r\n-            elem.extract()\r\n-        main_content = soup.find('main') or soup.find('article') or soup\r\n-        text = main_content.get_text(separator='\\n', strip=True)\r\n-        text = re.sub(r'\\s+', ' ', text)\r\n-        text = re.sub(r'[\\n\\t\\r]+', ' ', text)\r\n-        text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\r\n-        lines = [line.strip() for line in text.split('.') if len(line.strip()) > 20]\r\n-        cleaned_text = '. '.join(lines)\r\n-        yield (\"status\", f\"Step 2 completed: Cleaned text length: {len(cleaned_text)} characters.\")\r\n-\r\n-        # NLP Processing\r\n-        yield (\"status\", \"Step 3: Processing with NLP...\")\r\n-        doc = nlp(cleaned_text)\r\n-        processed_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.text.strip()]\r\n-        processed_text = ' '.join(processed_tokens)\r\n-        yield (\"status\", \"Step 3 completed: NLP processing done.\")\r\n-\r\n-        # Chunking\r\n-        yield (\"status\", \"Step 4: Chunking content...\")\r\n-        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\r\n-        chunk_size = 200  # Words per chunk\r\n-        chunks = []\r\n-        current_chunk = []\r\n-        current_word_count = 0\r\n-        for sent in sentences:\r\n-            word_count = len(sent.split())\r\n-            if current_word_count + word_count > chunk_size:\r\n-                chunks.append(' '.join(current_chunk))\r\n-                current_chunk = [sent]\r\n-                current_word_count = word_count\r\n-            else:\r\n-                current_chunk.append(sent)\r\n-                current_word_count += word_count\r\n-        if current_chunk:\r\n-            chunks.append(' '.join(current_chunk))\r\n-        yield (\"status\", f\"Step 4 completed: Created {len(chunks)} chunks.\")\r\n-\r\n-        # Optional Ollama enhancement\r\n-        if use_ollama:\r\n-            yield (\"status\", \"Step 5: Enhancing chunks with Ollama...\")\r\n-            ollama_url = \"http://localhost:11434/api/generate\"\r\n-            enhanced_chunks = []\r\n-            for i, chunk in enumerate(chunks):\r\n-                yield (\"status\", f\"Enhancing chunk {i+1}/{len(chunks)}...\")\r\n-                payload = {\r\n-                    \"model\": \"qwen2.5:7b\",\r\n-                    \"prompt\": f\"Enhance and correct this web content chunk for clarity and accuracy: {chunk}. Include only the corrected text, do not include a summarization of the changes.\",\r\n-                    \"stream\": False\r\n-                }\r\n-                try:\r\n-                    resp = requests.post(ollama_url, json=payload, timeout=30)\r\n-                    if resp.status_code == 200:\r\n-                        enhanced_text = resp.json()['response']\r\n-                        enhanced_chunks.append(enhanced_text)\r\n-                        yield (\"status\", f\"Chunk {i+1} enhanced.\")\r\n-                    else:\r\n-                        yield (\"status\", f\"Error enhancing chunk {i+1}: {resp.text}\")\r\n-                        enhanced_chunks.append(chunk)\r\n-                except requests.exceptions.RequestException as e:\r\n-                    yield (\"status\", f\"Ollama request failed for chunk {i+1}: {e}. Falling back.\")\r\n-                    enhanced_chunks.append(chunk)\r\n-            processed_text = '\\n\\n'.join(enhanced_chunks)\r\n-            yield (\"status\", \"Step 5 completed: Ollama enhancement done.\")\r\n-\r\n-        yield (\"content\", processed_text)\r\n-    except Exception as e:\r\n-        yield (\"status\", f\"Error cleaning {url}: {e}\")\r\n-        yield (\"content\", None)\r\n-\r\n-def process_urls(all_urls, response, history, message, is_chat=True, conn=None, source_tag=None, use_ollama=False):\r\n-    print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}, source_tag: {source_tag}, use_ollama: {use_ollama}\")\r\n-    documents = []\r\n-    sources = []\r\n-    for url in all_urls:\r\n-        stored = get_stored_content(conn, url)\r\n-        if stored:\r\n-            response += f\"Using stored content for {url}\\n\"\r\n-            if is_chat:\r\n-                history[-1]['content'] = response\r\n-                yield history, \"\"\r\n-            cleaned_text = stored\r\n-        else:\r\n-            response += f\"Fetching and processing {url}...\\n\"\r\n-            if is_chat:\r\n-                history[-1]['content'] = response\r\n-                yield history, \"\"\r\n-            gen = clean_web_content(url, use_ollama=use_ollama)\r\n-            cleaned_text = None\r\n-            for item in gen:\r\n-                item_type, value = item\r\n-                if item_type == \"status\":\r\n-                    response += value + \"\\n\"\r\n-                    if is_chat:\r\n-                        history[-1]['content'] = response\r\n-                        yield history, \"\"\r\n-                elif item_type == \"content\":\r\n-                    cleaned_text = value\r\n-\r\n-        if cleaned_text:\r\n-            sources.append(url)\r\n-            store_content(conn, url, cleaned_text)\r\n-\r\n-            prefix = \"ollama_\" if use_ollama else \"\"\r\n-            safe_filename = prefix + quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n-            filepath = os.path.join(RAW_DIR, safe_filename)\r\n-            with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n-                f.write(cleaned_text)\r\n-\r\n-            response += f\"[Download full raw text for {url}](/file={safe_filename})\\n\\n\"\r\n-            if is_chat:\r\n-                history[-1]['content'] = response\r\n-                yield history, \"\"\r\n-\r\n-            # Save as HTML for viewing\r\n-            html_safe_filename = safe_filename.replace(\".txt\", \".html\")\r\n-            html_filepath = os.path.join(RAW_DIR, html_safe_filename)\r\n-            escaped_text = html.escape(cleaned_text)\r\n-            html_content = f\"\"\"\r\n-<html>\r\n-<head>\r\n-    <style>\r\n-        body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }}\r\n-        pre {{ white-space: pre-wrap; word-wrap: break-word; }}\r\n-    </style>\r\n-</head>\r\n-<body>\r\n-    <h1>Processed Text for {url}</h1>\r\n-    <pre>{escaped_text}</pre>\r\n-</body>\r\n-</html>\r\n-\"\"\"\r\n-            with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n-                f.write(html_content)\r\n-\r\n-            response += f'<a href=\"/file={html_safe_filename}\" target=\"_blank\">View Processed Text for {url} in new tab</a>\\n\\n'\r\n-            if is_chat:\r\n-                history[-1]['content'] = response\r\n-                yield history, \"\"\r\n-\r\n-            display_text = cleaned_text[:MAX_DISPLAY_CHARS] + \"...\" if len(cleaned_text) > MAX_DISPLAY_CHARS else cleaned_text\r\n-            response += f\"Cleaned content from {url} (preview):\\n{display_text}\\n\\n\"\r\n-            if is_chat:\r\n-                history[-1]['content'] = response\r\n-                yield history, \"\"\r\n-\r\n-            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n-            chunks = text_splitter.split_text(cleaned_text)\r\n-            new_docs = []\r\n-            for chunk in chunks:\r\n-                if add_chunk_if_new(conn, chunk, url, tag=source_tag):\r\n-                    metadata = {\"source\": url, \"tag\": source_tag}\r\n-                    if 'lyrics' in message.lower():\r\n-                        metadata[\"source_type\"] = \"lyrics\"\r\n-                    new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n-\r\n-            if new_docs:\r\n-                with lock:\r\n-                    vs = get_vectorstore(source_tag)\r\n-                    vs.add_documents(new_docs)\r\n-                    print(f\"Added {len(new_docs)} documents to vectorstore for tag {source_tag}.\")\r\n-                documents.extend(new_docs)\r\n-\r\n-    response += f\"Number of new document chunks added: {len(documents)}\\n\\n\"\r\n-    if is_chat:\r\n-        history[-1]['content'] = response\r\n-        yield history, \"\"\r\n-\r\n-    print(\"process_urls completed.\")\r\n+    print(\"Debug: process_urls completed.\")\r\n     return sources, response, history\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756967871827,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -22,9 +22,9 @@\n         yield (\"status\", \"Debug: Step 1: Sending request to URL...\")\r\n         response = requests.get(url, timeout=10)\r\n         response.raise_for_status()\r\n         html = response.text\r\n-        yield (\"status\", \"Debug: Step 1 completed: Response received.\")\r\n+        yield (\"status\", \"Debug: Step 1 completed: Response received. Raw HTML length: {len(html)}\")\r\n \r\n         yield (\"status\", \"Debug: Step 2: Parsing HTML with BeautifulSoup...\")\r\n         soup = BeautifulSoup(html, 'html.parser')\r\n         for elem in soup.select('script, style, nav, header, footer, .ad, .advert, iframe, noscript'):\r\n@@ -67,13 +67,13 @@\n                 yield (\"status\", f\"Debug: Augmenting chunk {i+1}/{len(chunks)}...\")\r\n                 augmented_text = augment_chunk(chunk)\r\n                 augmented_chunks.append(augmented_text)\r\n             processed_text = '\\n\\n'.join(augmented_chunks)\r\n-            yield (\"status\", \"Debug: Step 4 completed: Augmentation done.\")\r\n+            yield (\"status\", \"Debug: Step 4 completed: Augmentation done. Processed text length: {len(processed_text)}\")\r\n         else:\r\n             # If no augmentation, just join chunks\r\n             processed_text = '\\n\\n'.join(chunks)\r\n-            yield (\"status\", \"Debug: Step 4 skipped: No augmentation requested.\")\r\n+            yield (\"status\", \"Debug: Step 4 skipped: No augmentation requested. Processed text length: {len(processed_text)}\")\r\n \r\n         yield (\"content\", processed_text)\r\n     except Exception as e:\r\n         yield (\"status\", f\"Debug: Error cleaning {url}: {e}\")\r\n@@ -85,9 +85,9 @@\n     sources = []\r\n     for url in all_urls:\r\n         stored = get_stored_content(conn, url)\r\n         if stored:\r\n-            response += f\"Using stored content for {url}\\n\"\r\n+            response += f\"Debug: Using stored content for {url}\\n\"\r\n             if is_chat:\r\n                 history[-1]['content'] = response\r\n                 yield history, \"\"\r\n             cleaned_text = stored\r\n"
                },
                {
                    "date": 1756975360231,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -33,9 +33,10 @@\n         text = main_content.get_text(separator='\\n', strip=True)\r\n         text = re.sub(r'\\s+', ' ', text)\r\n         text = re.sub(r'[\\n\\t\\r]+', ' ', text)\r\n         text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\r\n-        lines = [line.strip() for line in text.split('.') if len(line.strip()) > 20]\r\n+        # Removed the len >20 filter to keep short lines, e.g., for lyrics\r\n+        lines = [line.strip() for line in text.split('.') if line.strip()]\r\n         cleaned_text = '. '.join(lines)\r\n         yield (\"status\", f\"Debug: Step 2 completed: Cleaned text length: {len(cleaned_text)} characters.\")\r\n \r\n         # Chunking (before augmentation)\r\n@@ -56,8 +57,10 @@\n                 current_chunk.append(sent)\r\n                 current_word_count += word_count\r\n         if current_chunk:\r\n             chunks.append(' '.join(current_chunk))\r\n+        if len(chunks) == 0:\r\n+            yield (\"status\", \"Debug: Warning: No chunks created from cleaned text.\")\r\n         yield (\"status\", f\"Debug: Step 3 completed: Created {len(chunks)} chunks.\")\r\n \r\n         # Augment chunks if use_ollama is True\r\n         if use_ollama:\r\n"
                },
                {
                    "date": 1757004920568,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,17 +28,29 @@\n         yield (\"status\", \"Debug: Step 2: Parsing HTML with BeautifulSoup...\")\r\n         soup = BeautifulSoup(html, 'html.parser')\r\n         for elem in soup.select('script, style, nav, header, footer, .ad, .advert, iframe, noscript'):\r\n             elem.extract()\r\n-        main_content = soup.find('main') or soup.find('article') or soup\r\n-        text = main_content.get_text(separator='\\n', strip=True)\r\n+\r\n+        # Special handling for lyrics sites\r\n+        if 'genius.com' in url:\r\n+            yield (\"status\", \"Debug: Detected Genius.com - using special lyrics extraction.\")\r\n+            lyrics_divs = soup.find_all('div', class_=re.compile(r'Lyrics__Container'))\r\n+            text = '\\n'.join(div.get_text(separator='\\n', strip=True) for div in lyrics_divs)\r\n+        elif 'azlyrics.com' in url:\r\n+            yield (\"status\", \"Debug: Detected AZLyrics.com - using special lyrics extraction.\")\r\n+            lyrics_div = soup.find('div', class_='ringtone').find_next_sibling('div') if soup.find('div', class_='ringtone') else soup.find('div', id='lyrics-body-text')\r\n+            text = lyrics_div.get_text(separator='\\n', strip=True) if lyrics_div else ''\r\n+        else:\r\n+            main_content = soup.find('main') or soup.find('article') or soup\r\n+            text = main_content.get_text(separator='\\n', strip=True)\r\n+\r\n         text = re.sub(r'\\s+', ' ', text)\r\n         text = re.sub(r'[\\n\\t\\r]+', ' ', text)\r\n         text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\r\n-        # Removed the len >20 filter to keep short lines, e.g., for lyrics\r\n+        # For lyrics, keep short lines\r\n         lines = [line.strip() for line in text.split('.') if line.strip()]\r\n         cleaned_text = '. '.join(lines)\r\n-        yield (\"status\", f\"Debug: Step 2 completed: Cleaned text length: {len(cleaned_text)} characters.\")\r\n+        yield (\"status\", f\"Debug: Step 2 completed: Cleaned text length: {len(cleaned_text)} characters. Preview: {cleaned_text[:200]}...\")\r\n \r\n         # Chunking (before augmentation)\r\n         yield (\"status\", \"Debug: Step 3: Chunking content...\")\r\n         doc = nlp(cleaned_text)  # Use NLP for sentence splitting\r\n@@ -59,9 +71,10 @@\n         if current_chunk:\r\n             chunks.append(' '.join(current_chunk))\r\n         if len(chunks) == 0:\r\n             yield (\"status\", \"Debug: Warning: No chunks created from cleaned text.\")\r\n-        yield (\"status\", f\"Debug: Step 3 completed: Created {len(chunks)} chunks.\")\r\n+        else:\r\n+            yield (\"status\", f\"Debug: Step 3 completed: Created {len(chunks)} chunks. First chunk preview: {chunks[0][:100]}...\")\r\n \r\n         # Augment chunks if use_ollama is True\r\n         if use_ollama:\r\n             yield (\"status\", \"Debug: Step 4: Augmenting chunks with NLP and Ollama...\")\r\n@@ -70,13 +83,13 @@\n                 yield (\"status\", f\"Debug: Augmenting chunk {i+1}/{len(chunks)}...\")\r\n                 augmented_text = augment_chunk(chunk)\r\n                 augmented_chunks.append(augmented_text)\r\n             processed_text = '\\n\\n'.join(augmented_chunks)\r\n-            yield (\"status\", \"Debug: Step 4 completed: Augmentation done. Processed text length: {len(processed_text)}\")\r\n+            yield (\"status\", f\"Debug: Step 4 completed: Augmentation done. Processed text length: {len(processed_text)}. Preview: {processed_text[:200]}...\")\r\n         else:\r\n             # If no augmentation, just join chunks\r\n             processed_text = '\\n\\n'.join(chunks)\r\n-            yield (\"status\", \"Debug: Step 4 skipped: No augmentation requested. Processed text length: {len(processed_text)}\")\r\n+            yield (\"status\", f\"Debug: Step 4 skipped: No augmentation requested. Processed text length: {len(processed_text)}. Preview: {processed_text[:200]}...\")\r\n \r\n         yield (\"content\", processed_text)\r\n     except Exception as e:\r\n         yield (\"status\", f\"Debug: Error cleaning {url}: {e}\")\r\n@@ -88,9 +101,9 @@\n     sources = []\r\n     for url in all_urls:\r\n         stored = get_stored_content(conn, url)\r\n         if stored:\r\n-            response += f\"Debug: Using stored content for {url}\\n\"\r\n+            response += f\"Debug: Using stored content for {url}. Length: {len(stored)}\\n\"\r\n             if is_chat:\r\n                 history[-1]['content'] = response\r\n                 yield history, \"\"\r\n             cleaned_text = stored\r\n@@ -157,28 +170,35 @@\n             if is_chat:\r\n                 history[-1]['content'] = response\r\n                 yield history, \"\"\r\n \r\n+            print(f\"Debug: Splitting cleaned_text for embedding. Length: {len(cleaned_text)}\")\r\n             text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n             chunks = text_splitter.split_text(cleaned_text)\r\n+            print(f\"Debug: Split into {len(chunks)} chunks for embedding.\")\r\n             new_docs = []\r\n-            for chunk in chunks:\r\n+            for i, chunk in enumerate(chunks):\r\n+                print(f\"Debug: Checking/adding chunk {i+1}/{len(chunks)}. Chunk length: {len(chunk)}\")\r\n                 if add_chunk_if_new(conn, chunk, url, tag=source_tag):\r\n                     metadata = {\"source\": url, \"tag\": source_tag}\r\n                     if 'lyrics' in message.lower():\r\n                         metadata[\"source_type\"] = \"lyrics\"\r\n                     new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+                    print(\"Debug: New chunk added to new_docs.\")\r\n \r\n             if new_docs:\r\n                 with lock:\r\n                     vs = get_vectorstore(source_tag)\r\n+                    print(f\"Debug: Adding {len(new_docs)} new documents to vectorstore.\")\r\n                     vs.add_documents(new_docs)\r\n                     print(f\"Debug: Added {len(new_docs)} documents to vectorstore for tag {source_tag}. ntotal after add: {vs.index.ntotal}\")\r\n                     # Save the vectorstore to disk after adding documents\r\n                     save_path = os.path.join(FAISS_PATH, source_tag)\r\n                     vs.save_local(save_path)\r\n                     print(f\"Debug: Saved vectorstore for tag {source_tag} to {save_path}.\")\r\n                 documents.extend(new_docs)\r\n+            else:\r\n+                print(\"Debug: No new documents added for this URL.\")\r\n \r\n     response += f\"Number of new document chunks added: {len(documents)}\\n\\n\"\r\n     if is_chat:\r\n         history[-1]['content'] = response\r\n"
                },
                {
                    "date": 1757005288799,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,29 +28,17 @@\n         yield (\"status\", \"Debug: Step 2: Parsing HTML with BeautifulSoup...\")\r\n         soup = BeautifulSoup(html, 'html.parser')\r\n         for elem in soup.select('script, style, nav, header, footer, .ad, .advert, iframe, noscript'):\r\n             elem.extract()\r\n-\r\n-        # Special handling for lyrics sites\r\n-        if 'genius.com' in url:\r\n-            yield (\"status\", \"Debug: Detected Genius.com - using special lyrics extraction.\")\r\n-            lyrics_divs = soup.find_all('div', class_=re.compile(r'Lyrics__Container'))\r\n-            text = '\\n'.join(div.get_text(separator='\\n', strip=True) for div in lyrics_divs)\r\n-        elif 'azlyrics.com' in url:\r\n-            yield (\"status\", \"Debug: Detected AZLyrics.com - using special lyrics extraction.\")\r\n-            lyrics_div = soup.find('div', class_='ringtone').find_next_sibling('div') if soup.find('div', class_='ringtone') else soup.find('div', id='lyrics-body-text')\r\n-            text = lyrics_div.get_text(separator='\\n', strip=True) if lyrics_div else ''\r\n-        else:\r\n-            main_content = soup.find('main') or soup.find('article') or soup\r\n-            text = main_content.get_text(separator='\\n', strip=True)\r\n-\r\n+        main_content = soup.find('main') or soup.find('article') or soup\r\n+        text = main_content.get_text(separator='\\n', strip=True)\r\n         text = re.sub(r'\\s+', ' ', text)\r\n         text = re.sub(r'[\\n\\t\\r]+', ' ', text)\r\n         text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\r\n-        # For lyrics, keep short lines\r\n+        # Removed the len >20 filter to keep short lines, e.g., for lyrics\r\n         lines = [line.strip() for line in text.split('.') if line.strip()]\r\n         cleaned_text = '. '.join(lines)\r\n-        yield (\"status\", f\"Debug: Step 2 completed: Cleaned text length: {len(cleaned_text)} characters. Preview: {cleaned_text[:200]}...\")\r\n+        yield (\"status\", f\"Debug: Step 2 completed: Cleaned text length: {len(cleaned_text)} characters.\")\r\n \r\n         # Chunking (before augmentation)\r\n         yield (\"status\", \"Debug: Step 3: Chunking content...\")\r\n         doc = nlp(cleaned_text)  # Use NLP for sentence splitting\r\n@@ -71,10 +59,9 @@\n         if current_chunk:\r\n             chunks.append(' '.join(current_chunk))\r\n         if len(chunks) == 0:\r\n             yield (\"status\", \"Debug: Warning: No chunks created from cleaned text.\")\r\n-        else:\r\n-            yield (\"status\", f\"Debug: Step 3 completed: Created {len(chunks)} chunks. First chunk preview: {chunks[0][:100]}...\")\r\n+        yield (\"status\", f\"Debug: Step 3 completed: Created {len(chunks)} chunks.\")\r\n \r\n         # Augment chunks if use_ollama is True\r\n         if use_ollama:\r\n             yield (\"status\", \"Debug: Step 4: Augmenting chunks with NLP and Ollama...\")\r\n@@ -83,13 +70,13 @@\n                 yield (\"status\", f\"Debug: Augmenting chunk {i+1}/{len(chunks)}...\")\r\n                 augmented_text = augment_chunk(chunk)\r\n                 augmented_chunks.append(augmented_text)\r\n             processed_text = '\\n\\n'.join(augmented_chunks)\r\n-            yield (\"status\", f\"Debug: Step 4 completed: Augmentation done. Processed text length: {len(processed_text)}. Preview: {processed_text[:200]}...\")\r\n+            yield (\"status\", \"Debug: Step 4 completed: Augmentation done. Processed text length: {len(processed_text)}\")\r\n         else:\r\n             # If no augmentation, just join chunks\r\n             processed_text = '\\n\\n'.join(chunks)\r\n-            yield (\"status\", f\"Debug: Step 4 skipped: No augmentation requested. Processed text length: {len(processed_text)}. Preview: {processed_text[:200]}...\")\r\n+            yield (\"status\", \"Debug: Step 4 skipped: No augmentation requested. Processed text length: {len(processed_text)}\")\r\n \r\n         yield (\"content\", processed_text)\r\n     except Exception as e:\r\n         yield (\"status\", f\"Debug: Error cleaning {url}: {e}\")\r\n@@ -101,9 +88,9 @@\n     sources = []\r\n     for url in all_urls:\r\n         stored = get_stored_content(conn, url)\r\n         if stored:\r\n-            response += f\"Debug: Using stored content for {url}. Length: {len(stored)}\\n\"\r\n+            response += f\"Debug: Using stored content for {url}\\n\"\r\n             if is_chat:\r\n                 history[-1]['content'] = response\r\n                 yield history, \"\"\r\n             cleaned_text = stored\r\n@@ -170,35 +157,28 @@\n             if is_chat:\r\n                 history[-1]['content'] = response\r\n                 yield history, \"\"\r\n \r\n-            print(f\"Debug: Splitting cleaned_text for embedding. Length: {len(cleaned_text)}\")\r\n             text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n             chunks = text_splitter.split_text(cleaned_text)\r\n-            print(f\"Debug: Split into {len(chunks)} chunks for embedding.\")\r\n             new_docs = []\r\n-            for i, chunk in enumerate(chunks):\r\n-                print(f\"Debug: Checking/adding chunk {i+1}/{len(chunks)}. Chunk length: {len(chunk)}\")\r\n+            for chunk in chunks:\r\n                 if add_chunk_if_new(conn, chunk, url, tag=source_tag):\r\n                     metadata = {\"source\": url, \"tag\": source_tag}\r\n                     if 'lyrics' in message.lower():\r\n                         metadata[\"source_type\"] = \"lyrics\"\r\n                     new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n-                    print(\"Debug: New chunk added to new_docs.\")\r\n \r\n             if new_docs:\r\n                 with lock:\r\n                     vs = get_vectorstore(source_tag)\r\n-                    print(f\"Debug: Adding {len(new_docs)} new documents to vectorstore.\")\r\n                     vs.add_documents(new_docs)\r\n                     print(f\"Debug: Added {len(new_docs)} documents to vectorstore for tag {source_tag}. ntotal after add: {vs.index.ntotal}\")\r\n                     # Save the vectorstore to disk after adding documents\r\n                     save_path = os.path.join(FAISS_PATH, source_tag)\r\n                     vs.save_local(save_path)\r\n                     print(f\"Debug: Saved vectorstore for tag {source_tag} to {save_path}.\")\r\n                 documents.extend(new_docs)\r\n-            else:\r\n-                print(\"Debug: No new documents added for this URL.\")\r\n \r\n     response += f\"Number of new document chunks added: {len(documents)}\\n\\n\"\r\n     if is_chat:\r\n         history[-1]['content'] = response\r\n"
                },
                {
                    "date": 1757005631645,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,9 +15,9 @@\n from augment_utils import augment_chunk  # Import for augmentation\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n-def clean_web_content(url, use_ollama=False):\r\n+def clean_web_content(url, use_ollama=False, is_lyrics=False):\r\n     yield (\"status\", f\"Debug: Fetching and cleaning URL: {url} with Ollama: {use_ollama}\")\r\n     try:\r\n         yield (\"status\", \"Debug: Step 1: Sending request to URL...\")\r\n         response = requests.get(url, timeout=10)\r\n@@ -28,18 +28,36 @@\n         yield (\"status\", \"Debug: Step 2: Parsing HTML with BeautifulSoup...\")\r\n         soup = BeautifulSoup(html, 'html.parser')\r\n         for elem in soup.select('script, style, nav, header, footer, .ad, .advert, iframe, noscript'):\r\n             elem.extract()\r\n-        main_content = soup.find('main') or soup.find('article') or soup\r\n-        text = main_content.get_text(separator='\\n', strip=True)\r\n-        text = re.sub(r'\\s+', ' ', text)\r\n-        text = re.sub(r'[\\n\\t\\r]+', ' ', text)\r\n-        text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\r\n-        # Removed the len >20 filter to keep short lines, e.g., for lyrics\r\n-        lines = [line.strip() for line in text.split('.') if line.strip()]\r\n-        cleaned_text = '. '.join(lines)\r\n-        yield (\"status\", f\"Debug: Step 2 completed: Cleaned text length: {len(cleaned_text)} characters.\")\r\n \r\n+        # Special handling for lyrics sites\r\n+        if 'genius.com' in url:\r\n+            yield (\"status\", \"Debug: Detected Genius.com - using special lyrics extraction.\")\r\n+            lyrics_divs = soup.find_all('div', class_=re.compile(r'Lyrics__Container'))\r\n+            text = '\\n'.join(div.get_text(separator='\\n', strip=True) for div in lyrics_divs)\r\n+        elif 'azlyrics.com' in url:\r\n+            yield (\"status\", \"Debug: Detected AZLyrics.com - using special lyrics extraction.\")\r\n+            lyrics_div = soup.find('div', class_='ringtone').find_next_sibling('div') if soup.find('div', class_='ringtone') else soup.find('div', id='lyrics-body-text')\r\n+            text = lyrics_div.get_text(separator='\\n', strip=True) if lyrics_div else ''\r\n+        else:\r\n+            main_content = soup.find('main') or soup.find('article') or soup\r\n+            text = main_content.get_text(separator='\\n', strip=True)\r\n+\r\n+        # Conditional cleanup based on whether it's lyrics\r\n+        if is_lyrics or 'lyrics' in url.lower():\r\n+            yield (\"status\", \"Debug: Lyrics detected - applying minimal cleanup to preserve structure.\")\r\n+            cleaned_text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)  # Only remove URLs/emails, preserve newlines and structure\r\n+        else:\r\n+            yield (\"status\", \"Debug: General content - applying standard cleanup.\")\r\n+            text = re.sub(r'\\s+', ' ', text)\r\n+            text = re.sub(r'[\\n\\t\\r]+', ' ', text)\r\n+            text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\r\n+            lines = [line.strip() for line in text.split('.') if line.strip()]\r\n+            cleaned_text = '. '.join(lines)\r\n+\r\n+        yield (\"status\", f\"Debug: Step 2 completed: Cleaned text length: {len(cleaned_text)} characters. Preview: {cleaned_text[:200]}...\")\r\n+\r\n         # Chunking (before augmentation)\r\n         yield (\"status\", \"Debug: Step 3: Chunking content...\")\r\n         doc = nlp(cleaned_text)  # Use NLP for sentence splitting\r\n         sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\r\n@@ -59,9 +77,9 @@\n         if current_chunk:\r\n             chunks.append(' '.join(current_chunk))\r\n         if len(chunks) == 0:\r\n             yield (\"status\", \"Debug: Warning: No chunks created from cleaned text.\")\r\n-        yield (\"status\", f\"Debug: Step 3 completed: Created {len(chunks)} chunks.\")\r\n+        yield (\"status\", f\"Debug: Step 3 completed: Created {len(chunks)} chunks. First chunk preview: {chunks[0][:100]}...\" if chunks else \"No chunks.\")\r\n \r\n         # Augment chunks if use_ollama is True\r\n         if use_ollama:\r\n             yield (\"status\", \"Debug: Step 4: Augmenting chunks with NLP and Ollama...\")\r\n@@ -70,21 +88,23 @@\n                 yield (\"status\", f\"Debug: Augmenting chunk {i+1}/{len(chunks)}...\")\r\n                 augmented_text = augment_chunk(chunk)\r\n                 augmented_chunks.append(augmented_text)\r\n             processed_text = '\\n\\n'.join(augmented_chunks)\r\n-            yield (\"status\", \"Debug: Step 4 completed: Augmentation done. Processed text length: {len(processed_text)}\")\r\n+            yield (\"status\", f\"Debug: Step 4 completed: Augmentation done. Processed text length: {len(processed_text)}. Preview: {processed_text[:200]}...\")\r\n         else:\r\n             # If no augmentation, just join chunks\r\n             processed_text = '\\n\\n'.join(chunks)\r\n-            yield (\"status\", \"Debug: Step 4 skipped: No augmentation requested. Processed text length: {len(processed_text)}\")\r\n+            yield (\"status\", f\"Debug: Step 4 skipped: No augmentation requested. Processed text length: {len(processed_text)}. Preview: {processed_text[:200]}...\")\r\n \r\n         yield (\"content\", processed_text)\r\n     except Exception as e:\r\n         yield (\"status\", f\"Debug: Error cleaning {url}: {e}\")\r\n         yield (\"content\", None)\r\n \r\n def process_urls(all_urls, response, history, message, is_chat=True, conn=None, source_tag=None, use_ollama=False):\r\n     print(f\"Debug: Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}, source_tag: {source_tag}, use_ollama: {use_ollama}\")\r\n+    is_lyrics = 'lyrics' in message.lower()\r\n+    print(f\"Debug: Lyrics mode: {is_lyrics} (based on 'lyrics' in query)\")\r\n     documents = []\r\n     sources = []\r\n     for url in all_urls:\r\n         stored = get_stored_content(conn, url)\r\n@@ -98,9 +118,9 @@\n             response += f\"Debug: Fetching and processing {url}...\\n\"\r\n             if is_chat:\r\n                 history[-1]['content'] = response\r\n                 yield history, \"\"\r\n-            gen = clean_web_content(url, use_ollama=use_ollama)\r\n+            gen = clean_web_content(url, use_ollama=use_ollama, is_lyrics=is_lyrics)\r\n             cleaned_text = None\r\n             for item in gen:\r\n                 item_type, value = item\r\n                 if item_type == \"status\":\r\n@@ -163,9 +183,9 @@\n             new_docs = []\r\n             for chunk in chunks:\r\n                 if add_chunk_if_new(conn, chunk, url, tag=source_tag):\r\n                     metadata = {\"source\": url, \"tag\": source_tag}\r\n-                    if 'lyrics' in message.lower():\r\n+                    if is_lyrics:\r\n                         metadata[\"source_type\"] = \"lyrics\"\r\n                     new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n \r\n             if new_docs:\r\n"
                },
                {
                    "date": 1757005893708,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,9 +15,9 @@\n from augment_utils import augment_chunk  # Import for augmentation\r\n \r\n nlp = spacy.load(\"en_core_web_sm\")\r\n \r\n-def clean_web_content(url, use_ollama=False, is_lyrics=False):\r\n+def clean_web_content(url, use_ollama=False):\r\n     yield (\"status\", f\"Debug: Fetching and cleaning URL: {url} with Ollama: {use_ollama}\")\r\n     try:\r\n         yield (\"status\", \"Debug: Step 1: Sending request to URL...\")\r\n         response = requests.get(url, timeout=10)\r\n@@ -29,9 +29,9 @@\n         soup = BeautifulSoup(html, 'html.parser')\r\n         for elem in soup.select('script, style, nav, header, footer, .ad, .advert, iframe, noscript'):\r\n             elem.extract()\r\n \r\n-        # Special handling for lyrics sites\r\n+        # Special handling for lyrics sites to preserve structure\r\n         if 'genius.com' in url:\r\n             yield (\"status\", \"Debug: Detected Genius.com - using special lyrics extraction.\")\r\n             lyrics_divs = soup.find_all('div', class_=re.compile(r'Lyrics__Container'))\r\n             text = '\\n'.join(div.get_text(separator='\\n', strip=True) for div in lyrics_divs)\r\n@@ -42,20 +42,11 @@\n         else:\r\n             main_content = soup.find('main') or soup.find('article') or soup\r\n             text = main_content.get_text(separator='\\n', strip=True)\r\n \r\n-        # Conditional cleanup based on whether it's lyrics\r\n-        if is_lyrics or 'lyrics' in url.lower():\r\n-            yield (\"status\", \"Debug: Lyrics detected - applying minimal cleanup to preserve structure.\")\r\n-            cleaned_text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)  # Only remove URLs/emails, preserve newlines and structure\r\n-        else:\r\n-            yield (\"status\", \"Debug: General content - applying standard cleanup.\")\r\n-            text = re.sub(r'\\s+', ' ', text)\r\n-            text = re.sub(r'[\\n\\t\\r]+', ' ', text)\r\n-            text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\r\n-            lines = [line.strip() for line in text.split('.') if line.strip()]\r\n-            cleaned_text = '. '.join(lines)\r\n-\r\n+        # Minimal cleanup: only normalize excessive whitespace, preserve structure\r\n+        cleaned_text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces, but keep newlines if present in extraction\r\n+        cleaned_text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', cleaned_text)  # Remove URLs/emails\r\n         yield (\"status\", f\"Debug: Step 2 completed: Cleaned text length: {len(cleaned_text)} characters. Preview: {cleaned_text[:200]}...\")\r\n \r\n         # Chunking (before augmentation)\r\n         yield (\"status\", \"Debug: Step 3: Chunking content...\")\r\n@@ -101,10 +92,8 @@\n         yield (\"content\", None)\r\n \r\n def process_urls(all_urls, response, history, message, is_chat=True, conn=None, source_tag=None, use_ollama=False):\r\n     print(f\"Debug: Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}, source_tag: {source_tag}, use_ollama: {use_ollama}\")\r\n-    is_lyrics = 'lyrics' in message.lower()\r\n-    print(f\"Debug: Lyrics mode: {is_lyrics} (based on 'lyrics' in query)\")\r\n     documents = []\r\n     sources = []\r\n     for url in all_urls:\r\n         stored = get_stored_content(conn, url)\r\n@@ -118,9 +107,9 @@\n             response += f\"Debug: Fetching and processing {url}...\\n\"\r\n             if is_chat:\r\n                 history[-1]['content'] = response\r\n                 yield history, \"\"\r\n-            gen = clean_web_content(url, use_ollama=use_ollama, is_lyrics=is_lyrics)\r\n+            gen = clean_web_content(url, use_ollama=use_ollama)\r\n             cleaned_text = None\r\n             for item in gen:\r\n                 item_type, value = item\r\n                 if item_type == \"status\":\r\n@@ -183,9 +172,9 @@\n             new_docs = []\r\n             for chunk in chunks:\r\n                 if add_chunk_if_new(conn, chunk, url, tag=source_tag):\r\n                     metadata = {\"source\": url, \"tag\": source_tag}\r\n-                    if is_lyrics:\r\n+                    if 'lyrics' in message.lower():\r\n                         metadata[\"source_type\"] = \"lyrics\"\r\n                     new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n \r\n             if new_docs:\r\n"
                },
                {
                    "date": 1757006190538,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,24 +29,27 @@\n         soup = BeautifulSoup(html, 'html.parser')\r\n         for elem in soup.select('script, style, nav, header, footer, .ad, .advert, iframe, noscript'):\r\n             elem.extract()\r\n \r\n-        # Special handling for lyrics sites to preserve structure\r\n+        # Special handling for lyrics sites to preserve full structure\r\n         if 'genius.com' in url:\r\n-            yield (\"status\", \"Debug: Detected Genius.com - using special lyrics extraction.\")\r\n+            yield (\"status\", \"Debug: Detected Genius.com - extracting full lyrics with structure preserved.\")\r\n             lyrics_divs = soup.find_all('div', class_=re.compile(r'Lyrics__Container'))\r\n-            text = '\\n'.join(div.get_text(separator='\\n', strip=True) for div in lyrics_divs)\r\n+            text = '\\n'.join([div.get_text(separator='\\n', strip=False) for div in lyrics_divs])\r\n         elif 'azlyrics.com' in url:\r\n-            yield (\"status\", \"Debug: Detected AZLyrics.com - using special lyrics extraction.\")\r\n+            yield (\"status\", \"Debug: Detected AZLyrics.com - extracting full lyrics with structure preserved.\")\r\n             lyrics_div = soup.find('div', class_='ringtone').find_next_sibling('div') if soup.find('div', class_='ringtone') else soup.find('div', id='lyrics-body-text')\r\n-            text = lyrics_div.get_text(separator='\\n', strip=True) if lyrics_div else ''\r\n+            text = lyrics_div.get_text(separator='\\n', strip=False) if lyrics_div else ''\r\n         else:\r\n+            yield (\"status\", \"Debug: General content - extracting full text with structure preserved.\")\r\n             main_content = soup.find('main') or soup.find('article') or soup\r\n-            text = main_content.get_text(separator='\\n', strip=True)\r\n+            text = main_content.get_text(separator='\\n', strip=False)\r\n \r\n-        # Minimal cleanup: only normalize excessive whitespace, preserve structure\r\n-        cleaned_text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces, but keep newlines if present in extraction\r\n+        # Minimal cleanup: only normalize excessive whitespace, preserve newlines and structure\r\n+        cleaned_text = re.sub(r'[ \\t]+', ' ', text)  # Normalize horizontal whitespace\r\n+        cleaned_text = re.sub(r'\\n\\s*\\n+', '\\n\\n', cleaned_text)  # Normalize multiple newlines to double newline\r\n         cleaned_text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', cleaned_text)  # Remove URLs/emails\r\n+        cleaned_text = cleaned_text.strip()  # Trim leading/trailing whitespace\r\n         yield (\"status\", f\"Debug: Step 2 completed: Cleaned text length: {len(cleaned_text)} characters. Preview: {cleaned_text[:200]}...\")\r\n \r\n         # Chunking (before augmentation)\r\n         yield (\"status\", \"Debug: Step 3: Chunking content...\")\r\n@@ -81,9 +84,9 @@\n                 augmented_chunks.append(augmented_text)\r\n             processed_text = '\\n\\n'.join(augmented_chunks)\r\n             yield (\"status\", f\"Debug: Step 4 completed: Augmentation done. Processed text length: {len(processed_text)}. Preview: {processed_text[:200]}...\")\r\n         else:\r\n-            # If no augmentation, just join chunks\r\n+            # If no augmentation, just join chunks with newlines to preserve structure\r\n             processed_text = '\\n\\n'.join(chunks)\r\n             yield (\"status\", f\"Debug: Step 4 skipped: No augmentation requested. Processed text length: {len(processed_text)}. Preview: {processed_text[:200]}...\")\r\n \r\n         yield (\"content\", processed_text)\r\n"
                },
                {
                    "date": 1757006617739,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -43,12 +43,11 @@\n             yield (\"status\", \"Debug: General content - extracting full text with structure preserved.\")\r\n             main_content = soup.find('main') or soup.find('article') or soup\r\n             text = main_content.get_text(separator='\\n', strip=False)\r\n \r\n-        # Minimal cleanup: only normalize excessive whitespace, preserve newlines and structure\r\n-        cleaned_text = re.sub(r'[ \\t]+', ' ', text)  # Normalize horizontal whitespace\r\n-        cleaned_text = re.sub(r'\\n\\s*\\n+', '\\n\\n', cleaned_text)  # Normalize multiple newlines to double newline\r\n-        cleaned_text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', cleaned_text)  # Remove URLs/emails\r\n+        # Minimal cleanup: normalize blank lines, remove URLs/emails, preserve all else\r\n+        cleaned_text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)  # Remove URLs/emails\r\n+        cleaned_text = re.sub(r'\\n\\s*\\n+', '\\n\\n', cleaned_text)  # Normalize multiple blank lines to double newline\r\n         cleaned_text = cleaned_text.strip()  # Trim leading/trailing whitespace\r\n         yield (\"status\", f\"Debug: Step 2 completed: Cleaned text length: {len(cleaned_text)} characters. Preview: {cleaned_text[:200]}...\")\r\n \r\n         # Chunking (before augmentation)\r\n"
                },
                {
                    "date": 1757007120965,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -43,33 +43,33 @@\n             yield (\"status\", \"Debug: General content - extracting full text with structure preserved.\")\r\n             main_content = soup.find('main') or soup.find('article') or soup\r\n             text = main_content.get_text(separator='\\n', strip=False)\r\n \r\n-        # Minimal cleanup: normalize blank lines, remove URLs/emails, preserve all else\r\n+        # Minimal cleanup: remove URLs/emails, normalize multiple blank lines, preserve all else\r\n         cleaned_text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)  # Remove URLs/emails\r\n         cleaned_text = re.sub(r'\\n\\s*\\n+', '\\n\\n', cleaned_text)  # Normalize multiple blank lines to double newline\r\n         cleaned_text = cleaned_text.strip()  # Trim leading/trailing whitespace\r\n         yield (\"status\", f\"Debug: Step 2 completed: Cleaned text length: {len(cleaned_text)} characters. Preview: {cleaned_text[:200]}...\")\r\n \r\n         # Chunking (before augmentation)\r\n         yield (\"status\", \"Debug: Step 3: Chunking content...\")\r\n         doc = nlp(cleaned_text)  # Use NLP for sentence splitting\r\n-        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\r\n+        sentences = [sent.text for sent in doc.sents]  # Keep original whitespace\r\n         chunk_size = 200  # Words per chunk\r\n         chunks = []\r\n         current_chunk = []\r\n         current_word_count = 0\r\n         for sent in sentences:\r\n             word_count = len(sent.split())\r\n             if current_word_count + word_count > chunk_size:\r\n-                chunks.append(' '.join(current_chunk))\r\n+                chunks.append(''.join(current_chunk))  # Join without extra spaces\r\n                 current_chunk = [sent]\r\n                 current_word_count = word_count\r\n             else:\r\n                 current_chunk.append(sent)\r\n                 current_word_count += word_count\r\n         if current_chunk:\r\n-            chunks.append(' '.join(current_chunk))\r\n+            chunks.append(''.join(current_chunk))\r\n         if len(chunks) == 0:\r\n             yield (\"status\", \"Debug: Warning: No chunks created from cleaned text.\")\r\n         yield (\"status\", f\"Debug: Step 3 completed: Created {len(chunks)} chunks. First chunk preview: {chunks[0][:100]}...\" if chunks else \"No chunks.\")\r\n \r\n"
                },
                {
                    "date": 1757007469669,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -43,12 +43,11 @@\n             yield (\"status\", \"Debug: General content - extracting full text with structure preserved.\")\r\n             main_content = soup.find('main') or soup.find('article') or soup\r\n             text = main_content.get_text(separator='\\n', strip=False)\r\n \r\n-        # Minimal cleanup: remove URLs/emails, normalize multiple blank lines, preserve all else\r\n-        cleaned_text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)  # Remove URLs/emails\r\n-        cleaned_text = re.sub(r'\\n\\s*\\n+', '\\n\\n', cleaned_text)  # Normalize multiple blank lines to double newline\r\n-        cleaned_text = cleaned_text.strip()  # Trim leading/trailing whitespace\r\n+        # Extremely minimal cleanup: only remove URLs/emails, preserve all whitespace and structure\r\n+        cleaned_text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)  # Remove URLs/emails only\r\n+        cleaned_text = cleaned_text.strip()  # Trim leading/trailing whitespace, but keep internal\r\n         yield (\"status\", f\"Debug: Step 2 completed: Cleaned text length: {len(cleaned_text)} characters. Preview: {cleaned_text[:200]}...\")\r\n \r\n         # Chunking (before augmentation)\r\n         yield (\"status\", \"Debug: Step 3: Chunking content...\")\r\n"
                },
                {
                    "date": 1757007625499,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -43,39 +43,30 @@\n             yield (\"status\", \"Debug: General content - extracting full text with structure preserved.\")\r\n             main_content = soup.find('main') or soup.find('article') or soup\r\n             text = main_content.get_text(separator='\\n', strip=False)\r\n \r\n-        # Extremely minimal cleanup: only remove URLs/emails, preserve all whitespace and structure\r\n-        cleaned_text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)  # Remove URLs/emails only\r\n-        cleaned_text = cleaned_text.strip()  # Trim leading/trailing whitespace, but keep internal\r\n+        # Extremely minimal cleanup: remove URLs/emails only, preserve all whitespace and structure\r\n+        cleaned_text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)  # Remove URLs/emails\r\n+        cleaned_text = cleaned_text.strip()  # Trim leading/trailing whitespace only\r\n         yield (\"status\", f\"Debug: Step 2 completed: Cleaned text length: {len(cleaned_text)} characters. Preview: {cleaned_text[:200]}...\")\r\n \r\n         # Chunking (before augmentation)\r\n         yield (\"status\", \"Debug: Step 3: Chunking content...\")\r\n-        doc = nlp(cleaned_text)  # Use NLP for sentence splitting\r\n-        sentences = [sent.text for sent in doc.sents]  # Keep original whitespace\r\n-        chunk_size = 200  # Words per chunk\r\n-        chunks = []\r\n-        current_chunk = []\r\n-        current_word_count = 0\r\n-        for sent in sentences:\r\n-            word_count = len(sent.split())\r\n-            if current_word_count + word_count > chunk_size:\r\n-                chunks.append(''.join(current_chunk))  # Join without extra spaces\r\n-                current_chunk = [sent]\r\n-                current_word_count = word_count\r\n-            else:\r\n-                current_chunk.append(sent)\r\n-                current_word_count += word_count\r\n-        if current_chunk:\r\n-            chunks.append(''.join(current_chunk))\r\n+        # Use splitter that preserves structure better\r\n+        text_splitter = RecursiveCharacterTextSplitter(\r\n+            chunk_size=500,\r\n+            chunk_overlap=100,\r\n+            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # Prioritize newlines for structure\r\n+            keep_separator=True\r\n+        )\r\n+        chunks = text_splitter.split_text(cleaned_text)\r\n         if len(chunks) == 0:\r\n             yield (\"status\", \"Debug: Warning: No chunks created from cleaned text.\")\r\n         yield (\"status\", f\"Debug: Step 3 completed: Created {len(chunks)} chunks. First chunk preview: {chunks[0][:100]}...\" if chunks else \"No chunks.\")\r\n \r\n         # Augment chunks if use_ollama is True\r\n         if use_ollama:\r\n-            yield (\"status\", \"Debug: Step 4: Augmenting chunks with NLP and Ollama...\")\r\n+            yield (\"status\", \"Debug: Step 4: Augmenting chunks with Ollama correction...\")\r\n             augmented_chunks = []\r\n             for i, chunk in enumerate(chunks):\r\n                 yield (\"status\", f\"Debug: Augmenting chunk {i+1}/{len(chunks)}...\")\r\n                 augmented_text = augment_chunk(chunk)\r\n"
                },
                {
                    "date": 1757007836308,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,12 +10,11 @@\n import html\r\n import requests\r\n from bs4 import BeautifulSoup\r\n import re\r\n-import spacy\r\n from augment_utils import augment_chunk  # Import for augmentation\r\n \r\n-nlp = spacy.load(\"en_core_web_sm\")\r\n+# Removed spaCy import and usage to avoid any potential modification during extraction\r\n \r\n def clean_web_content(url, use_ollama=False):\r\n     yield (\"status\", f\"Debug: Fetching and cleaning URL: {url} with Ollama: {use_ollama}\")\r\n     try:\r\n@@ -48,11 +47,10 @@\n         cleaned_text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)  # Remove URLs/emails\r\n         cleaned_text = cleaned_text.strip()  # Trim leading/trailing whitespace only\r\n         yield (\"status\", f\"Debug: Step 2 completed: Cleaned text length: {len(cleaned_text)} characters. Preview: {cleaned_text[:200]}...\")\r\n \r\n-        # Chunking (before augmentation)\r\n+        # Chunking (before augmentation) - use character-based splitter to preserve structure\r\n         yield (\"status\", \"Debug: Step 3: Chunking content...\")\r\n-        # Use splitter that preserves structure better\r\n         text_splitter = RecursiveCharacterTextSplitter(\r\n             chunk_size=500,\r\n             chunk_overlap=100,\r\n             separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # Prioritize newlines for structure\r\n"
                }
            ],
            "date": 1756856980340,
            "name": "Commit-0",
            "content": "from config import RAW_DIR, MAX_DISPLAY_CHARS\r\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\r\nfrom langchain_core.documents import Document\r\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\r\nfrom urllib.parse import quote\r\nfrom db_utils import get_stored_content, store_content, add_chunk_if_new\r\nfrom vectorstore_utils import vectorstore\r\nfrom utils import lock\r\nfrom web_utils import clean_web_content\r\n\r\ndef process_urls(all_urls, response, history, message, is_chat=True, conn=None):\r\n    print(f\"Starting process_urls with {len(all_urls)} URLs. is_chat: {is_chat}\")\r\n    documents = []\r\n    sources = []\r\n    with ThreadPoolExecutor(max_workers=5) as executor:\r\n        future_to_url = {}\r\n        for url in all_urls:\r\n            stored = get_stored_content(conn, url)\r\n            if stored:\r\n                response += f\"Using stored content for {url}\\n\"\r\n                if is_chat:\r\n                    history[-1]['content'] = response\r\n                    yield \"\", history\r\n                cleaned_text = stored\r\n            else:\r\n                future = executor.submit(clean_web_content, url)\r\n                future_to_url[future] = url\r\n                cleaned_text = None\r\n\r\n        for future in as_completed(future_to_url):\r\n            url = future_to_url[future]\r\n            response += f\"Processed future for {url}\\n\"\r\n            if is_chat:\r\n                history[-1]['content'] = response\r\n                yield \"\", history\r\n            try:\r\n                cleaned_text = future.result()\r\n                if cleaned_text:\r\n                    store_content(conn, url, cleaned_text)\r\n            except Exception as e:\r\n                print(f\"Exception in future result for {url}: {e}\")\r\n                response += f\"Error processing {url}: {e}\\n\"\r\n                if is_chat:\r\n                    history[-1]['content'] = response\r\n                    yield \"\", history\r\n\r\n            if cleaned_text:\r\n                sources.append(url)\r\n                display_text = cleaned_text[:MAX_DISPLAY_CHARS] + \"...\" if len(cleaned_text) > MAX_DISPLAY_CHARS else cleaned_text\r\n                response += f\"Cleaned content from {url} (preview):\\n{display_text}\\n\\n\"\r\n                if is_chat:\r\n                    history[-1]['content'] = response\r\n                    yield \"\", history\r\n\r\n                safe_filename = quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n                filepath = os.path.join(RAW_DIR, safe_filename)\r\n                with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n                    f.write(cleaned_text)\r\n\r\n                response += f\"[Download full raw text for {url}](/file={filepath})\\n\\n\"\r\n                if is_chat:\r\n                    history[-1]['content'] = response\r\n                    yield \"\", history\r\n\r\n                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\r\n                chunks = text_splitter.split_text(cleaned_text)\r\n                new_docs = []\r\n                for chunk in chunks:\r\n                    if add_chunk_if_new(conn, chunk, url):\r\n                        new_docs.append(Document(page_content=chunk, metadata={\"source\": url}))\r\n\r\n                if new_docs:\r\n                    with lock:\r\n                        vectorstore.add_documents(new_docs)\r\n                        vectorstore.save_local(FAISS_PATH)\r\n                    documents.extend(new_docs)\r\n\r\n    response += f\"Number of new document chunks added: {len(documents)}\\n\\n\"\r\n    if is_chat:\r\n        history[-1]['content'] = response\r\n        yield \"\", history\r\n\r\n    print(\"process_urls completed.\")\r\n    return sources, response, history"
        }
    ]
}