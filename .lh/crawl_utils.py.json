{
    "sourceFile": "crawl_utils.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 23,
            "patches": [
                {
                    "date": 1756856992432,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1756857264575,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,9 @@\n import threading\r\n from web_utils import search_web\r\n from process_utils import process_urls\r\n from config import MAX_URLS\r\n+import os\r\n \r\n def run_crawl(task_id, subreddit, timelimit, query, tasks, conn=None):\r\n     print(f\"Starting crawl task {task_id} for subreddit {subreddit}\")\r\n     try:\r\n"
                },
                {
                    "date": 1756857470464,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,9 +1,8 @@\n import threading\r\n from web_utils import search_web\r\n from process_utils import process_urls\r\n from config import MAX_URLS\r\n-import os\r\n \r\n def run_crawl(task_id, subreddit, timelimit, query, tasks, conn=None):\r\n     print(f\"Starting crawl task {task_id} for subreddit {subreddit}\")\r\n     try:\r\n"
                },
                {
                    "date": 1756857582036,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,9 @@\n import threading\r\n from web_utils import search_web\r\n from process_utils import process_urls\r\n from config import MAX_URLS\r\n+import os\r\n \r\n def run_crawl(task_id, subreddit, timelimit, query, tasks, conn=None):\r\n     print(f\"Starting crawl task {task_id} for subreddit {subreddit}\")\r\n     try:\r\n"
                },
                {
                    "date": 1756858516669,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,9 +1,8 @@\n import threading\r\n from web_utils import search_web\r\n from process_utils import process_urls\r\n from config import MAX_URLS\r\n-import os\r\n \r\n def run_crawl(task_id, subreddit, timelimit, query, tasks, conn=None):\r\n     print(f\"Starting crawl task {task_id} for subreddit {subreddit}\")\r\n     try:\r\n"
                },
                {
                    "date": 1756858899444,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,36 +1,39 @@\n+import os\r\n import threading\r\n from web_utils import search_web\r\n from process_utils import process_urls\r\n from config import MAX_URLS\r\n \r\n-def run_crawl(task_id, subreddit, timelimit, query, tasks, conn=None):\r\n+def run_crawl(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn=None):\r\n     print(f\"Starting crawl task {task_id} for subreddit {subreddit}\")\r\n     try:\r\n         site = f\"reddit.com/r/{subreddit}\"\r\n         timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n         urls = search_web(query, site=site, timelimit=timelimit_code)\r\n         all_urls = list(set(urls))[:MAX_URLS]\r\n \r\n+        tag = f\"{subreddit}_{query}_{timelimit}\"\r\n         history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy history to avoid index error\r\n         message = query\r\n         response = \"\"\r\n-        process_gen = process_urls(all_urls, response, history, message, is_chat=False, conn=conn)\r\n+        process_gen = process_urls(all_urls, response, history, message, is_chat=False, conn=conn, source_tag=tag)\r\n         for _ in process_gen:\r\n             pass\r\n \r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = \"Crawl completed. New content added to vectorstore if applicable.\"\r\n+        completed_crawls.append({'name': f\"{subreddit} - {query} ({timelimit})\", 'tag': tag})\r\n         print(f\"Crawl task {task_id} completed.\")\r\n     except Exception as e:\r\n         tasks[task_id]['status'] = 'error'\r\n         tasks[task_id]['message'] = str(e)\r\n         print(f\"Crawl task {task_id} error: {e}\")\r\n \r\n-def start_crawl(subreddit, timelimit, query, tasks, conn=None):\r\n+def start_crawl(subreddit, timelimit, query, tasks, completed_crawls, conn=None):\r\n     print(\"Starting new crawl...\")\r\n\\ No newline at end of file\n     task_id = len(tasks)\r\n     task = {'id': task_id, 'subreddit': subreddit, 'timelimit': timelimit, 'query': query, 'status': 'running', 'message': ''}\r\n     tasks.append(task)\r\n-    threading.Thread(target=run_crawl, args=(task_id, subreddit, timelimit, query, tasks, conn)).start()\r\n+    threading.Thread(target=run_crawl, args=(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn)).start()\r\n     print(f\"Crawl task {task_id} started.\")\r\n-    return \"Crawl started in background.\", tasks\n+    return \"Crawl started in background.\", tasks, completed_crawls\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756859281178,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,41 @@\n+import os\r\n+import threading\r\n+from web_utils import search_web\r\n+from process_utils import process_urls\r\n+from config import MAX_URLS\r\n+\r\n+def run_crawl(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn=None):\r\n+    print(f\"Starting crawl task {task_id} for subreddit {subreddit}\")\r\n+    try:\r\n+        site = f\"reddit.com/r/{subreddit}\"\r\n+        timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n+        urls = search_web(query, site=site, timelimit=timelimit_code)\r\n+        all_urls = list(set(urls))[:MAX_URLS]\r\n+\r\n+        tag = f\"{subreddit}_{query}_{timelimit}\"\r\n+        history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy history to avoid index error\r\n+        message = query\r\n+        response = \"\"\r\n+        process_gen = process_urls(all_urls, response, history, message, is_chat=False, conn=conn, source_tag=tag)\r\n+        for _ in process_gen:\r\n+            pass\r\n+\r\n+        tasks[task_id]['status'] = 'completed'\r\n+        tasks[task_id]['message'] = \"Crawl completed. New content added to vectorstore if applicable.\"\r\n+        tasks[task_id]['urls'] = all_urls\r\n+        tasks[task_id]['tag'] = tag\r\n+        completed_crawls.append({'name': f\"{subreddit} - {query} ({timelimit})\", 'tag': tag})\r\n+        print(f\"Crawl task {task_id} completed.\")\r\n+    except Exception as e:\r\n+        tasks[task_id]['status'] = 'error'\r\n+        tasks[task_id]['message'] = str(e)\r\n+        print(f\"Crawl task {task_id} error: {e}\")\r\n+\r\n+def start_crawl(subreddit, timelimit, query, tasks, completed_crawls, conn=None):\r\n+    print(\"Starting new crawl...\")\r\n+    task_id = len(tasks)\r\n+    task = {'id': task_id, 'subreddit': subreddit, 'timelimit': timelimit, 'query': query, 'status': 'running', 'message': ''}\r\n+    tasks.append(task)\r\n+    threading.Thread(target=run_crawl, args=(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn)).start()\r\n+    print(f\"Crawl task {task_id} started.\")\r\n+    return \"Crawl started in background.\", tasks, completed_crawls\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756874899714,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,49 +2,65 @@\n import threading\r\n from web_utils import search_web\r\n from process_utils import process_urls\r\n from config import MAX_URLS\r\n+import re\r\n+import requests\r\n+import json\r\n+from xml.etree import ElementTree as ET\r\n \r\n-def run_crawl(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn=None):\r\n-    print(f\"Starting crawl task {task_id} for subreddit {subreddit}\")\r\n-    try:\r\n-        site = f\"reddit.com/r/{subreddit}\"\r\n-        timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n-        urls = search_web(query, site=site, timelimit=timelimit_code)\r\n-        all_urls = list(set(urls))[:MAX_URLS]\r\n+def fetch_youtube_transcripts(all_urls, response, history, conn=None, message=None, is_chat=True):\r\n+    print(f\"Fetching YouTube transcripts for {len(all_urls)} URLs\")\r\n+    transcripts = []\r\n+    for url in all_urls:\r\n+        match = re.search(r\"v=([^&]+)\", url)\r\n+        if match:\r\n+            video_id = match.group(1)\r\n+            try:\r\n+                api_url = f\"https://www.youtube.com/api/timedtext?v={video_id}&lang=en&fmt=json3\"\r\n+                api_response = requests.get(api_url)\r\n+                print(f\"Debug: Timedtext API status: {api_response.status_code}, content: {api_response.text[:200]}\")\r\n+                if api_response.status_code == 200:\r\n+                    if api_response.text.startswith('{'):\r\n+                        json_data = api_response.json()\r\n+                        if 'events' in json_data:\r\n+                            transcript_text = \" \".join([event['segs'][0]['utf8'] for event in json_data['events'] if 'segs' in event and event['segs']])\r\n+                            transcripts.append(transcript_text)\r\n+                            response += f\"Transcript fetched for {url} (direct timedtext API - JSON)\\n\"\r\n+                        else:\r\n+                            response += f\"Debug: No 'events' in JSON for {url}.\\n\"\r\n+                    elif api_response.text.startswith('<'):\r\n+                        # Handle XML fallback\r\n+                        root = ET.fromstring(api_response.text)\r\n+                        transcript_text = \" \".join([elem.text for elem in root.findall('.//text') if elem.text])\r\n+                        transcripts.append(transcript_text)\r\n+                        response += f\"Transcript fetched for {url} (direct timedtext API - XML fallback)\\n\"\r\n+                    else:\r\n+                        response += f\"Debug: Unknown format for {url}.\\n\"\r\n+                else:\r\n+                    response += f\"Debug: Status {api_response.status_code} for {url}.\\n\"\r\n+                    response += f\"Error fetching transcript for {url}: Failed to get transcript.\\n\"\r\n+            except Exception as e:\r\n+                response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n+            if is_chat:\r\n+                history[-1][\"content\"] = response\r\n+                yield history, \"\"\r\n \r\n-        tag = f\"{subreddit}_{query}_{timelimit}\"\r\n-        history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy history to avoid index error\r\n-        message = query\r\n-        response = \"\"\r\n-        process_gen = process_urls(all_urls, response, history, message, is_chat=False, conn=conn, source_tag=tag)\r\n-        for _ in process_gen:\r\n-            pass\r\n+    combined_transcript = \" \".join(transcripts)\r\n+    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n+    chunks = text_splitter.split_text(combined_transcript)\r\n+    new_docs = []\r\n+    for chunk in chunks:\r\n+        if add_chunk_if_new(conn, chunk, \"YouTube transcripts\", tag=\"youtube_\" + message.replace(\" \", \"_\")):\r\n+            metadata = {\"source\": \"YouTube\", \"tag\": \"youtube_\" + message.replace(\" \", \"_\")}\r\n+            new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n \r\n-        tasks[task_id]['status'] = 'completed'\r\n-        tasks[task_id]['message'] = \"Crawl completed. New content added to vectorstore if applicable.\"\r\n-        tasks[task_id]['urls'] = all_urls\r\n-        tasks[task_id]['tag'] = tag\r\n-        completed_crawls.append({'name': f\"{subreddit} - {query} ({timelimit})\", 'tag': tag})\r\n-        print(f\"Crawl task {task_id} completed.\")\r\n-    except Exception as e:\r\n-        tasks[task_id]['status'] = 'error'\r\n-        tasks[task_id]['message'] = str(e)\r\n-        print(f\"Crawl task {task_id} error: {e}\")\r\n+    if new_docs:\r\n+        with lock:\r\n+            vectorstore.add_documents(new_docs)\r\n+            vectorstore.save_local(FAISS_PATH)\r\n \r\n-def start_crawl(subreddit, timelimit, query, tasks, completed_crawls, conn=None):\r\n-    print(\"Starting new crawl...\")\r\n-    task_id = len(tasks)\r\n-    task = {'id': task_id, 'subreddit': subreddit, 'timelimit': timelimit, 'query': query, 'status': 'running', 'message': ''}\r\n-    tasks.append(task)\r\n-    threading.Thread(target=run_crawl, args=(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn)).start()\r\n-    print(f\"Crawl task {task_id} started.\")\r\n-    return \"Crawl started in background.\", tasks, completed_crawls\n-import os\r\n-import threading\r\n-from web_utils import search_web\r\n-from process_utils import process_urls\r\n-from config import MAX_URLS\r\n+    return combined_transcript, response, history\r\n \r\n def run_crawl(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn=None):\r\n     print(f\"Starting crawl task {task_id} for subreddit {subreddit}\")\r\n     try:\r\n@@ -62,8 +78,10 @@\n             pass\r\n \r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = \"Crawl completed. New content added to vectorstore if applicable.\"\r\n+        tasks[task_id]['urls'] = all_urls\r\n+        tasks[task_id]['tag'] = tag\r\n         completed_crawls.append({'name': f\"{subreddit} - {query} ({timelimit})\", 'tag': tag})\r\n         print(f\"Crawl task {task_id} completed.\")\r\n     except Exception as e:\r\n         tasks[task_id]['status'] = 'error'\r\n"
                },
                {
                    "date": 1756875847316,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,63 +6,53 @@\n import re\r\n import requests\r\n import json\r\n from xml.etree import ElementTree as ET\r\n+from selenium import webdriver\r\n+from selenium.webdriver.chrome.options import Options\r\n+from selenium.webdriver.common.by import By\r\n+from selenium.webdriver.support.ui import WebDriverWait\r\n+from selenium.webdriver.support import expected_conditions as EC\r\n+import time\r\n+from bs4 import BeautifulSoup\r\n \r\n-def fetch_youtube_transcripts(all_urls, response, history, conn=None, message=None, is_chat=True):\r\n+def fetch_youtube_transcripts(all_urls, response, history, conn=None, message=None, is_chat=True, max_transcripts=5):\r\n     print(f\"Fetching YouTube transcripts for {len(all_urls)} URLs\")\r\n     transcripts = []\r\n-    for url in all_urls:\r\n-        match = re.search(r\"v=([^&]+)\", url)\r\n-        if match:\r\n-            video_id = match.group(1)\r\n+    chrome_options = Options()\r\n+    chrome_options.add_argument(\"--headless\")\r\n+    chrome_options.add_argument(\"--no-sandbox\")\r\n+    chrome_options.add_argument(\"--disable-dev-shm-usage\")\r\n+    driver = webdriver.Chrome(options=chrome_options)\r\n+    for url in all_urls[:max_transcripts]:\r\n+        try:\r\n+            driver.get(url)\r\n+            # Wait for transcript button and click\r\n             try:\r\n-                api_url = f\"https://www.youtube.com/api/timedtext?v={video_id}&lang=en&fmt=json3\"\r\n-                api_response = requests.get(api_url)\r\n-                print(f\"Debug: Timedtext API status: {api_response.status_code}, content: {api_response.text[:200]}\")\r\n-                if api_response.status_code == 200:\r\n-                    if api_response.text.startswith('{'):\r\n-                        json_data = api_response.json()\r\n-                        if 'events' in json_data:\r\n-                            transcript_text = \" \".join([event['segs'][0]['utf8'] for event in json_data['events'] if 'segs' in event and event['segs']])\r\n-                            transcripts.append(transcript_text)\r\n-                            response += f\"Transcript fetched for {url} (direct timedtext API - JSON)\\n\"\r\n-                        else:\r\n-                            response += f\"Debug: No 'events' in JSON for {url}.\\n\"\r\n-                    elif api_response.text.startswith('<'):\r\n-                        # Handle XML fallback\r\n-                        root = ET.fromstring(api_response.text)\r\n-                        transcript_text = \" \".join([elem.text for elem in root.findall('.//text') if elem.text])\r\n-                        transcripts.append(transcript_text)\r\n-                        response += f\"Transcript fetched for {url} (direct timedtext API - XML fallback)\\n\"\r\n-                    else:\r\n-                        response += f\"Debug: Unknown format for {url}.\\n\"\r\n-                else:\r\n-                    response += f\"Debug: Status {api_response.status_code} for {url}.\\n\"\r\n-                    response += f\"Error fetching transcript for {url}: Failed to get transcript.\\n\"\r\n+                transcript_button = WebDriverWait(driver, 10).until(\r\n+                    EC.element_to_be_clickable((By.CSS_SELECTOR, '[aria-label=\"Show transcript\"]'))\r\n+                )\r\n+                transcript_button.click()\r\n+                transcript_section = WebDriverWait(driver, 10).until(\r\n+                    EC.visibility_of_element_located((By.ID, \"transcript-section\"))\r\n+                )\r\n+                soup = BeautifulSoup(transcript_section.get_attribute('innerHTML'), 'html.parser')\r\n+                transcript_text = \" \".join([p.text for p in soup.find_all('p') if p.text])\r\n+                transcripts.append(transcript_text)\r\n+                response += f\"Transcript fetched for {url} (headless browser scrape)\\n\"\r\n             except Exception as e:\r\n-                response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n-            if is_chat:\r\n-                history[-1][\"content\"] = response\r\n-                yield history, \"\"\r\n+                response += f\"Debug: Headless browser scrape failed: {e}\\n\"\r\n+                response += f\"Error fetching transcript for {url}: Failed to get transcript.\\n\"\r\n+            time.sleep(2)  # Avoid rate limiting\r\n+        except Exception as e:\r\n+            response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n+        if is_chat:\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+    driver.quit()\r\n+    return transcripts, response, history\r\n \r\n-    combined_transcript = \" \".join(transcripts)\r\n-    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n-    chunks = text_splitter.split_text(combined_transcript)\r\n-    new_docs = []\r\n-    for chunk in chunks:\r\n-        if add_chunk_if_new(conn, chunk, \"YouTube transcripts\", tag=\"youtube_\" + message.replace(\" \", \"_\")):\r\n-            metadata = {\"source\": \"YouTube\", \"tag\": \"youtube_\" + message.replace(\" \", \"_\")}\r\n-            new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n-\r\n-    if new_docs:\r\n-        with lock:\r\n-            vectorstore.add_documents(new_docs)\r\n-            vectorstore.save_local(FAISS_PATH)\r\n-\r\n-    return combined_transcript, response, history\r\n-\r\n-def run_crawl(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn=None):\r\n+def run_crawl(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn=None, max_comments=50, max_threads=20):\r\n     print(f\"Starting crawl task {task_id} for subreddit {subreddit}\")\r\n     try:\r\n         site = f\"reddit.com/r/{subreddit}\"\r\n         timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n"
                },
                {
                    "date": 1756876609220,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,28 +3,27 @@\n from web_utils import search_web\r\n from process_utils import process_urls\r\n from config import MAX_URLS\r\n import re\r\n-import requests\r\n-import json\r\n-from xml.etree import ElementTree as ET\r\n from selenium import webdriver\r\n from selenium.webdriver.chrome.options import Options\r\n from selenium.webdriver.common.by import By\r\n from selenium.webdriver.support.ui import WebDriverWait\r\n from selenium.webdriver.support import expected_conditions as EC\r\n+from selenium.common.exceptions import TimeoutException\r\n import time\r\n+import requests\r\n from bs4 import BeautifulSoup\r\n \r\n-def fetch_youtube_transcripts(all_urls, response, history, conn=None, message=None, is_chat=True, max_transcripts=5):\r\n+def fetch_youtube_transcripts(all_urls, response, history, conn=None, message=None, is_chat=True, max_videos=5):\r\n     print(f\"Fetching YouTube transcripts for {len(all_urls)} URLs\")\r\n     transcripts = []\r\n     chrome_options = Options()\r\n     chrome_options.add_argument(\"--headless\")\r\n     chrome_options.add_argument(\"--no-sandbox\")\r\n     chrome_options.add_argument(\"--disable-dev-shm-usage\")\r\n     driver = webdriver.Chrome(options=chrome_options)\r\n-    for url in all_urls[:max_transcripts]:\r\n+    for url in all_urls[:max_videos]:\r\n         try:\r\n             driver.get(url)\r\n             # Wait for transcript button and click\r\n             try:\r\n@@ -38,8 +37,11 @@\n                 soup = BeautifulSoup(transcript_section.get_attribute('innerHTML'), 'html.parser')\r\n                 transcript_text = \" \".join([p.text for p in soup.find_all('p') if p.text])\r\n                 transcripts.append(transcript_text)\r\n                 response += f\"Transcript fetched for {url} (headless browser scrape)\\n\"\r\n+            except TimeoutException:\r\n+                response += f\"Debug: Timeout waiting for transcript button on {url}.\\n\"\r\n+                response += f\"Error fetching transcript for {url}: Timeout.\\n\"\r\n             except Exception as e:\r\n                 response += f\"Debug: Headless browser scrape failed: {e}\\n\"\r\n                 response += f\"Error fetching transcript for {url}: Failed to get transcript.\\n\"\r\n             time.sleep(2)  # Avoid rate limiting\r\n@@ -50,24 +52,53 @@\n             yield history, \"\"\r\n     driver.quit()\r\n     return transcripts, response, history\r\n \r\n-def run_crawl(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn=None, max_comments=50, max_threads=20):\r\n+def run_crawl(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn=None, max_threads=20, max_comments=50):\r\n     print(f\"Starting crawl task {task_id} for subreddit {subreddit}\")\r\n     try:\r\n         site = f\"reddit.com/r/{subreddit}\"\r\n         timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n         urls = search_web(query, site=site, timelimit=timelimit_code)\r\n-        all_urls = list(set(urls))[:MAX_URLS]\r\n+        all_urls = list(set(urls))[:max_threads]\r\n \r\n         tag = f\"{subreddit}_{query}_{timelimit}\"\r\n         history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy history to avoid index error\r\n         message = query\r\n         response = \"\"\r\n-        process_gen = process_urls(all_urls, response, history, message, is_chat=False, conn=conn, source_tag=tag)\r\n-        for _ in process_gen:\r\n-            pass\r\n+        # Fetch full thread content including comments\r\n+        full_content = []\r\n+        for url in all_urls:\r\n+            try:\r\n+                json_url = url + '.json'\r\n+                json_response = requests.get(json_url, headers={'User-Agent': 'Mozilla/5.0'})\r\n+                if json_response.status_code == 200:\r\n+                    data = json_response.json()\r\n+                    post_text = data[0]['data']['children'][0]['data']['selftext']\r\n+                    comments = data[1]['data']['children']\r\n+                    comment_texts = [comment['data']['body'] for comment in comments[:max_comments] if 'body' in comment['data']]\r\n+                    full_thread_text = post_text + \" \".join(comment_texts)\r\n+                    full_content.append(full_thread_text)\r\n+                    response += f\"Fetched full thread and comments for {url}\\n\"\r\n+                else:\r\n+                    response += f\"Failed to fetch JSON for {url}: Status {json_response.status_code}\\n\"\r\n+            except Exception as e:\r\n+                response += f\"Error fetching thread for {url}: {e}\\n\"\r\n \r\n+        combined_content = \" \".join(full_content)\r\n+        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n+        chunks = text_splitter.split_text(combined_content)\r\n+        new_docs = []\r\n+        for chunk in chunks:\r\n+            if add_chunk_if_new(conn, chunk, subreddit, tag=tag):\r\n+                metadata = {\"source\": subreddit, \"tag\": tag}\r\n+                new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+\r\n+        if new_docs:\r\n+            with lock:\r\n+                vectorstore.add_documents(new_docs)\r\n+                vectorstore.save_local(FAISS_PATH)\r\n+\r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = \"Crawl completed. New content added to vectorstore if applicable.\"\r\n         tasks[task_id]['urls'] = all_urls\r\n         tasks[task_id]['tag'] = tag\r\n"
                },
                {
                    "date": 1756877189811,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,16 +3,18 @@\n from web_utils import search_web\r\n from process_utils import process_urls\r\n from config import MAX_URLS\r\n import re\r\n+import requests\r\n+import json\r\n+from xml.etree import ElementTree as ET\r\n from selenium import webdriver\r\n from selenium.webdriver.chrome.options import Options\r\n from selenium.webdriver.common.by import By\r\n from selenium.webdriver.support.ui import WebDriverWait\r\n from selenium.webdriver.support import expected_conditions as EC\r\n from selenium.common.exceptions import TimeoutException\r\n import time\r\n-import requests\r\n from bs4 import BeautifulSoup\r\n \r\n def fetch_youtube_transcripts(all_urls, response, history, conn=None, message=None, is_chat=True, max_videos=5):\r\n     print(f\"Fetching YouTube transcripts for {len(all_urls)} URLs\")\r\n"
                },
                {
                    "date": 1756877342352,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,9 +1,14 @@\n import os\r\n import threading\r\n from web_utils import search_web\r\n from process_utils import process_urls\r\n-from config import MAX_URLS\r\n+from config import MAX_URLS, FAISS_PATH\r\n+from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n+from langchain_core.documents import Document\r\n+from db_utils import add_chunk_if_new\r\n+from vectorstore_utils import vectorstore\r\n+from utils import lock\r\n import re\r\n import requests\r\n import json\r\n from xml.etree import ElementTree as ET\r\n@@ -11,21 +16,21 @@\n from selenium.webdriver.chrome.options import Options\r\n from selenium.webdriver.common.by import By\r\n from selenium.webdriver.support.ui import WebDriverWait\r\n from selenium.webdriver.support import expected_conditions as EC\r\n-from selenium.common.exceptions import TimeoutException\r\n+from selenium.webdriver.common.keys import Keys\r\n import time\r\n from bs4 import BeautifulSoup\r\n \r\n-def fetch_youtube_transcripts(all_urls, response, history, conn=None, message=None, is_chat=True, max_videos=5):\r\n+def fetch_youtube_transcripts(all_urls, response, history, conn=None, message=None, is_chat=True, max_transcripts=5):\r\n     print(f\"Fetching YouTube transcripts for {len(all_urls)} URLs\")\r\n     transcripts = []\r\n     chrome_options = Options()\r\n     chrome_options.add_argument(\"--headless\")\r\n     chrome_options.add_argument(\"--no-sandbox\")\r\n     chrome_options.add_argument(\"--disable-dev-shm-usage\")\r\n     driver = webdriver.Chrome(options=chrome_options)\r\n-    for url in all_urls[:max_videos]:\r\n+    for url in all_urls[:max_transcripts]:\r\n         try:\r\n             driver.get(url)\r\n             # Wait for transcript button and click\r\n             try:\r\n"
                },
                {
                    "date": 1756877665824,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,14 +1,9 @@\n import os\r\n import threading\r\n from web_utils import search_web\r\n from process_utils import process_urls\r\n-from config import MAX_URLS, FAISS_PATH\r\n-from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n-from langchain_core.documents import Document\r\n-from db_utils import add_chunk_if_new\r\n-from vectorstore_utils import vectorstore\r\n-from utils import lock\r\n+from config import MAX_URLS\r\n import re\r\n import requests\r\n import json\r\n from xml.etree import ElementTree as ET\r\n@@ -16,9 +11,9 @@\n from selenium.webdriver.chrome.options import Options\r\n from selenium.webdriver.common.by import By\r\n from selenium.webdriver.support.ui import WebDriverWait\r\n from selenium.webdriver.support import expected_conditions as EC\r\n-from selenium.webdriver.common.keys import Keys\r\n+from selenium.common.exceptions import TimeoutException\r\n import time\r\n from bs4 import BeautifulSoup\r\n \r\n def fetch_youtube_transcripts(all_urls, response, history, conn=None, message=None, is_chat=True, max_transcripts=5):\r\n"
                },
                {
                    "date": 1756877939547,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,9 +1,14 @@\n import os\r\n import threading\r\n from web_utils import search_web\r\n from process_utils import process_urls\r\n-from config import MAX_URLS\r\n+from config import MAX_URLS, FAISS_PATH\r\n+from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n+from langchain_core.documents import Document\r\n+from db_utils import add_chunk_if_new\r\n+from vectorstore_utils import vectorstore\r\n+from utils import lock\r\n import re\r\n import requests\r\n import json\r\n from xml.etree import ElementTree as ET\r\n"
                },
                {
                    "date": 1756878813954,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,121 @@\n+import os\r\n+import threading\r\n+from web_utils import search_web\r\n+from process_utils import process_urls\r\n+from config import MAX_URLS\r\n+import re\r\n+import requests\r\n+import json\r\n+from xml.etree import ElementTree as ET\r\n+from selenium import webdriver\r\n+from selenium.webdriver.chrome.options import Options\r\n+from selenium.webdriver.common.by import By\r\n+from selenium.webdriver.support.ui import WebDriverWait\r\n+from selenium.webdriver.support import expected_conditions as EC\r\n+from selenium.common.exceptions import TimeoutException\r\n+import time\r\n+from bs4 import BeautifulSoup\r\n+\r\n+def fetch_youtube_transcripts(all_urls, response, history, conn=None, message=None, is_chat=True, max_transcripts=5):\r\n+    print(f\"Fetching YouTube transcripts for {len(all_urls)} URLs\")\r\n+    transcripts = []\r\n+    chrome_options = Options()\r\n+    chrome_options.add_argument(\"--headless\")\r\n+    chrome_options.add_argument(\"--no-sandbox\")\r\n+    chrome_options.add_argument(\"--disable-dev-shm-usage\")\r\n+    driver = webdriver.Chrome(options=chrome_options)\r\n+    for url in all_urls[:max_transcripts]:\r\n+        try:\r\n+            driver.get(url)\r\n+            # Wait for transcript button and click\r\n+            try:\r\n+                transcript_button = WebDriverWait(driver, 10).until(\r\n+                    EC.element_to_be_clickable((By.CSS_SELECTOR, '[aria-label=\"Show transcript\"]'))\r\n+                )\r\n+                transcript_button.click()\r\n+                transcript_section = WebDriverWait(driver, 10).until(\r\n+                    EC.visibility_of_element_located((By.ID, \"transcript-section\"))\r\n+                )\r\n+                soup = BeautifulSoup(transcript_section.get_attribute('innerHTML'), 'html.parser')\r\n+                transcript_text = \" \".join([p.text for p in soup.find_all('p') if p.text])\r\n+                transcripts.append(transcript_text)\r\n+                response += f\"Transcript fetched for {url} (headless browser scrape)\\n\"\r\n+            except TimeoutException:\r\n+                response += f\"Debug: Timeout waiting for transcript button on {url}.\\n\"\r\n+                response += f\"Error fetching transcript for {url}: Timeout.\\n\"\r\n+            except Exception as e:\r\n+                response += f\"Debug: Headless browser scrape failed: {e}\\n\"\r\n+                response += f\"Error fetching transcript for {url}: Failed to get transcript.\\n\"\r\n+            time.sleep(2)  # Avoid rate limiting\r\n+        except Exception as e:\r\n+            response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n+        if is_chat:\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+    driver.quit()\r\n+    return transcripts, response, history\r\n+\r\n+def run_crawl(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn=None, max_threads=20, max_comments=50):\r\n+    print(f\"Starting crawl task {task_id} for subreddit {subreddit}\")\r\n+    try:\r\n+        site = f\"reddit.com/r/{subreddit}\"\r\n+        timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n+        urls = search_web(query, site=site, timelimit=timelimit_code)\r\n+        all_urls = list(set(urls))[:max_threads]\r\n+\r\n+        tag = f\"{subreddit}_{query}_{timelimit}\"\r\n+        history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy history to avoid index error\r\n+        message = query\r\n+        response = \"\"\r\n+        # Fetch full thread content including comments\r\n+        full_content = []\r\n+        for url in all_urls:\r\n+            try:\r\n+                json_url = url + '.json'\r\n+                json_response = requests.get(json_url, headers={'User-Agent': 'Mozilla/5.0'})\r\n+                if json_response.status_code == 200:\r\n+                    data = json_response.json()\r\n+                    post_text = data[0]['data']['children'][0]['data']['selftext']\r\n+                    comments = data[1]['data']['children']\r\n+                    comment_texts = [comment['data']['body'] for comment in comments[:max_comments] if 'body' in comment['data']]\r\n+                    full_thread_text = post_text + \" \".join(comment_texts)\r\n+                    full_content.append(full_thread_text)\r\n+                    response += f\"Fetched full thread and comments for {url}\\n\"\r\n+                else:\r\n+                    response += f\"Failed to fetch JSON for {url}: Status {json_response.status_code}\\n\"\r\n+            except Exception as e:\r\n+                response += f\"Error fetching thread for {url}: {e}\\n\"\r\n+\r\n+        combined_content = \" \".join(full_content)\r\n+        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n+        chunks = text_splitter.split_text(combined_content)\r\n+        new_docs = []\r\n+        for chunk in chunks:\r\n+            if add_chunk_if_new(conn, chunk, subreddit, tag=tag):\r\n+                metadata = {\"source\": subreddit, \"tag\": tag}\r\n+                new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+\r\n+        if new_docs:\r\n+            with lock:\r\n+                vectorstore.add_documents(new_docs)\r\n+                vectorstore.save_local(FAISS_PATH)\r\n+\r\n+        tasks[task_id]['status'] = 'completed'\r\n+        tasks[task_id]['message'] = \"Crawl completed. New content added to vectorstore if applicable.\"\r\n+        tasks[task_id]['urls'] = all_urls\r\n+        tasks[task_id]['tag'] = tag\r\n+        completed_crawls.append({'name': f\"{subreddit} - {query} ({timelimit})\", 'tag': tag})\r\n+        print(f\"Crawl task {task_id} completed.\")\r\n+    except Exception as e:\r\n+        tasks[task_id]['status'] = 'error'\r\n+        tasks[task_id]['message'] = str(e)\r\n+        print(f\"Crawl task {task_id} error: {e}\")\r\n+\r\n+def start_crawl(subreddit, timelimit, query, tasks, completed_crawls, conn=None):\r\n+    print(\"Starting new crawl...\")\r\n+    task_id = len(tasks)\r\n+    task = {'id': task_id, 'subreddit': subreddit, 'timelimit': timelimit, 'query': query, 'status': 'running', 'message': ''}\r\n+    tasks.append(task)\r\n+    threading.Thread(target=run_crawl, args=(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn)).start()\r\n+    print(f\"Crawl task {task_id} started.\")\r\n+    return \"Crawl started in background.\", tasks, completed_crawls\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756878982425,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,129 +1,8 @@\n import os\r\n import threading\r\n from web_utils import search_web\r\n from process_utils import process_urls\r\n-from config import MAX_URLS\r\n-import re\r\n-import requests\r\n-import json\r\n-from xml.etree import ElementTree as ET\r\n-from selenium import webdriver\r\n-from selenium.webdriver.chrome.options import Options\r\n-from selenium.webdriver.common.by import By\r\n-from selenium.webdriver.support.ui import WebDriverWait\r\n-from selenium.webdriver.support import expected_conditions as EC\r\n-from selenium.common.exceptions import TimeoutException\r\n-import time\r\n-from bs4 import BeautifulSoup\r\n-\r\n-def fetch_youtube_transcripts(all_urls, response, history, conn=None, message=None, is_chat=True, max_transcripts=5):\r\n-    print(f\"Fetching YouTube transcripts for {len(all_urls)} URLs\")\r\n-    transcripts = []\r\n-    chrome_options = Options()\r\n-    chrome_options.add_argument(\"--headless\")\r\n-    chrome_options.add_argument(\"--no-sandbox\")\r\n-    chrome_options.add_argument(\"--disable-dev-shm-usage\")\r\n-    driver = webdriver.Chrome(options=chrome_options)\r\n-    for url in all_urls[:max_transcripts]:\r\n-        try:\r\n-            driver.get(url)\r\n-            # Wait for transcript button and click\r\n-            try:\r\n-                transcript_button = WebDriverWait(driver, 10).until(\r\n-                    EC.element_to_be_clickable((By.CSS_SELECTOR, '[aria-label=\"Show transcript\"]'))\r\n-                )\r\n-                transcript_button.click()\r\n-                transcript_section = WebDriverWait(driver, 10).until(\r\n-                    EC.visibility_of_element_located((By.ID, \"transcript-section\"))\r\n-                )\r\n-                soup = BeautifulSoup(transcript_section.get_attribute('innerHTML'), 'html.parser')\r\n-                transcript_text = \" \".join([p.text for p in soup.find_all('p') if p.text])\r\n-                transcripts.append(transcript_text)\r\n-                response += f\"Transcript fetched for {url} (headless browser scrape)\\n\"\r\n-            except TimeoutException:\r\n-                response += f\"Debug: Timeout waiting for transcript button on {url}.\\n\"\r\n-                response += f\"Error fetching transcript for {url}: Timeout.\\n\"\r\n-            except Exception as e:\r\n-                response += f\"Debug: Headless browser scrape failed: {e}\\n\"\r\n-                response += f\"Error fetching transcript for {url}: Failed to get transcript.\\n\"\r\n-            time.sleep(2)  # Avoid rate limiting\r\n-        except Exception as e:\r\n-            response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n-        if is_chat:\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-    driver.quit()\r\n-    return transcripts, response, history\r\n-\r\n-def run_crawl(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn=None, max_threads=20, max_comments=50):\r\n-    print(f\"Starting crawl task {task_id} for subreddit {subreddit}\")\r\n-    try:\r\n-        site = f\"reddit.com/r/{subreddit}\"\r\n-        timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n-        urls = search_web(query, site=site, timelimit=timelimit_code)\r\n-        all_urls = list(set(urls))[:max_threads]\r\n-\r\n-        tag = f\"{subreddit}_{query}_{timelimit}\"\r\n-        history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy history to avoid index error\r\n-        message = query\r\n-        response = \"\"\r\n-        # Fetch full thread content including comments\r\n-        full_content = []\r\n-        for url in all_urls:\r\n-            try:\r\n-                json_url = url + '.json'\r\n-                json_response = requests.get(json_url, headers={'User-Agent': 'Mozilla/5.0'})\r\n-                if json_response.status_code == 200:\r\n-                    data = json_response.json()\r\n-                    post_text = data[0]['data']['children'][0]['data']['selftext']\r\n-                    comments = data[1]['data']['children']\r\n-                    comment_texts = [comment['data']['body'] for comment in comments[:max_comments] if 'body' in comment['data']]\r\n-                    full_thread_text = post_text + \" \".join(comment_texts)\r\n-                    full_content.append(full_thread_text)\r\n-                    response += f\"Fetched full thread and comments for {url}\\n\"\r\n-                else:\r\n-                    response += f\"Failed to fetch JSON for {url}: Status {json_response.status_code}\\n\"\r\n-            except Exception as e:\r\n-                response += f\"Error fetching thread for {url}: {e}\\n\"\r\n-\r\n-        combined_content = \" \".join(full_content)\r\n-        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n-        chunks = text_splitter.split_text(combined_content)\r\n-        new_docs = []\r\n-        for chunk in chunks:\r\n-            if add_chunk_if_new(conn, chunk, subreddit, tag=tag):\r\n-                metadata = {\"source\": subreddit, \"tag\": tag}\r\n-                new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n-\r\n-        if new_docs:\r\n-            with lock:\r\n-                vectorstore.add_documents(new_docs)\r\n-                vectorstore.save_local(FAISS_PATH)\r\n-\r\n-        tasks[task_id]['status'] = 'completed'\r\n-        tasks[task_id]['message'] = \"Crawl completed. New content added to vectorstore if applicable.\"\r\n-        tasks[task_id]['urls'] = all_urls\r\n-        tasks[task_id]['tag'] = tag\r\n-        completed_crawls.append({'name': f\"{subreddit} - {query} ({timelimit})\", 'tag': tag})\r\n-        print(f\"Crawl task {task_id} completed.\")\r\n-    except Exception as e:\r\n-        tasks[task_id]['status'] = 'error'\r\n-        tasks[task_id]['message'] = str(e)\r\n-        print(f\"Crawl task {task_id} error: {e}\")\r\n-\r\n-def start_crawl(subreddit, timelimit, query, tasks, completed_crawls, conn=None):\r\n-    print(\"Starting new crawl...\")\r\n-    task_id = len(tasks)\r\n-    task = {'id': task_id, 'subreddit': subreddit, 'timelimit': timelimit, 'query': query, 'status': 'running', 'message': ''}\r\n-    tasks.append(task)\r\n-    threading.Thread(target=run_crawl, args=(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn)).start()\r\n-    print(f\"Crawl task {task_id} started.\")\r\n-    return \"Crawl started in background.\", tasks, completed_crawls\n-import os\r\n-import threading\r\n-from web_utils import search_web\r\n-from process_utils import process_urls\r\n from config import MAX_URLS, FAISS_PATH\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n from db_utils import add_chunk_if_new\r\n"
                },
                {
                    "date": 1756879313106,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,14 +1,9 @@\n import os\r\n import threading\r\n from web_utils import search_web\r\n from process_utils import process_urls\r\n-from config import MAX_URLS, FAISS_PATH\r\n-from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n-from langchain_core.documents import Document\r\n-from db_utils import add_chunk_if_new\r\n-from vectorstore_utils import vectorstore\r\n-from utils import lock\r\n+from config import MAX_URLS\r\n import re\r\n import requests\r\n import json\r\n from xml.etree import ElementTree as ET\r\n"
                },
                {
                    "date": 1756879934233,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,9 +1,14 @@\n import os\r\n import threading\r\n from web_utils import search_web\r\n from process_utils import process_urls\r\n-from config import MAX_URLS\r\n+from config import MAX_URLS, FAISS_PATH\r\n+from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n+from langchain_core.documents import Document\r\n+from db_utils import add_chunk_if_new\r\n+from vectorstore_utils import vectorstore\r\n+from utils import lock\r\n import re\r\n import requests\r\n import json\r\n from xml.etree import ElementTree as ET\r\n"
                },
                {
                    "date": 1756916522842,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,121 @@\n+import os\r\n+import threading\r\n+import subprocess\r\n+import tempfile\r\n+from web_utils import search_web\r\n+from process_utils import process_urls\r\n+from config import MAX_URLS, FAISS_PATH\r\n+from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n+from langchain_core.documents import Document\r\n+from db_utils import add_chunk_if_new\r\n+from vectorstore_utils import vectorstore\r\n+from utils import lock\r\n+import re\r\n+import requests\r\n+import json\r\n+from xml.etree import ElementTree as ET\r\n+from selenium import webdriver\r\n+from selenium.webdriver.chrome.options import Options\r\n+from selenium.webdriver.common.by import By\r\n+from selenium.webdriver.support.ui import WebDriverWait\r\n+from selenium.webdriver.support import expected_conditions as EC\r\n+from selenium.common.exceptions import TimeoutException\r\n+import time\r\n+from bs4 import BeautifulSoup\r\n+\r\n+def fetch_youtube_transcripts(all_urls, response, history, conn=None, message=None, is_chat=True, max_transcripts=5):\r\n+    print(f\"Fetching YouTube transcripts for {len(all_urls)} URLs\")\r\n+    transcripts = []\r\n+    for url in all_urls[:max_transcripts]:\r\n+        try:\r\n+            with tempfile.TemporaryDirectory() as tmpdir:\r\n+                # Use yt-dlp to download auto-subtitles (transcript) without video\r\n+                cmd = [\"yt-dlp\", \"--write-auto-sub\", \"--skip-download\", \"--sub-langs\", \"en\", \"--sub-format\", \"vtt\", \"-o\", os.path.join(tmpdir, \"transcript\"), url]\r\n+                subprocess.run(cmd, check=True, capture_output=True)\r\n+                vtt_file = os.path.join(tmpdir, \"transcript.en.vtt\")\r\n+                if os.path.exists(vtt_file):\r\n+                    with open(vtt_file, \"r\", encoding=\"utf-8\") as f:\r\n+                        vtt_content = f.read()\r\n+                    # Parse VTT to plain text\r\n+                    transcript_text = re.sub(r'^\\d+\\n\\d{2}:\\d{2}:\\d{2}\\.\\d{3} --> \\d{2}:\\d{2}:\\d{2}\\.\\d{3}\\n', '', vtt_content)\r\n+                    transcript_text = re.sub(r'\\n\\n', '\\n', transcript_text).strip()\r\n+                    transcripts.append(transcript_text)\r\n+                    response += f\"Transcript fetched for {url} (yt-dlp)\\n\"\r\n+                else:\r\n+                    response += f\"Debug: No transcript file generated for {url}.\\n\"\r\n+                    response += f\"Error fetching transcript for {url}: No transcript available.\\n\"\r\n+            time.sleep(2)  # Avoid rate limiting\r\n+        except subprocess.CalledProcessError as e:\r\n+            response += f\"Debug: yt-dlp failed: {e.stderr.decode()}\\n\"\r\n+            response += f\"Error fetching transcript for {url}: yt-dlp error.\\n\"\r\n+        except Exception as e:\r\n+            response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n+        if is_chat:\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n+    return transcripts, response, history\r\n+\r\n+def run_crawl(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn=None, max_threads=20, max_comments=50):\r\n+    print(f\"Starting crawl task {task_id} for subreddit {subreddit}\")\r\n+    try:\r\n+        site = f\"reddit.com/r/{subreddit}\"\r\n+        timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n+        urls = search_web(query, site=site, timelimit=timelimit_code)\r\n+        all_urls = list(set(urls))[:max_threads]\r\n+\r\n+        tag = f\"{subreddit}_{query}_{timelimit}\"\r\n+        history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy history to avoid index error\r\n+        message = query\r\n+        response = \"\"\r\n+        # Fetch full thread content including comments\r\n+        full_content = []\r\n+        for url in all_urls:\r\n+            try:\r\n+                json_url = url + '.json'\r\n+                json_response = requests.get(json_url, headers={'User-Agent': 'Mozilla/5.0'})\r\n+                if json_response.status_code == 200:\r\n+                    data = json_response.json()\r\n+                    post_text = data[0]['data']['children'][0]['data']['selftext']\r\n+                    comments = data[1]['data']['children']\r\n+                    comment_texts = [comment['data']['body'] for comment in comments[:max_comments] if 'body' in comment['data']]\r\n+                    full_thread_text = post_text + \" \".join(comment_texts)\r\n+                    full_content.append(full_thread_text)\r\n+                    response += f\"Fetched full thread and comments for {url}\\n\"\r\n+                else:\r\n+                    response += f\"Failed to fetch JSON for {url}: Status {json_response.status_code}\\n\"\r\n+            except Exception as e:\r\n+                response += f\"Error fetching thread for {url}: {e}\\n\"\r\n+\r\n+        combined_content = \" \".join(full_content)\r\n+        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n+        chunks = text_splitter.split_text(combined_content)\r\n+        new_docs = []\r\n+        for chunk in chunks:\r\n+            if add_chunk_if_new(conn, chunk, subreddit, tag=tag):\r\n+                metadata = {\"source\": subreddit, \"tag\": tag}\r\n+                new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+\r\n+        if new_docs:\r\n+            with lock:\r\n+                vectorstore.add_documents(new_docs)\r\n+                vectorstore.save_local(FAISS_PATH)\r\n+\r\n+        tasks[task_id]['status'] = 'completed'\r\n+        tasks[task_id]['message'] = \"Crawl completed. New content added to vectorstore if applicable.\"\r\n+        tasks[task_id]['urls'] = all_urls\r\n+        tasks[task_id]['tag'] = tag\r\n+        completed_crawls.append({'name': f\"{subreddit} - {query} ({timelimit})\", 'tag': tag})\r\n+        print(f\"Crawl task {task_id} completed.\")\r\n+    except Exception as e:\r\n+        tasks[task_id]['status'] = 'error'\r\n+        tasks[task_id]['message'] = str(e)\r\n+        print(f\"Crawl task {task_id} error: {e}\")\r\n+\r\n+def start_crawl(subreddit, timelimit, query, tasks, completed_crawls, conn=None):\r\n+    print(\"Starting new crawl...\")\r\n+    task_id = len(tasks)\r\n+    task = {'id': task_id, 'subreddit': subreddit, 'timelimit': timelimit, 'query': query, 'status': 'running', 'message': ''}\r\n+    tasks.append(task)\r\n+    threading.Thread(target=run_crawl, args=(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn)).start()\r\n+    print(f\"Crawl task {task_id} started.\")\r\n+    return \"Crawl started in background.\", tasks, completed_crawls\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756916922549,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,6 @@\n import os\r\n import threading\r\n-import subprocess\r\n-import tempfile\r\n from web_utils import search_web\r\n from process_utils import process_urls\r\n from config import MAX_URLS, FAISS_PATH\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n@@ -21,165 +19,47 @@\n from selenium.webdriver.support import expected_conditions as EC\r\n from selenium.common.exceptions import TimeoutException\r\n import time\r\n from bs4 import BeautifulSoup\r\n+import yt_dlp\r\n \r\n def fetch_youtube_transcripts(all_urls, response, history, conn=None, message=None, is_chat=True, max_transcripts=5):\r\n     print(f\"Fetching YouTube transcripts for {len(all_urls)} URLs\")\r\n     transcripts = []\r\n-    for url in all_urls[:max_transcripts]:\r\n-        try:\r\n-            with tempfile.TemporaryDirectory() as tmpdir:\r\n-                # Use yt-dlp to download auto-subtitles (transcript) without video\r\n-                cmd = [\"yt-dlp\", \"--write-auto-sub\", \"--skip-download\", \"--sub-langs\", \"en\", \"--sub-format\", \"vtt\", \"-o\", os.path.join(tmpdir, \"transcript\"), url]\r\n-                subprocess.run(cmd, check=True, capture_output=True)\r\n-                vtt_file = os.path.join(tmpdir, \"transcript.en.vtt\")\r\n+    ydl_opts = {\r\n+        'writesubtitles': True,\r\n+        'writeautomaticsub': True,\r\n+        'subtitleslangs': ['en'],\r\n+        'subtitlesformat': 'vtt',\r\n+        'skip_download': True,\r\n+        'quiet': True,\r\n+        'outtmpl': '%(id)s.%(ext)s'\r\n+    }\r\n+    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\r\n+        for url in all_urls[:max_transcripts]:\r\n+            try:\r\n+                info = ydl.extract_info(url, download=False)\r\n+                video_id = info['id']\r\n+                ydl.download([url])\r\n+                vtt_file = f\"{video_id}.en.vtt\"\r\n                 if os.path.exists(vtt_file):\r\n                     with open(vtt_file, \"r\", encoding=\"utf-8\") as f:\r\n                         vtt_content = f.read()\r\n                     # Parse VTT to plain text\r\n                     transcript_text = re.sub(r'^\\d+\\n\\d{2}:\\d{2}:\\d{2}\\.\\d{3} --> \\d{2}:\\d{2}:\\d{2}\\.\\d{3}\\n', '', vtt_content)\r\n                     transcript_text = re.sub(r'\\n\\n', '\\n', transcript_text).strip()\r\n                     transcripts.append(transcript_text)\r\n                     response += f\"Transcript fetched for {url} (yt-dlp)\\n\"\r\n+                    os.remove(vtt_file)\r\n                 else:\r\n                     response += f\"Debug: No transcript file generated for {url}.\\n\"\r\n                     response += f\"Error fetching transcript for {url}: No transcript available.\\n\"\r\n-            time.sleep(2)  # Avoid rate limiting\r\n-        except subprocess.CalledProcessError as e:\r\n-            response += f\"Debug: yt-dlp failed: {e.stderr.decode()}\\n\"\r\n-            response += f\"Error fetching transcript for {url}: yt-dlp error.\\n\"\r\n-        except Exception as e:\r\n-            response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n-        if is_chat:\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-    return transcripts, response, history\r\n-\r\n-def run_crawl(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn=None, max_threads=20, max_comments=50):\r\n-    print(f\"Starting crawl task {task_id} for subreddit {subreddit}\")\r\n-    try:\r\n-        site = f\"reddit.com/r/{subreddit}\"\r\n-        timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n-        urls = search_web(query, site=site, timelimit=timelimit_code)\r\n-        all_urls = list(set(urls))[:max_threads]\r\n-\r\n-        tag = f\"{subreddit}_{query}_{timelimit}\"\r\n-        history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy history to avoid index error\r\n-        message = query\r\n-        response = \"\"\r\n-        # Fetch full thread content including comments\r\n-        full_content = []\r\n-        for url in all_urls:\r\n-            try:\r\n-                json_url = url + '.json'\r\n-                json_response = requests.get(json_url, headers={'User-Agent': 'Mozilla/5.0'})\r\n-                if json_response.status_code == 200:\r\n-                    data = json_response.json()\r\n-                    post_text = data[0]['data']['children'][0]['data']['selftext']\r\n-                    comments = data[1]['data']['children']\r\n-                    comment_texts = [comment['data']['body'] for comment in comments[:max_comments] if 'body' in comment['data']]\r\n-                    full_thread_text = post_text + \" \".join(comment_texts)\r\n-                    full_content.append(full_thread_text)\r\n-                    response += f\"Fetched full thread and comments for {url}\\n\"\r\n-                else:\r\n-                    response += f\"Failed to fetch JSON for {url}: Status {json_response.status_code}\\n\"\r\n+                time.sleep(2)  # Avoid rate limiting\r\n             except Exception as e:\r\n-                response += f\"Error fetching thread for {url}: {e}\\n\"\r\n-\r\n-        combined_content = \" \".join(full_content)\r\n-        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n-        chunks = text_splitter.split_text(combined_content)\r\n-        new_docs = []\r\n-        for chunk in chunks:\r\n-            if add_chunk_if_new(conn, chunk, subreddit, tag=tag):\r\n-                metadata = {\"source\": subreddit, \"tag\": tag}\r\n-                new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n-\r\n-        if new_docs:\r\n-            with lock:\r\n-                vectorstore.add_documents(new_docs)\r\n-                vectorstore.save_local(FAISS_PATH)\r\n-\r\n-        tasks[task_id]['status'] = 'completed'\r\n-        tasks[task_id]['message'] = \"Crawl completed. New content added to vectorstore if applicable.\"\r\n-        tasks[task_id]['urls'] = all_urls\r\n-        tasks[task_id]['tag'] = tag\r\n-        completed_crawls.append({'name': f\"{subreddit} - {query} ({timelimit})\", 'tag': tag})\r\n-        print(f\"Crawl task {task_id} completed.\")\r\n-    except Exception as e:\r\n-        tasks[task_id]['status'] = 'error'\r\n-        tasks[task_id]['message'] = str(e)\r\n-        print(f\"Crawl task {task_id} error: {e}\")\r\n-\r\n-def start_crawl(subreddit, timelimit, query, tasks, completed_crawls, conn=None):\r\n-    print(\"Starting new crawl...\")\r\n-    task_id = len(tasks)\r\n-    task = {'id': task_id, 'subreddit': subreddit, 'timelimit': timelimit, 'query': query, 'status': 'running', 'message': ''}\r\n-    tasks.append(task)\r\n-    threading.Thread(target=run_crawl, args=(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn)).start()\r\n-    print(f\"Crawl task {task_id} started.\")\r\n-    return \"Crawl started in background.\", tasks, completed_crawls\n-import os\r\n-import threading\r\n-from web_utils import search_web\r\n-from process_utils import process_urls\r\n-from config import MAX_URLS, FAISS_PATH\r\n-from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n-from langchain_core.documents import Document\r\n-from db_utils import add_chunk_if_new\r\n-from vectorstore_utils import vectorstore\r\n-from utils import lock\r\n-import re\r\n-import requests\r\n-import json\r\n-from xml.etree import ElementTree as ET\r\n-from selenium import webdriver\r\n-from selenium.webdriver.chrome.options import Options\r\n-from selenium.webdriver.common.by import By\r\n-from selenium.webdriver.support.ui import WebDriverWait\r\n-from selenium.webdriver.support import expected_conditions as EC\r\n-from selenium.common.exceptions import TimeoutException\r\n-import time\r\n-from bs4 import BeautifulSoup\r\n-\r\n-def fetch_youtube_transcripts(all_urls, response, history, conn=None, message=None, is_chat=True, max_transcripts=5):\r\n-    print(f\"Fetching YouTube transcripts for {len(all_urls)} URLs\")\r\n-    transcripts = []\r\n-    chrome_options = Options()\r\n-    chrome_options.add_argument(\"--headless\")\r\n-    chrome_options.add_argument(\"--no-sandbox\")\r\n-    chrome_options.add_argument(\"--disable-dev-shm-usage\")\r\n-    driver = webdriver.Chrome(options=chrome_options)\r\n-    for url in all_urls[:max_transcripts]:\r\n-        try:\r\n-            driver.get(url)\r\n-            # Wait for transcript button and click\r\n-            try:\r\n-                transcript_button = WebDriverWait(driver, 10).until(\r\n-                    EC.element_to_be_clickable((By.CSS_SELECTOR, '[aria-label=\"Show transcript\"]'))\r\n-                )\r\n-                transcript_button.click()\r\n-                transcript_section = WebDriverWait(driver, 10).until(\r\n-                    EC.visibility_of_element_located((By.ID, \"transcript-section\"))\r\n-                )\r\n-                soup = BeautifulSoup(transcript_section.get_attribute('innerHTML'), 'html.parser')\r\n-                transcript_text = \" \".join([p.text for p in soup.find_all('p') if p.text])\r\n-                transcripts.append(transcript_text)\r\n-                response += f\"Transcript fetched for {url} (headless browser scrape)\\n\"\r\n-            except TimeoutException:\r\n-                response += f\"Debug: Timeout waiting for transcript button on {url}.\\n\"\r\n-                response += f\"Error fetching transcript for {url}: Timeout.\\n\"\r\n-            except Exception as e:\r\n-                response += f\"Debug: Headless browser scrape failed: {e}\\n\"\r\n-                response += f\"Error fetching transcript for {url}: Failed to get transcript.\\n\"\r\n-            time.sleep(2)  # Avoid rate limiting\r\n-        except Exception as e:\r\n-            response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n-        if is_chat:\r\n-            history[-1][\"content\"] = response\r\n-            yield history, \"\"\r\n-    driver.quit()\r\n+                response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n+            if is_chat:\r\n+                history[-1][\"content\"] = response\r\n+                yield history, \"\"\r\n     return transcripts, response, history\r\n \r\n def run_crawl(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn=None, max_threads=20, max_comments=50):\r\n     print(f\"Starting crawl task {task_id} for subreddit {subreddit}\")\r\n"
                },
                {
                    "date": 1756917341347,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,6 +1,8 @@\n import os\r\n import threading\r\n+import yt_dlp\r\n+import tempfile\r\n from web_utils import search_web\r\n from process_utils import process_urls\r\n from config import MAX_URLS, FAISS_PATH\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n@@ -19,47 +21,46 @@\n from selenium.webdriver.support import expected_conditions as EC\r\n from selenium.common.exceptions import TimeoutException\r\n import time\r\n from bs4 import BeautifulSoup\r\n-import yt_dlp\r\n \r\n def fetch_youtube_transcripts(all_urls, response, history, conn=None, message=None, is_chat=True, max_transcripts=5):\r\n     print(f\"Fetching YouTube transcripts for {len(all_urls)} URLs\")\r\n     transcripts = []\r\n-    ydl_opts = {\r\n-        'writesubtitles': True,\r\n-        'writeautomaticsub': True,\r\n-        'subtitleslangs': ['en'],\r\n-        'subtitlesformat': 'vtt',\r\n-        'skip_download': True,\r\n-        'quiet': True,\r\n-        'outtmpl': '%(id)s.%(ext)s'\r\n-    }\r\n-    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\r\n-        for url in all_urls[:max_transcripts]:\r\n-            try:\r\n-                info = ydl.extract_info(url, download=False)\r\n-                video_id = info['id']\r\n-                ydl.download([url])\r\n-                vtt_file = f\"{video_id}.en.vtt\"\r\n-                if os.path.exists(vtt_file):\r\n-                    with open(vtt_file, \"r\", encoding=\"utf-8\") as f:\r\n-                        vtt_content = f.read()\r\n-                    # Parse VTT to plain text\r\n-                    transcript_text = re.sub(r'^\\d+\\n\\d{2}:\\d{2}:\\d{2}\\.\\d{3} --> \\d{2}:\\d{2}:\\d{2}\\.\\d{3}\\n', '', vtt_content)\r\n-                    transcript_text = re.sub(r'\\n\\n', '\\n', transcript_text).strip()\r\n-                    transcripts.append(transcript_text)\r\n-                    response += f\"Transcript fetched for {url} (yt-dlp)\\n\"\r\n-                    os.remove(vtt_file)\r\n-                else:\r\n-                    response += f\"Debug: No transcript file generated for {url}.\\n\"\r\n-                    response += f\"Error fetching transcript for {url}: No transcript available.\\n\"\r\n+    for url in all_urls[:max_transcripts]:\r\n+        try:\r\n+            with tempfile.TemporaryDirectory() as tmpdir:\r\n+                ydl_opts = {\r\n+                    'writesubtitles': True,\r\n+                    'writeautomaticsub': True,\r\n+                    'subtitleslangs': ['en'],\r\n+                    'subtitlesformat': 'vtt',\r\n+                    'skip_download': True,\r\n+                    'quiet': True,\r\n+                    'outtmpl': os.path.join(tmpdir, 'transcript.%(id)s.%(ext)s')\r\n+                }\r\n+                with yt_dlp.YoutubeDL(ydl_opts) as ydl:\r\n+                    info = ydl.extract_info(url, download=False)\r\n+                    video_id = info['id']\r\n+                    ydl.download([url])\r\n+                    vtt_file = os.path.join(tmpdir, f'transcript.{video_id}.en.vtt')\r\n+                    if os.path.exists(vtt_file):\r\n+                        with open(vtt_file, \"r\", encoding=\"utf-8\") as f:\r\n+                            vtt_content = f.read()\r\n+                        # Parse VTT to plain text\r\n+                        transcript_text = re.sub(r'^\\d+\\n\\d{2}:\\d{2}:\\d{2}\\.\\d{3} --> \\d{2}:\\d{2}:\\d{2}\\.\\d{3}\\n', '', vtt_content)\r\n+                        transcript_text = re.sub(r'\\n\\n', '\\n', transcript_text).strip()\r\n+                        transcripts.append(transcript_text)\r\n+                        response += f\"Transcript fetched for {url} (yt-dlp)\\n\"\r\n+                    else:\r\n+                        response += f\"Debug: No transcript file generated for {url}.\\n\"\r\n+                        response += f\"Error fetching transcript for {url}: No transcript available.\\n\"\r\n                 time.sleep(2)  # Avoid rate limiting\r\n-            except Exception as e:\r\n-                response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n-            if is_chat:\r\n-                history[-1][\"content\"] = response\r\n-                yield history, \"\"\r\n+        except Exception as e:\r\n+            response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n+        if is_chat:\r\n+            history[-1][\"content\"] = response\r\n+            yield history, \"\"\r\n     return transcripts, response, history\r\n \r\n def run_crawl(task_id, subreddit, timelimit, query, tasks, completed_crawls, conn=None, max_threads=20, max_comments=50):\r\n     print(f\"Starting crawl task {task_id} for subreddit {subreddit}\")\r\n"
                },
                {
                    "date": 1756918969888,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,4 +1,5 @@\n+# crawl_utils.py\r\n import os\r\n import threading\r\n import yt_dlp\r\n import tempfile\r\n@@ -21,13 +22,23 @@\n from selenium.webdriver.support import expected_conditions as EC\r\n from selenium.common.exceptions import TimeoutException\r\n import time\r\n from bs4 import BeautifulSoup\r\n+from db_utils import get_stored_content, store_content\r\n \r\n def fetch_youtube_transcripts(all_urls, response, history, conn=None, message=None, is_chat=True, max_transcripts=5):\r\n     print(f\"Fetching YouTube transcripts for {len(all_urls)} URLs\")\r\n-    transcripts = []\r\n+    transcripts = []  # Now list of (url, transcript_text) tuples\r\n     for url in all_urls[:max_transcripts]:\r\n+        stored = get_stored_content(conn, url)\r\n+        if stored:\r\n+            transcripts.append((url, stored))\r\n+            response += f\"Using stored transcript for {url}\\n\"\r\n+            if is_chat:\r\n+                history[-1][\"content\"] = response\r\n+                yield history, \"\"\r\n+            continue\r\n+\r\n         try:\r\n             with tempfile.TemporaryDirectory() as tmpdir:\r\n                 ydl_opts = {\r\n                     'writesubtitles': True,\r\n@@ -45,17 +56,17 @@\n                     vtt_file = os.path.join(tmpdir, f'transcript.{video_id}.en.vtt')\r\n                     if os.path.exists(vtt_file):\r\n                         with open(vtt_file, \"r\", encoding=\"utf-8\") as f:\r\n                             vtt_content = f.read()\r\n-                        # Parse VTT to plain text\r\n                         transcript_text = re.sub(r'^\\d+\\n\\d{2}:\\d{2}:\\d{2}\\.\\d{3} --> \\d{2}:\\d{2}:\\d{2}\\.\\d{3}\\n', '', vtt_content)\r\n                         transcript_text = re.sub(r'\\n\\n', '\\n', transcript_text).strip()\r\n-                        transcripts.append(transcript_text)\r\n+                        store_content(conn, url, transcript_text)  # NEW: Store fetched transcript\r\n+                        transcripts.append((url, transcript_text))\r\n                         response += f\"Transcript fetched for {url} (yt-dlp)\\n\"\r\n                     else:\r\n                         response += f\"Debug: No transcript file generated for {url}.\\n\"\r\n                         response += f\"Error fetching transcript for {url}: No transcript available.\\n\"\r\n-                time.sleep(2)  # Avoid rate limiting\r\n+                time.sleep(5)  # Increased from 2 to 5 to mitigate 429\r\n         except Exception as e:\r\n             response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n         if is_chat:\r\n             history[-1][\"content\"] = response\r\n"
                },
                {
                    "date": 1756919208515,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,38 +37,59 @@\n                 history[-1][\"content\"] = response\r\n                 yield history, \"\"\r\n             continue\r\n \r\n-        try:\r\n-            with tempfile.TemporaryDirectory() as tmpdir:\r\n-                ydl_opts = {\r\n-                    'writesubtitles': True,\r\n-                    'writeautomaticsub': True,\r\n-                    'subtitleslangs': ['en'],\r\n-                    'subtitlesformat': 'vtt',\r\n-                    'skip_download': True,\r\n-                    'quiet': True,\r\n-                    'outtmpl': os.path.join(tmpdir, 'transcript.%(id)s.%(ext)s')\r\n-                }\r\n-                with yt_dlp.YoutubeDL(ydl_opts) as ydl:\r\n-                    info = ydl.extract_info(url, download=False)\r\n-                    video_id = info['id']\r\n-                    ydl.download([url])\r\n-                    vtt_file = os.path.join(tmpdir, f'transcript.{video_id}.en.vtt')\r\n-                    if os.path.exists(vtt_file):\r\n-                        with open(vtt_file, \"r\", encoding=\"utf-8\") as f:\r\n-                            vtt_content = f.read()\r\n-                        transcript_text = re.sub(r'^\\d+\\n\\d{2}:\\d{2}:\\d{2}\\.\\d{3} --> \\d{2}:\\d{2}:\\d{2}\\.\\d{3}\\n', '', vtt_content)\r\n-                        transcript_text = re.sub(r'\\n\\n', '\\n', transcript_text).strip()\r\n-                        store_content(conn, url, transcript_text)  # NEW: Store fetched transcript\r\n-                        transcripts.append((url, transcript_text))\r\n-                        response += f\"Transcript fetched for {url} (yt-dlp)\\n\"\r\n-                    else:\r\n-                        response += f\"Debug: No transcript file generated for {url}.\\n\"\r\n-                        response += f\"Error fetching transcript for {url}: No transcript available.\\n\"\r\n-                time.sleep(5)  # Increased from 2 to 5 to mitigate 429\r\n-        except Exception as e:\r\n-            response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n+        retries = 3\r\n+        for attempt in range(retries):\r\n+            try:\r\n+                with tempfile.TemporaryDirectory() as tmpdir:\r\n+                    ydl_opts = {\r\n+                        'writesubtitles': True,\r\n+                        'writeautomaticsub': True,\r\n+                        'subtitleslangs': ['en'],\r\n+                        'subtitlesformat': 'vtt',\r\n+                        'skip_download': True,\r\n+                        'quiet': True,\r\n+                        'outtmpl': os.path.join(tmpdir, 'transcript.%(id)s.%(ext)s'),\r\n+                        'impersonate': 'chrome'  # Added to avoid detection and rate limits\r\n+                    }\r\n+                    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\r\n+                        info = ydl.extract_info(url, download=False)\r\n+                        video_id = info['id']\r\n+                        ydl.download([url])\r\n+                        vtt_file = os.path.join(tmpdir, f'transcript.{video_id}.en.vtt')\r\n+                        if os.path.exists(vtt_file):\r\n+                            with open(vtt_file, \"r\", encoding=\"utf-8\") as f:\r\n+                                vtt_content = f.read()\r\n+                            transcript_text = re.sub(r'^\\d+\\n\\d{2}:\\d{2}:\\d{2}\\.\\d{3} --> \\d{2}:\\d{2}:\\d{2}\\.\\d{3}\\n', '', vtt_content)\r\n+                            transcript_text = re.sub(r'\\n\\n', '\\n', transcript_text).strip()\r\n+                            store_content(conn, url, transcript_text)  # Store fetched transcript\r\n+                            transcripts.append((url, transcript_text))\r\n+                            response += f\"Transcript fetched for {url} (yt-dlp)\\n\"\r\n+                        else:\r\n+                            response += f\"Debug: No transcript file generated for {url}.\\n\"\r\n+                            response += f\"Error fetching transcript for {url}: No transcript available.\\n\"\r\n+                break\r\n+            except Exception as e:\r\n+                if '429' in str(e):\r\n+                    response += f\"Rate limit hit for {url}, retrying after 30s... (attempt {attempt+1}/{retries})\\n\"\r\n+                    if is_chat:\r\n+                        history[-1][\"content\"] = response\r\n+                        yield history, \"\"\r\n+                    time.sleep(30)\r\n+                else:\r\n+                    response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n+                    if is_chat:\r\n+                        history[-1][\"content\"] = response\r\n+                        yield history, \"\"\r\n+                    break\r\n+        else:\r\n+            response += f\"Failed to fetch transcript for {url} after {retries} attempts.\\n\"\r\n+            if is_chat:\r\n+                history[-1][\"content\"] = response\r\n+                yield history, \"\"\r\n+\r\n+        time.sleep(10)  # Increased sleep to 10 seconds to further mitigate rate limiting\r\n         if is_chat:\r\n             history[-1][\"content\"] = response\r\n             yield history, \"\"\r\n     return transcripts, response, history\r\n"
                },
                {
                    "date": 1756919834964,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,17 +23,22 @@\n from selenium.common.exceptions import TimeoutException\r\n import time\r\n from bs4 import BeautifulSoup\r\n from db_utils import get_stored_content, store_content\r\n+import glob\r\n \r\n def fetch_youtube_transcripts(all_urls, response, history, conn=None, message=None, is_chat=True, max_transcripts=5):\r\n     print(f\"Fetching YouTube transcripts for {len(all_urls)} URLs\")\r\n-    transcripts = []  # Now list of (url, transcript_text) tuples\r\n+    transcripts = []  # list of (url, transcript_text) tuples\r\n     for url in all_urls[:max_transcripts]:\r\n         stored = get_stored_content(conn, url)\r\n-        if stored:\r\n-            transcripts.append((url, stored))\r\n-            response += f\"Using stored transcript for {url}\\n\"\r\n+        if stored is not None:  # If stored exists, even if empty\r\n+            if stored:\r\n+                transcripts.append((url, stored))\r\n+                response += f\"Using stored transcript for {url}\\n\"\r\n+            else:\r\n+                response += f\"No transcript available for {url} (stored)\\n\"\r\n+                transcripts.append((url, \"\"))\r\n             if is_chat:\r\n                 history[-1][\"content\"] = response\r\n                 yield history, \"\"\r\n             continue\r\n@@ -41,44 +46,73 @@\n         retries = 3\r\n         for attempt in range(retries):\r\n             try:\r\n                 with tempfile.TemporaryDirectory() as tmpdir:\r\n-                    ydl_opts = {\r\n+                    ydl_opts_base = {\r\n                         'writesubtitles': True,\r\n                         'writeautomaticsub': True,\r\n-                        'subtitleslangs': ['en'],\r\n                         'subtitlesformat': 'vtt',\r\n                         'skip_download': True,\r\n-                        'quiet': True,\r\n+                        'quiet': False,\r\n+                        'verbose': True,\r\n                         'outtmpl': os.path.join(tmpdir, 'transcript.%(id)s.%(ext)s'),\r\n-                        'impersonate': 'chrome'  # Added to avoid detection and rate limits\r\n+                        'impersonate': 'chrome',\r\n+                        'sleep_subtitles': 5,\r\n+                        'extractor_args': {'youtube': {'skip': ['translated_subs']}}\r\n                     }\r\n-                    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\r\n+                    with yt_dlp.YoutubeDL(ydl_opts_base) as ydl:\r\n                         info = ydl.extract_info(url, download=False)\r\n                         video_id = info['id']\r\n-                        ydl.download([url])\r\n-                        vtt_file = os.path.join(tmpdir, f'transcript.{video_id}.en.vtt')\r\n+\r\n+                        # Check for English subtitles or auto-captions\r\n+                        subs = info.get('subtitles', {})\r\n+                        auto = info.get('automatic_captions', {})\r\n+                        en_subs_langs = [l for l in subs if l.startswith('en')]\r\n+                        en_auto_langs = [l for l in auto if l.startswith('en')]\r\n+                        preferred_langs = en_subs_langs or en_auto_langs\r\n+                        if not preferred_langs:\r\n+                            response += f\"No English subtitles or auto-captions available for {url}\\n\"\r\n+                            store_content(conn, url, \"\")\r\n+                            transcripts.append((url, \"\"))\r\n+                            break\r\n+\r\n+                        # Prefer 'en' if available, else first\r\n+                        def get_preferred_lang(langs):\r\n+                            if 'en' in langs:\r\n+                                return 'en'\r\n+                            return langs[0]\r\n+\r\n+                        lang = get_preferred_lang(preferred_langs)\r\n+                        ydl_opts = ydl_opts_base.copy()\r\n+                        ydl_opts['subtitleslangs'] = [lang]\r\n+                        with yt_dlp.YoutubeDL(ydl_opts) as ydl_with_opts:\r\n+                            ydl_with_opts.download([url])\r\n+\r\n+                        vtt_file = os.path.join(tmpdir, f'transcript.{video_id}.{lang}.vtt')\r\n                         if os.path.exists(vtt_file):\r\n                             with open(vtt_file, \"r\", encoding=\"utf-8\") as f:\r\n                                 vtt_content = f.read()\r\n                             transcript_text = re.sub(r'^\\d+\\n\\d{2}:\\d{2}:\\d{2}\\.\\d{3} --> \\d{2}:\\d{2}:\\d{2}\\.\\d{3}\\n', '', vtt_content)\r\n                             transcript_text = re.sub(r'\\n\\n', '\\n', transcript_text).strip()\r\n-                            store_content(conn, url, transcript_text)  # Store fetched transcript\r\n+                            store_content(conn, url, transcript_text)\r\n                             transcripts.append((url, transcript_text))\r\n-                            response += f\"Transcript fetched for {url} (yt-dlp)\\n\"\r\n+                            response += f\"Transcript fetched for {url} (yt-dlp, lang: {lang})\\n\"\r\n                         else:\r\n-                            response += f\"Debug: No transcript file generated for {url}.\\n\"\r\n+                            response += f\"Debug: No transcript file generated for {url} despite availability indication.\\n\"\r\n                             response += f\"Error fetching transcript for {url}: No transcript available.\\n\"\r\n+                            store_content(conn, url, \"\")\r\n+                            transcripts.append((url, \"\"))\r\n                 break\r\n             except Exception as e:\r\n-                if '429' in str(e):\r\n+                error_str = str(e)\r\n+                if '429' in error_str:\r\n                     response += f\"Rate limit hit for {url}, retrying after 30s... (attempt {attempt+1}/{retries})\\n\"\r\n                     if is_chat:\r\n                         history[-1][\"content\"] = response\r\n                         yield history, \"\"\r\n                     time.sleep(30)\r\n                 else:\r\n-                    response += f\"Error fetching transcript for {url}: {e}\\n\"\r\n+                    response += f\"Error fetching transcript for {url}: {error_str}\\n\"\r\n                     if is_chat:\r\n                         history[-1][\"content\"] = response\r\n                         yield history, \"\"\r\n                     break\r\n"
                }
            ],
            "date": 1756856992432,
            "name": "Commit-0",
            "content": "import threading\r\nfrom web_utils import search_web\r\nfrom process_utils import process_urls\r\nfrom config import MAX_URLS\r\n\r\ndef run_crawl(task_id, subreddit, timelimit, query, tasks, conn=None):\r\n    print(f\"Starting crawl task {task_id} for subreddit {subreddit}\")\r\n    try:\r\n        site = f\"reddit.com/r/{subreddit}\"\r\n        timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n        urls = search_web(query, site=site, timelimit=timelimit_code)\r\n        all_urls = list(set(urls))[:MAX_URLS]\r\n\r\n        history = [{\"role\": \"assistant\", \"content\": \"\"}]  # Dummy history to avoid index error\r\n        message = query\r\n        response = \"\"\r\n        process_gen = process_urls(all_urls, response, history, message, is_chat=False, conn=conn)\r\n        for _ in process_gen:\r\n            pass\r\n\r\n        tasks[task_id]['status'] = 'completed'\r\n        tasks[task_id]['message'] = \"Crawl completed. New content added to vectorstore if applicable.\"\r\n        print(f\"Crawl task {task_id} completed.\")\r\n    except Exception as e:\r\n        tasks[task_id]['status'] = 'error'\r\n        tasks[task_id]['message'] = str(e)\r\n        print(f\"Crawl task {task_id} error: {e}\")\r\n\r\ndef start_crawl(subreddit, timelimit, query, tasks, conn=None):\r\n    print(\"Starting new crawl...\")\r\n    task_id = len(tasks)\r\n    task = {'id': task_id, 'subreddit': subreddit, 'timelimit': timelimit, 'query': query, 'status': 'running', 'message': ''}\r\n    tasks.append(task)\r\n    threading.Thread(target=run_crawl, args=(task_id, subreddit, timelimit, query, tasks, conn)).start()\r\n    print(f\"Crawl task {task_id} started.\")\r\n    return \"Crawl started in background.\", tasks"
        }
    ]
}