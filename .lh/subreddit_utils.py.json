{
    "sourceFile": "subreddit_utils.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 9,
            "patches": [
                {
                    "date": 1756935549480,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1756936241886,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,19 +1,34 @@\n # subreddit_utils.py\r\n import os\r\n import threading\r\n import requests\r\n+import sqlite3\r\n from web_utils import search_web\r\n from config import MAX_URLS, FAISS_PATH\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n from db_utils import add_chunk_if_new\r\n from vectorstore_utils import vectorstore\r\n from utils import lock\r\n \r\n-def run_subreddit_collection(task_id, subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections, conn=None):\r\n+def run_subreddit_collection(task_id, subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections):\r\n     print(f\"Starting Subreddit collection task {task_id} for subreddit {subreddit}\")\r\n+    conn = sqlite3.connect('crawled.db')\r\n+    c = conn.cursor()\r\n+    c.execute('''CREATE TABLE IF NOT EXISTS urls\r\n+                 (url TEXT PRIMARY KEY, timestamp DATETIME, cleaned_text TEXT)''')\r\n+    c.execute('''CREATE TABLE IF NOT EXISTS chunks\r\n+                 (hash TEXT PRIMARY KEY, content TEXT, source TEXT, tag TEXT)''')\r\n     try:\r\n+        c.execute(\"ALTER TABLE chunks ADD COLUMN tag TEXT\")\r\n+        print(\"Added 'tag' column to chunks table.\")\r\n+    except sqlite3.OperationalError as e:\r\n+        if \"duplicate column name\" not in str(e):\r\n+            raise e\r\n+        print(\"'tag' column already exists in chunks table.\")\r\n+    conn.commit()\r\n+    try:\r\n         site = f\"reddit.com/r/{subreddit}\"\r\n         timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n         urls = search_web(query, site=site, timelimit=timelimit_code)\r\n         all_urls = list(set(urls))[:MAX_URLS]\r\n@@ -60,11 +75,13 @@\n     except Exception as e:\r\n         tasks[task_id]['status'] = 'error'\r\n         tasks[task_id]['message'] = str(e)\r\n         print(f\"Subreddit collection task {task_id} error: {e}\")\r\n+    finally:\r\n+        conn.close()\r\n \r\n-def start_subreddit_collection(subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections, conn=None):\r\n+def start_subreddit_collection(subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections):\r\n     task_id = len(tasks)\r\n     task = {'id': task_id, 'type': 'subreddit', 'subreddit': subreddit, 'timelimit': timelimit, 'query': query, 'use_ollama': use_ollama, 'max_comments': max_comments, 'status': 'running', 'message': ''}\r\n     tasks.append(task)\r\n-    threading.Thread(target=run_subreddit_collection, args=(task_id, subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections, conn)).start()\r\n+    threading.Thread(target=run_subreddit_collection, args=(task_id, subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections)).start()\r\n     return \"Subreddit collection started in background.\", tasks, completed_collections\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756938058361,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,9 +10,9 @@\n from db_utils import add_chunk_if_new\r\n from vectorstore_utils import vectorstore\r\n from utils import lock\r\n \r\n-def run_subreddit_collection(task_id, subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections):\r\n+def run_subreddit_collection(task_id, custom_name, subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections):\r\n     print(f\"Starting Subreddit collection task {task_id} for subreddit {subreddit}\")\r\n     conn = sqlite3.connect('crawled.db')\r\n     c = conn.cursor()\r\n     c.execute('''CREATE TABLE IF NOT EXISTS urls\r\n@@ -33,8 +33,9 @@\n         urls = search_web(query, site=site, timelimit=timelimit_code)\r\n         all_urls = list(set(urls))[:MAX_URLS]\r\n \r\n         tag = f\"subreddit_{subreddit}_{query}_{timelimit}\"\r\n+        name = custom_name or f\"Subreddit {subreddit} - {query} ({timelimit})\"\r\n         response = \"\"\r\n         full_content = []\r\n         for url in all_urls:\r\n             try:\r\n@@ -69,19 +70,19 @@\n \r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = \"Collection completed. New content added to vectorstore if applicable.\"\r\n         tasks[task_id]['tag'] = tag\r\n-        completed_collections.append({'name': f\"Subreddit {subreddit} - {query} ({timelimit})\", 'tag': tag})\r\n+        completed_collections.append({'name': name, 'tag': tag})\r\n         print(f\"Subreddit collection task {task_id} completed.\")\r\n     except Exception as e:\r\n         tasks[task_id]['status'] = 'error'\r\n         tasks[task_id]['message'] = str(e)\r\n         print(f\"Subreddit collection task {task_id} error: {e}\")\r\n     finally:\r\n         conn.close()\r\n \r\n-def start_subreddit_collection(subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections):\r\n+def start_subreddit_collection(custom_name, subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections):\r\n     task_id = len(tasks)\r\n-    task = {'id': task_id, 'type': 'subreddit', 'subreddit': subreddit, 'timelimit': timelimit, 'query': query, 'use_ollama': use_ollama, 'max_comments': max_comments, 'status': 'running', 'message': ''}\r\n+    task = {'id': task_id, 'type': 'subreddit', 'custom_name': custom_name, 'subreddit': subreddit, 'timelimit': timelimit, 'query': query, 'use_ollama': use_ollama, 'max_comments': max_comments, 'status': 'running', 'message': ''}\r\n     tasks.append(task)\r\n-    threading.Thread(target=run_subreddit_collection, args=(task_id, subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections)).start()\r\n+    threading.Thread(target=run_subreddit_collection, args=(task_id, custom_name, subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections)).start()\r\n     return \"Subreddit collection started in background.\", tasks, completed_collections\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756943224593,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,14 +3,16 @@\n import threading\r\n import requests\r\n import sqlite3\r\n from web_utils import search_web\r\n-from config import MAX_URLS, FAISS_PATH\r\n+from config import MAX_URLS, FAISS_PATH, RAW_DIR\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n from db_utils import add_chunk_if_new\r\n-from vectorstore_utils import vectorstore\r\n+from vectorstore_manager import get_vectorstore\r\n from utils import lock\r\n+from urllib.parse import quote\r\n+import html\r\n \r\n def run_subreddit_collection(task_id, custom_name, subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections):\r\n     print(f\"Starting Subreddit collection task {task_id} for subreddit {subreddit}\")\r\n     conn = sqlite3.connect('crawled.db')\r\n@@ -45,10 +47,24 @@\n                     data = json_response.json()\r\n                     post_text = data[0]['data']['children'][0]['data']['selftext']\r\n                     comments = data[1]['data']['children']\r\n                     comment_texts = [comment['data']['body'] for comment in comments[:max_comments] if 'body' in comment['data']]\r\n-                    full_thread_text = post_text + \" \".join(comment_texts)\r\n-                    full_content.append(full_thread_text)\r\n+                    full_text = post_text + \" \".join(comment_texts)\r\n+                    # Save to raw_contents\r\n+                    prefix = \"ollama_\" if use_ollama else \"\"\r\n+                    safe_filename = prefix + quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n+                    filepath = os.path.join(RAW_DIR, safe_filename)\r\n+                    with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                        f.write(full_text)\r\n+\r\n+                    # HTML view\r\n+                    html_safe_filename = safe_filename.replace(\".txt\", \".html\")\r\n+                    html_filepath = os.path.join(RAW_DIR, html_safe_filename)\r\n+                    escaped_text = html.escape(full_text)\r\n+                    html_content = f\"\"\"<html><head><style>body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }} pre {{ white-space: pre-wrap; word-wrap: break-word; }}</style></head><body><h1>Extracted Content for {url}</h1><pre>{escaped_text}</pre></body></html>\"\"\"\r\n+                    with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                        f.write(html_content)\r\n+\r\n                     response += f\"Fetched full thread and comments for {url}\\n\"\r\n                 else:\r\n                     response += f\"Failed to fetch JSON for {url}: Status {json_response.status_code}\\n\"\r\n             except Exception as e:\r\n@@ -64,10 +80,10 @@\n                 new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n \r\n         if new_docs:\r\n             with lock:\r\n-                vectorstore.add_documents(new_docs)\r\n-                vectorstore.save_local(FAISS_PATH)\r\n+                vs = get_vectorstore(tag)\r\n+                vs.add_documents(new_docs)\r\n \r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = \"Collection completed. New content added to vectorstore if applicable.\"\r\n         tasks[task_id]['tag'] = tag\r\n"
                },
                {
                    "date": 1756945837890,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,16 +3,14 @@\n import threading\r\n import requests\r\n import sqlite3\r\n from web_utils import search_web\r\n-from config import MAX_URLS, FAISS_PATH, RAW_DIR\r\n+from config import MAX_URLS, FAISS_PATH\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n-from db_utils import add_chunk_if_new\r\n+from db_utils import add_chunk_if_new, add_collection\r\n from vectorstore_manager import get_vectorstore\r\n from utils import lock\r\n-from urllib.parse import quote\r\n-import html\r\n \r\n def run_subreddit_collection(task_id, custom_name, subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections):\r\n     print(f\"Starting Subreddit collection task {task_id} for subreddit {subreddit}\")\r\n     conn = sqlite3.connect('crawled.db')\r\n@@ -47,24 +45,10 @@\n                     data = json_response.json()\r\n                     post_text = data[0]['data']['children'][0]['data']['selftext']\r\n                     comments = data[1]['data']['children']\r\n                     comment_texts = [comment['data']['body'] for comment in comments[:max_comments] if 'body' in comment['data']]\r\n-                    full_text = post_text + \" \".join(comment_texts)\r\n-                    # Save to raw_contents\r\n-                    prefix = \"ollama_\" if use_ollama else \"\"\r\n-                    safe_filename = prefix + quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n-                    filepath = os.path.join(RAW_DIR, safe_filename)\r\n-                    with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n-                        f.write(full_text)\r\n-\r\n-                    # HTML view\r\n-                    html_safe_filename = safe_filename.replace(\".txt\", \".html\")\r\n-                    html_filepath = os.path.join(RAW_DIR, html_safe_filename)\r\n-                    escaped_text = html.escape(full_text)\r\n-                    html_content = f\"\"\"<html><head><style>body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }} pre {{ white-space: pre-wrap; word-wrap: break-word; }}</style></head><body><h1>Extracted Content for {url}</h1><pre>{escaped_text}</pre></body></html>\"\"\"\r\n-                    with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n-                        f.write(html_content)\r\n-\r\n+                    full_thread_text = post_text + \" \".join(comment_texts)\r\n+                    full_content.append(full_thread_text)\r\n                     response += f\"Fetched full thread and comments for {url}\\n\"\r\n                 else:\r\n                     response += f\"Failed to fetch JSON for {url}: Status {json_response.status_code}\\n\"\r\n             except Exception as e:\r\n@@ -83,8 +67,9 @@\n             with lock:\r\n                 vs = get_vectorstore(tag)\r\n                 vs.add_documents(new_docs)\r\n \r\n+        add_collection(conn, name, tag)\r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = \"Collection completed. New content added to vectorstore if applicable.\"\r\n         tasks[task_id]['tag'] = tag\r\n         completed_collections.append({'name': name, 'tag': tag})\r\n"
                },
                {
                    "date": 1756946379036,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,14 +3,16 @@\n import threading\r\n import requests\r\n import sqlite3\r\n from web_utils import search_web\r\n-from config import MAX_URLS, FAISS_PATH\r\n+from config import MAX_URLS, FAISS_PATH, RAW_DIR\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n from db_utils import add_chunk_if_new, add_collection\r\n from vectorstore_manager import get_vectorstore\r\n from utils import lock\r\n+from urllib.parse import quote\r\n+import html\r\n \r\n def run_subreddit_collection(task_id, custom_name, subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections):\r\n     print(f\"Starting Subreddit collection task {task_id} for subreddit {subreddit}\")\r\n     conn = sqlite3.connect('crawled.db')\r\n@@ -45,10 +47,24 @@\n                     data = json_response.json()\r\n                     post_text = data[0]['data']['children'][0]['data']['selftext']\r\n                     comments = data[1]['data']['children']\r\n                     comment_texts = [comment['data']['body'] for comment in comments[:max_comments] if 'body' in comment['data']]\r\n-                    full_thread_text = post_text + \" \".join(comment_texts)\r\n-                    full_content.append(full_thread_text)\r\n+                    full_text = post_text + \" \".join(comment_texts)\r\n+                    # Save to raw_contents\r\n+                    prefix = \"ollama_\" if use_ollama else \"\"\r\n+                    safe_filename = prefix + quote(url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")[:100]) + \".txt\"\r\n+                    filepath = os.path.join(RAW_DIR, safe_filename)\r\n+                    with open(filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                        f.write(full_text)\r\n+\r\n+                    # HTML view\r\n+                    html_safe_filename = safe_filename.replace(\".txt\", \".html\")\r\n+                    html_filepath = os.path.join(RAW_DIR, html_safe_filename)\r\n+                    escaped_text = html.escape(full_text)\r\n+                    html_content = f\"\"\"<html><head><style>body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }} pre {{ white-space: pre-wrap; word-wrap: break-word; }}</style></head><body><h1>Extracted Content for {url}</h1><pre>{escaped_text}</pre></body></html>\"\"\"\r\n+                    with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n+                        f.write(html_content)\r\n+\r\n                     response += f\"Fetched full thread and comments for {url}\\n\"\r\n                 else:\r\n                     response += f\"Failed to fetch JSON for {url}: Status {json_response.status_code}\\n\"\r\n             except Exception as e:\r\n@@ -67,9 +83,10 @@\n             with lock:\r\n                 vs = get_vectorstore(tag)\r\n                 vs.add_documents(new_docs)\r\n \r\n-        add_collection(conn, name, tag)\r\n+        add_collection(conn, name, tag)  # Save to DB\r\n+\r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = \"Collection completed. New content added to vectorstore if applicable.\"\r\n         tasks[task_id]['tag'] = tag\r\n         completed_collections.append({'name': name, 'tag': tag})\r\n"
                },
                {
                    "date": 1756958184098,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,9 +6,9 @@\n from web_utils import search_web\r\n from config import MAX_URLS, FAISS_PATH, RAW_DIR\r\n from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n from langchain_core.documents import Document\r\n-from db_utils import add_chunk_if_new, add_collection\r\n+from db_utils import add_chunk_if_new, store_content, get_stored_content, add_collection\r\n from vectorstore_manager import get_vectorstore\r\n from utils import lock\r\n from urllib.parse import quote\r\n import html\r\n@@ -63,8 +63,9 @@\n                     html_content = f\"\"\"<html><head><style>body {{ font-family: sans-serif; padding: 20px; line-height: 1.6; max-width: 800px; margin: auto; }} pre {{ white-space: pre-wrap; word-wrap: break-word; }}</style></head><body><h1>Extracted Content for {url}</h1><pre>{escaped_text}</pre></body></html>\"\"\"\r\n                     with open(html_filepath, \"w\", encoding=\"utf-8\") as f:\r\n                         f.write(html_content)\r\n \r\n+                    store_content(conn, url, full_text)\r\n                     response += f\"Fetched full thread and comments for {url}\\n\"\r\n                 else:\r\n                     response += f\"Failed to fetch JSON for {url}: Status {json_response.status_code}\\n\"\r\n             except Exception as e:\r\n@@ -82,8 +83,9 @@\n         if new_docs:\r\n             with lock:\r\n                 vs = get_vectorstore(tag)\r\n                 vs.add_documents(new_docs)\r\n+                print(f\"Added {len(new_docs)} documents to vectorstore for tag {tag}.\")\r\n \r\n         add_collection(conn, name, tag)  # Save to DB\r\n \r\n         tasks[task_id]['status'] = 'completed'\r\n"
                },
                {
                    "date": 1756961502237,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,10 +11,21 @@\n from vectorstore_manager import get_vectorstore\r\n from utils import lock\r\n from urllib.parse import quote\r\n import html\r\n+import re  # Added for sanitization\r\n \r\n-def run_subreddit_collection(task_id, custom_name, subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections):\r\n+def sanitize_tag(name):\r\n+    # Replace invalid path characters with '_'\r\n+    invalid_chars = r'[<>:\"/\\\\|?*]'\r\n+    sanitized = re.sub(invalid_chars, '_', name)\r\n+    # Strip leading/trailing whitespace\r\n+    sanitized = sanitized.strip()\r\n+    # Collapse multiple '_' into single\r\n+    sanitized = re.sub(r'_+', '_', sanitized)\r\n+    return sanitized\r\n+\r\n+def run_subreddit_collection(task_id, custom_name, subreddit, timelimit, query, max_urls, use_ollama, max_comments, tasks, completed_collections):\r\n     print(f\"Starting Subreddit collection task {task_id} for subreddit {subreddit}\")\r\n     conn = sqlite3.connect('crawled.db')\r\n     c = conn.cursor()\r\n     c.execute('''CREATE TABLE IF NOT EXISTS urls\r\n@@ -32,11 +43,13 @@\n     try:\r\n         site = f\"reddit.com/r/{subreddit}\"\r\n         timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n         urls = search_web(query, site=site, timelimit=timelimit_code)\r\n-        all_urls = list(set(urls))[:MAX_URLS]\r\n+        all_urls = list(set(urls))[:max_urls]\r\n \r\n-        tag = f\"subreddit_{subreddit}_{query}_{timelimit}\"\r\n+        raw_tag = f\"subreddit_{subreddit}_{query}_{timelimit}\"\r\n+        tag = sanitize_tag(raw_tag)  # Sanitize to prevent invalid path characters\r\n+        print(f\"Debug: Sanitized tag from '{raw_tag}' to '{tag}'\")\r\n         name = custom_name or f\"Subreddit {subreddit} - {query} ({timelimit})\"\r\n         response = \"\"\r\n         full_content = []\r\n         for url in all_urls:\r\n@@ -99,10 +112,10 @@\n         print(f\"Subreddit collection task {task_id} error: {e}\")\r\n     finally:\r\n         conn.close()\r\n \r\n-def start_subreddit_collection(custom_name, subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections):\r\n+def start_subreddit_collection(custom_name, subreddit, timelimit, query, max_urls=10, use_ollama=False, max_comments=50, tasks=None, completed_collections=None):\r\n     task_id = len(tasks)\r\n-    task = {'id': task_id, 'type': 'subreddit', 'custom_name': custom_name, 'subreddit': subreddit, 'timelimit': timelimit, 'query': query, 'use_ollama': use_ollama, 'max_comments': max_comments, 'status': 'running', 'message': ''}\r\n+    task = {'id': task_id, 'type': 'subreddit', 'custom_name': custom_name, 'subreddit': subreddit, 'timelimit': timelimit, 'query': query, 'max_urls': max_urls, 'use_ollama': use_ollama, 'max_comments': max_comments, 'status': 'running', 'message': ''}\r\n     tasks.append(task)\r\n-    threading.Thread(target=run_subreddit_collection, args=(task_id, custom_name, subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections)).start()\r\n+    threading.Thread(target=run_subreddit_collection, args=(task_id, custom_name, subreddit, timelimit, query, max_urls, use_ollama, max_comments, tasks, completed_collections)).start()\r\n     return \"Subreddit collection started in background.\", tasks, completed_collections\n\\ No newline at end of file\n"
                },
                {
                    "date": 1756962979299,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,9 +50,9 @@\n         tag = sanitize_tag(raw_tag)  # Sanitize to prevent invalid path characters\r\n         print(f\"Debug: Sanitized tag from '{raw_tag}' to '{tag}'\")\r\n         name = custom_name or f\"Subreddit {subreddit} - {query} ({timelimit})\"\r\n         response = \"\"\r\n-        full_content = []\r\n+        new_docs_total = 0\r\n         for url in all_urls:\r\n             try:\r\n                 json_url = url + '.json'\r\n                 json_response = requests.get(json_url, headers={'User-Agent': 'Mozilla/5.0'})\r\n@@ -78,28 +78,32 @@\n                         f.write(html_content)\r\n \r\n                     store_content(conn, url, full_text)\r\n                     response += f\"Fetched full thread and comments for {url}\\n\"\r\n+\r\n+                    # Chunk and add per URL\r\n+                    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n+                    chunks = text_splitter.split_text(full_text)\r\n+                    new_docs = []\r\n+                    for chunk in chunks:\r\n+                        if add_chunk_if_new(conn, chunk, url, tag=tag):\r\n+                            metadata = {\"source\": url, \"tag\": tag}\r\n+                            new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+                    if new_docs:\r\n+                        with lock:\r\n+                            vs = get_vectorstore(tag)\r\n+                            vs.add_documents(new_docs)\r\n+                            print(f\"Added {len(new_docs)} documents to vectorstore for tag {tag}.\")\r\n+                            # Save the vectorstore to disk after adding documents\r\n+                            save_path = os.path.join(FAISS_PATH, tag)\r\n+                            vs.save_local(save_path)\r\n+                            print(f\"Debug: Saved vectorstore for tag {tag} to {save_path}.\")\r\n+                        new_docs_total += len(new_docs)\r\n                 else:\r\n                     response += f\"Failed to fetch JSON for {url}: Status {json_response.status_code}\\n\"\r\n             except Exception as e:\r\n                 response += f\"Error fetching thread for {url}: {e}\\n\"\r\n \r\n-        combined_content = \" \".join(full_content)\r\n-        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n-        chunks = text_splitter.split_text(combined_content)\r\n-        new_docs = []\r\n-        for chunk in chunks:\r\n-            if add_chunk_if_new(conn, chunk, subreddit, tag=tag):\r\n-                metadata = {\"source\": subreddit, \"tag\": tag}\r\n-                new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n-\r\n-        if new_docs:\r\n-            with lock:\r\n-                vs = get_vectorstore(tag)\r\n-                vs.add_documents(new_docs)\r\n-                print(f\"Added {len(new_docs)} documents to vectorstore for tag {tag}.\")\r\n-\r\n         add_collection(conn, name, tag)  # Save to DB\r\n \r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = \"Collection completed. New content added to vectorstore if applicable.\"\r\n"
                },
                {
                    "date": 1756964017707,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,9 +50,8 @@\n         tag = sanitize_tag(raw_tag)  # Sanitize to prevent invalid path characters\r\n         print(f\"Debug: Sanitized tag from '{raw_tag}' to '{tag}'\")\r\n         name = custom_name or f\"Subreddit {subreddit} - {query} ({timelimit})\"\r\n         response = \"\"\r\n-        new_docs_total = 0\r\n         for url in all_urls:\r\n             try:\r\n                 json_url = url + '.json'\r\n                 json_response = requests.get(json_url, headers={'User-Agent': 'Mozilla/5.0'})\r\n@@ -78,32 +77,36 @@\n                         f.write(html_content)\r\n \r\n                     store_content(conn, url, full_text)\r\n                     response += f\"Fetched full thread and comments for {url}\\n\"\r\n-\r\n-                    # Chunk and add per URL\r\n-                    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n-                    chunks = text_splitter.split_text(full_text)\r\n-                    new_docs = []\r\n-                    for chunk in chunks:\r\n-                        if add_chunk_if_new(conn, chunk, url, tag=tag):\r\n-                            metadata = {\"source\": url, \"tag\": tag}\r\n-                            new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n-                    if new_docs:\r\n-                        with lock:\r\n-                            vs = get_vectorstore(tag)\r\n-                            vs.add_documents(new_docs)\r\n-                            print(f\"Added {len(new_docs)} documents to vectorstore for tag {tag}.\")\r\n-                            # Save the vectorstore to disk after adding documents\r\n-                            save_path = os.path.join(FAISS_PATH, tag)\r\n-                            vs.save_local(save_path)\r\n-                            print(f\"Debug: Saved vectorstore for tag {tag} to {save_path}.\")\r\n-                        new_docs_total += len(new_docs)\r\n                 else:\r\n                     response += f\"Failed to fetch JSON for {url}: Status {json_response.status_code}\\n\"\r\n             except Exception as e:\r\n                 response += f\"Error fetching thread for {url}: {e}\\n\"\r\n \r\n+        # Then process to chunks per url\r\n+        new_docs_total = 0\r\n+        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n+        for url in all_urls:\r\n+            content = get_stored_content(conn, url)\r\n+            if content:\r\n+                chunks = text_splitter.split_text(content)\r\n+                new_docs = []\r\n+                for chunk in chunks:\r\n+                    if add_chunk_if_new(conn, chunk, url, tag=tag):\r\n+                        metadata = {\"source\": url, \"tag\": tag}\r\n+                        new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n+                if new_docs:\r\n+                    with lock:\r\n+                        vs = get_vectorstore(tag)\r\n+                        vs.add_documents(new_docs)\r\n+                        print(f\"Debug: Added {len(new_docs)} documents to vectorstore for tag {tag}. ntotal after add: {vs.index.ntotal}\")\r\n+                        # Save the vectorstore to disk after adding documents\r\n+                        save_path = os.path.join(FAISS_PATH, tag)\r\n+                        vs.save_local(save_path)\r\n+                        print(f\"Debug: Saved vectorstore for tag {tag} to {save_path}.\")\r\n+                    new_docs_total += len(new_docs)\r\n+\r\n         add_collection(conn, name, tag)  # Save to DB\r\n \r\n         tasks[task_id]['status'] = 'completed'\r\n         tasks[task_id]['message'] = \"Collection completed. New content added to vectorstore if applicable.\"\r\n"
                }
            ],
            "date": 1756935549480,
            "name": "Commit-0",
            "content": "# subreddit_utils.py\r\nimport os\r\nimport threading\r\nimport requests\r\nfrom web_utils import search_web\r\nfrom config import MAX_URLS, FAISS_PATH\r\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\r\nfrom langchain_core.documents import Document\r\nfrom db_utils import add_chunk_if_new\r\nfrom vectorstore_utils import vectorstore\r\nfrom utils import lock\r\n\r\ndef run_subreddit_collection(task_id, subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections, conn=None):\r\n    print(f\"Starting Subreddit collection task {task_id} for subreddit {subreddit}\")\r\n    try:\r\n        site = f\"reddit.com/r/{subreddit}\"\r\n        timelimit_code = {'Day': 'd', 'Week': 'w', 'Month': 'm', 'Year': 'y'}.get(timelimit)\r\n        urls = search_web(query, site=site, timelimit=timelimit_code)\r\n        all_urls = list(set(urls))[:MAX_URLS]\r\n\r\n        tag = f\"subreddit_{subreddit}_{query}_{timelimit}\"\r\n        response = \"\"\r\n        full_content = []\r\n        for url in all_urls:\r\n            try:\r\n                json_url = url + '.json'\r\n                json_response = requests.get(json_url, headers={'User-Agent': 'Mozilla/5.0'})\r\n                if json_response.status_code == 200:\r\n                    data = json_response.json()\r\n                    post_text = data[0]['data']['children'][0]['data']['selftext']\r\n                    comments = data[1]['data']['children']\r\n                    comment_texts = [comment['data']['body'] for comment in comments[:max_comments] if 'body' in comment['data']]\r\n                    full_thread_text = post_text + \" \".join(comment_texts)\r\n                    full_content.append(full_thread_text)\r\n                    response += f\"Fetched full thread and comments for {url}\\n\"\r\n                else:\r\n                    response += f\"Failed to fetch JSON for {url}: Status {json_response.status_code}\\n\"\r\n            except Exception as e:\r\n                response += f\"Error fetching thread for {url}: {e}\\n\"\r\n\r\n        combined_content = \" \".join(full_content)\r\n        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\r\n        chunks = text_splitter.split_text(combined_content)\r\n        new_docs = []\r\n        for chunk in chunks:\r\n            if add_chunk_if_new(conn, chunk, subreddit, tag=tag):\r\n                metadata = {\"source\": subreddit, \"tag\": tag}\r\n                new_docs.append(Document(page_content=chunk, metadata=metadata))\r\n\r\n        if new_docs:\r\n            with lock:\r\n                vectorstore.add_documents(new_docs)\r\n                vectorstore.save_local(FAISS_PATH)\r\n\r\n        tasks[task_id]['status'] = 'completed'\r\n        tasks[task_id]['message'] = \"Collection completed. New content added to vectorstore if applicable.\"\r\n        tasks[task_id]['tag'] = tag\r\n        completed_collections.append({'name': f\"Subreddit {subreddit} - {query} ({timelimit})\", 'tag': tag})\r\n        print(f\"Subreddit collection task {task_id} completed.\")\r\n    except Exception as e:\r\n        tasks[task_id]['status'] = 'error'\r\n        tasks[task_id]['message'] = str(e)\r\n        print(f\"Subreddit collection task {task_id} error: {e}\")\r\n\r\ndef start_subreddit_collection(subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections, conn=None):\r\n    task_id = len(tasks)\r\n    task = {'id': task_id, 'type': 'subreddit', 'subreddit': subreddit, 'timelimit': timelimit, 'query': query, 'use_ollama': use_ollama, 'max_comments': max_comments, 'status': 'running', 'message': ''}\r\n    tasks.append(task)\r\n    threading.Thread(target=run_subreddit_collection, args=(task_id, subreddit, timelimit, query, use_ollama, max_comments, tasks, completed_collections, conn)).start()\r\n    return \"Subreddit collection started in background.\", tasks, completed_collections"
        }
    ]
}